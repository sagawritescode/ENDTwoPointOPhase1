1
	Train Loss: 1.607 | Train Acc: 21.78%
	 Val. Loss: 1.593 |  Val. Acc: 26.80% 

2
	Train Loss: 1.604 | Train Acc: 22.81%
	 Val. Loss: 1.589 |  Val. Acc: 27.93% 

3
	Train Loss: 1.603 | Train Acc: 23.06%
	 Val. Loss: 1.587 |  Val. Acc: 28.93% 

4
	Train Loss: 1.602 | Train Acc: 23.64%
	 Val. Loss: 1.586 |  Val. Acc: 28.70% 

5
	Train Loss: 1.599 | Train Acc: 24.94%
	 Val. Loss: 1.583 |  Val. Acc: 29.06% 

6
	Train Loss: 1.596 | Train Acc: 25.68%
	 Val. Loss: 1.580 |  Val. Acc: 29.40% 

7
	Train Loss: 1.592 | Train Acc: 26.85%
	 Val. Loss: 1.580 |  Val. Acc: 27.91% 

8
	Train Loss: 1.585 | Train Acc: 27.80%
	 Val. Loss: 1.568 |  Val. Acc: 30.28% 

9
	Train Loss: 1.580 | Train Acc: 28.63%
	 Val. Loss: 1.565 |  Val. Acc: 29.67% 

10
	Train Loss: 1.570 | Train Acc: 29.91%
	 Val. Loss: 1.557 |  Val. Acc: 30.48% 

11
	Train Loss: 1.560 | Train Acc: 31.12%
	 Val. Loss: 1.552 |  Val. Acc: 31.71% 

12
	Train Loss: 1.549 | Train Acc: 32.62%
	 Val. Loss: 1.552 |  Val. Acc: 31.56% 

13
	Train Loss: 1.541 | Train Acc: 33.64%
	 Val. Loss: 1.549 |  Val. Acc: 32.19% 

14
	Train Loss: 1.530 | Train Acc: 35.42%
	 Val. Loss: 1.547 |  Val. Acc: 32.23% 

15
	Train Loss: 1.523 | Train Acc: 35.74%
	 Val. Loss: 1.538 |  Val. Acc: 33.34% 

16
	Train Loss: 1.515 | Train Acc: 36.70%
	 Val. Loss: 1.537 |  Val. Acc: 33.87% 

17
	Train Loss: 1.510 | Train Acc: 37.23%
	 Val. Loss: 1.534 |  Val. Acc: 34.28% 

18
	Train Loss: 1.503 | Train Acc: 38.17%
	 Val. Loss: 1.542 |  Val. Acc: 33.75% 

19
	Train Loss: 1.496 | Train Acc: 38.66%
	 Val. Loss: 1.534 |  Val. Acc: 34.54% 

20
	Train Loss: 1.489 | Train Acc: 39.99%
	 Val. Loss: 1.533 |  Val. Acc: 34.91% 

label distribution: actual vs pred: {0: {2: 77, 1: 254, 3: 91, 0: 360, 4: 8}, 4: {1: 107, 0: 158, 2: 28, 3: 50, 4: 3}, 1: {3: 216, 2: 48, 1: 379, 0: 139, 4: 3}, 3: {3: 216, 1: 183, 0: 42, 2: 21, 4: 2}, 2: {1: 223, 2: 79, 4: 4, 0: 198, 3: 75}}
	 pred vs actuals: {0: {2: 77, 1: 254, 3: 91, 0: 360, 4: 8}, 4: {1: 107, 0: 158, 2: 28, 3: 50, 4: 3}, 1: {3: 216, 2: 48, 1: 379, 0: 139, 4: 3}, 3: {3: 216, 1: 183, 0: 42, 2: 21, 4: 2}, 2: {1: 223, 2: 79, 4: 4, 0: 198, 3: 75}}
21
	Train Loss: 1.487 | Train Acc: 39.56%
	 Val. Loss: 1.532 |  Val. Acc: 35.08% 

22
	Train Loss: 1.480 | Train Acc: 40.60%
	 Val. Loss: 1.529 |  Val. Acc: 35.82% 

23
	Train Loss: 1.475 | Train Acc: 41.43%
	 Val. Loss: 1.527 |  Val. Acc: 36.21% 

24
	Train Loss: 1.468 | Train Acc: 41.96%
	 Val. Loss: 1.522 |  Val. Acc: 36.48% 

25
	Train Loss: 1.461 | Train Acc: 42.52%
	 Val. Loss: 1.526 |  Val. Acc: 35.92% 

26
	Train Loss: 1.455 | Train Acc: 43.77%
	 Val. Loss: 1.523 |  Val. Acc: 36.86% 

27
	Train Loss: 1.448 | Train Acc: 44.23%
	 Val. Loss: 1.524 |  Val. Acc: 36.42% 

28
	Train Loss: 1.442 | Train Acc: 44.87%
	 Val. Loss: 1.530 |  Val. Acc: 35.11% 

29
	Train Loss: 1.439 | Train Acc: 45.27%
	 Val. Loss: 1.525 |  Val. Acc: 36.35% 

30
	Train Loss: 1.432 | Train Acc: 46.08%
	 Val. Loss: 1.526 |  Val. Acc: 36.45% 

31
	Train Loss: 1.427 | Train Acc: 46.48%
	 Val. Loss: 1.530 |  Val. Acc: 36.02% 

32
	Train Loss: 1.424 | Train Acc: 46.90%
	 Val. Loss: 1.525 |  Val. Acc: 35.42% 

33
	Train Loss: 1.420 | Train Acc: 47.53%
	 Val. Loss: 1.519 |  Val. Acc: 36.79% 

34
	Train Loss: 1.417 | Train Acc: 47.85%
	 Val. Loss: 1.529 |  Val. Acc: 35.98% 

35
	Train Loss: 1.408 | Train Acc: 48.91%
	 Val. Loss: 1.523 |  Val. Acc: 36.95% 

36
	Train Loss: 1.405 | Train Acc: 49.18%
	 Val. Loss: 1.528 |  Val. Acc: 36.00% 

37
	Train Loss: 1.399 | Train Acc: 49.75%
	 Val. Loss: 1.519 |  Val. Acc: 37.60% 

38
	Train Loss: 1.397 | Train Acc: 49.70%
	 Val. Loss: 1.521 |  Val. Acc: 36.96% 

39
	Train Loss: 1.391 | Train Acc: 50.38%
	 Val. Loss: 1.522 |  Val. Acc: 36.76% 

40
	Train Loss: 1.384 | Train Acc: 51.42%
	 Val. Loss: 1.517 |  Val. Acc: 37.76% 

label distribution: actual vs pred: {0: {2: 82, 1: 229, 4: 46, 0: 383, 3: 50}, 4: {1: 98, 0: 172, 2: 24, 4: 28, 3: 24}, 1: {3: 184, 1: 395, 2: 60, 4: 10, 0: 136}, 3: {3: 213, 1: 166, 4: 18, 0: 46, 2: 21}, 2: {1: 212, 2: 97, 4: 16, 0: 202, 3: 52}}
	 pred vs actuals: {0: {2: 82, 1: 229, 4: 46, 0: 383, 3: 50}, 4: {1: 98, 0: 172, 2: 24, 4: 28, 3: 24}, 1: {3: 184, 1: 395, 2: 60, 4: 10, 0: 136}, 3: {3: 213, 1: 166, 4: 18, 0: 46, 2: 21}, 2: {1: 212, 2: 97, 4: 16, 0: 202, 3: 52}}
41
	Train Loss: 1.380 | Train Acc: 52.13%
	 Val. Loss: 1.519 |  Val. Acc: 37.13% 

42
	Train Loss: 1.377 | Train Acc: 52.24%
	 Val. Loss: 1.528 |  Val. Acc: 36.06% 

43
	Train Loss: 1.376 | Train Acc: 52.42%
	 Val. Loss: 1.519 |  Val. Acc: 37.10% 

44
	Train Loss: 1.368 | Train Acc: 53.61%
	 Val. Loss: 1.520 |  Val. Acc: 36.92% 

45
	Train Loss: 1.360 | Train Acc: 53.62%
	 Val. Loss: 1.515 |  Val. Acc: 37.37% 

46
	Train Loss: 1.355 | Train Acc: 55.26%
	 Val. Loss: 1.515 |  Val. Acc: 37.67% 

47
	Train Loss: 1.355 | Train Acc: 54.36%
	 Val. Loss: 1.517 |  Val. Acc: 37.06% 

48
	Train Loss: 1.348 | Train Acc: 55.54%
	 Val. Loss: 1.519 |  Val. Acc: 36.89% 

49
	Train Loss: 1.345 | Train Acc: 55.52%
	 Val. Loss: 1.517 |  Val. Acc: 37.57% 

50
	Train Loss: 1.336 | Train Acc: 56.88%
	 Val. Loss: 1.517 |  Val. Acc: 37.41% 

51
	Train Loss: 1.338 | Train Acc: 56.46%
	 Val. Loss: 1.515 |  Val. Acc: 37.24% 

52
	Train Loss: 1.334 | Train Acc: 56.98%
	 Val. Loss: 1.514 |  Val. Acc: 37.43% 

53
	Train Loss: 1.328 | Train Acc: 57.01%
	 Val. Loss: 1.518 |  Val. Acc: 37.43% 

54
	Train Loss: 1.327 | Train Acc: 57.60%
	 Val. Loss: 1.514 |  Val. Acc: 37.50% 

55
	Train Loss: 1.321 | Train Acc: 58.39%
	 Val. Loss: 1.513 |  Val. Acc: 37.64% 

56
	Train Loss: 1.316 | Train Acc: 58.85%
	 Val. Loss: 1.512 |  Val. Acc: 38.16% 

57
	Train Loss: 1.312 | Train Acc: 59.17%
	 Val. Loss: 1.516 |  Val. Acc: 37.13% 

58
	Train Loss: 1.309 | Train Acc: 59.52%
	 Val. Loss: 1.512 |  Val. Acc: 37.66% 

59
	Train Loss: 1.306 | Train Acc: 59.93%
	 Val. Loss: 1.513 |  Val. Acc: 37.67% 

60
	Train Loss: 1.301 | Train Acc: 60.61%
	 Val. Loss: 1.515 |  Val. Acc: 37.50% 

label distribution: actual vs pred: {0: {2: 117, 1: 211, 0: 350, 4: 62, 3: 50}, 4: {0: 159, 2: 35, 4: 42, 3: 22, 1: 88}, 1: {3: 214, 1: 371, 4: 16, 2: 71, 0: 113}, 3: {3: 230, 1: 164, 4: 14, 0: 35, 2: 21}, 2: {1: 192, 2: 113, 4: 33, 0: 176, 3: 65}}
	 pred vs actuals: {0: {2: 117, 1: 211, 0: 350, 4: 62, 3: 50}, 4: {0: 159, 2: 35, 4: 42, 3: 22, 1: 88}, 1: {3: 214, 1: 371, 4: 16, 2: 71, 0: 113}, 3: {3: 230, 1: 164, 4: 14, 0: 35, 2: 21}, 2: {1: 192, 2: 113, 4: 33, 0: 176, 3: 65}}
61
	Train Loss: 1.297 | Train Acc: 60.95%
	 Val. Loss: 1.505 |  Val. Acc: 38.46% 

62
	Train Loss: 1.291 | Train Acc: 61.29%
	 Val. Loss: 1.515 |  Val. Acc: 37.46% 

63
	Train Loss: 1.293 | Train Acc: 61.28%
	 Val. Loss: 1.516 |  Val. Acc: 37.37% 

64
	Train Loss: 1.289 | Train Acc: 61.29%
	 Val. Loss: 1.515 |  Val. Acc: 37.56% 

65
	Train Loss: 1.286 | Train Acc: 61.94%
	 Val. Loss: 1.519 |  Val. Acc: 37.24% 

66
	Train Loss: 1.285 | Train Acc: 62.07%
	 Val. Loss: 1.514 |  Val. Acc: 37.15% 

67
	Train Loss: 1.280 | Train Acc: 62.56%
	 Val. Loss: 1.511 |  Val. Acc: 37.63% 

68
	Train Loss: 1.277 | Train Acc: 62.66%
	 Val. Loss: 1.512 |  Val. Acc: 37.82% 

69
	Train Loss: 1.272 | Train Acc: 63.58%
	 Val. Loss: 1.514 |  Val. Acc: 37.53% 

70
	Train Loss: 1.273 | Train Acc: 63.02%
	 Val. Loss: 1.511 |  Val. Acc: 37.90% 

71
	Train Loss: 1.271 | Train Acc: 63.58%
	 Val. Loss: 1.507 |  Val. Acc: 38.16% 

72
	Train Loss: 1.262 | Train Acc: 64.17%
	 Val. Loss: 1.505 |  Val. Acc: 38.94% 

73
	Train Loss: 1.257 | Train Acc: 64.74%
	 Val. Loss: 1.506 |  Val. Acc: 38.77% 

74
	Train Loss: 1.258 | Train Acc: 64.39%
	 Val. Loss: 1.511 |  Val. Acc: 37.59% 

75
	Train Loss: 1.253 | Train Acc: 65.10%
	 Val. Loss: 1.510 |  Val. Acc: 38.22% 

76
	Train Loss: 1.253 | Train Acc: 65.36%
	 Val. Loss: 1.510 |  Val. Acc: 38.20% 

77
	Train Loss: 1.248 | Train Acc: 65.62%
	 Val. Loss: 1.518 |  Val. Acc: 37.35% 

78
	Train Loss: 1.248 | Train Acc: 65.43%
	 Val. Loss: 1.514 |  Val. Acc: 38.02% 

79
	Train Loss: 1.241 | Train Acc: 67.02%
	 Val. Loss: 1.510 |  Val. Acc: 38.23% 

80
	Train Loss: 1.241 | Train Acc: 66.69%
	 Val. Loss: 1.513 |  Val. Acc: 37.97% 

label distribution: actual vs pred: {0: {4: 72, 1: 211, 2: 113, 0: 348, 3: 46}, 4: {1: 80, 4: 54, 2: 37, 0: 154, 3: 21}, 1: {3: 197, 1: 390, 4: 12, 2: 80, 0: 106}, 3: {3: 218, 1: 172, 4: 14, 0: 36, 2: 24}, 2: {1: 203, 2: 110, 4: 35, 0: 172, 3: 59}}
	 pred vs actuals: {0: {4: 72, 1: 211, 2: 113, 0: 348, 3: 46}, 4: {1: 80, 4: 54, 2: 37, 0: 154, 3: 21}, 1: {3: 197, 1: 390, 4: 12, 2: 80, 0: 106}, 3: {3: 218, 1: 172, 4: 14, 0: 36, 2: 24}, 2: {1: 203, 2: 110, 4: 35, 0: 172, 3: 59}}
81
	Train Loss: 1.239 | Train Acc: 66.51%
	 Val. Loss: 1.512 |  Val. Acc: 37.90% 

82
	Train Loss: 1.235 | Train Acc: 67.21%
	 Val. Loss: 1.512 |  Val. Acc: 38.15% 

83
	Train Loss: 1.235 | Train Acc: 66.83%
	 Val. Loss: 1.513 |  Val. Acc: 37.91% 

84
	Train Loss: 1.229 | Train Acc: 67.38%
	 Val. Loss: 1.511 |  Val. Acc: 38.05% 

85
	Train Loss: 1.230 | Train Acc: 67.79%
	 Val. Loss: 1.509 |  Val. Acc: 38.09% 

86
	Train Loss: 1.228 | Train Acc: 67.79%
	 Val. Loss: 1.511 |  Val. Acc: 38.22% 

87
	Train Loss: 1.224 | Train Acc: 67.81%
	 Val. Loss: 1.514 |  Val. Acc: 37.91% 

88
	Train Loss: 1.224 | Train Acc: 68.37%
	 Val. Loss: 1.513 |  Val. Acc: 37.95% 

89
	Train Loss: 1.223 | Train Acc: 68.05%
	 Val. Loss: 1.509 |  Val. Acc: 38.11% 

90
	Train Loss: 1.215 | Train Acc: 69.22%
	 Val. Loss: 1.511 |  Val. Acc: 38.09% 

91
	Train Loss: 1.213 | Train Acc: 69.73%
	 Val. Loss: 1.513 |  Val. Acc: 37.76% 

92
	Train Loss: 1.213 | Train Acc: 69.53%
	 Val. Loss: 1.510 |  Val. Acc: 38.30% 

93
	Train Loss: 1.207 | Train Acc: 69.68%
	 Val. Loss: 1.512 |  Val. Acc: 37.90% 

94
	Train Loss: 1.208 | Train Acc: 69.84%
	 Val. Loss: 1.514 |  Val. Acc: 37.63% 

95
	Train Loss: 1.206 | Train Acc: 69.77%
	 Val. Loss: 1.507 |  Val. Acc: 38.73% 

96
	Train Loss: 1.203 | Train Acc: 70.23%
	 Val. Loss: 1.509 |  Val. Acc: 38.37% 

97
	Train Loss: 1.204 | Train Acc: 70.05%
	 Val. Loss: 1.507 |  Val. Acc: 38.48% 

98
	Train Loss: 1.200 | Train Acc: 70.50%
	 Val. Loss: 1.508 |  Val. Acc: 38.50% 

99
	Train Loss: 1.202 | Train Acc: 70.31%
	 Val. Loss: 1.517 |  Val. Acc: 37.86% 

100
	Train Loss: 1.201 | Train Acc: 70.55%
	 Val. Loss: 1.511 |  Val. Acc: 38.09% 

label distribution: actual vs pred: {0: {2: 135, 1: 189, 4: 85, 0: 340, 3: 41}, 4: {0: 144, 4: 63, 2: 49, 1: 74, 3: 16}, 1: {1: 394, 4: 23, 2: 96, 3: 172, 0: 100}, 3: {3: 210, 1: 182, 4: 16, 0: 31, 2: 25}, 2: {1: 192, 4: 46, 2: 119, 0: 165, 3: 57}}
	 pred vs actuals: {0: {2: 135, 1: 189, 4: 85, 0: 340, 3: 41}, 4: {0: 144, 4: 63, 2: 49, 1: 74, 3: 16}, 1: {1: 394, 4: 23, 2: 96, 3: 172, 0: 100}, 3: {3: 210, 1: 182, 4: 16, 0: 31, 2: 25}, 2: {1: 192, 4: 46, 2: 119, 0: 165, 3: 57}}
101
	Train Loss: 1.193 | Train Acc: 71.22%
	 Val. Loss: 1.517 |  Val. Acc: 37.33% 

102
	Train Loss: 1.195 | Train Acc: 70.90%
	 Val. Loss: 1.509 |  Val. Acc: 38.48% 

103
	Train Loss: 1.194 | Train Acc: 71.21%
	 Val. Loss: 1.508 |  Val. Acc: 38.43% 

104
	Train Loss: 1.186 | Train Acc: 71.81%
	 Val. Loss: 1.509 |  Val. Acc: 38.60% 

105
	Train Loss: 1.188 | Train Acc: 71.72%
	 Val. Loss: 1.503 |  Val. Acc: 38.43% 

106
	Train Loss: 1.182 | Train Acc: 72.04%
	 Val. Loss: 1.509 |  Val. Acc: 38.39% 

107
	Train Loss: 1.185 | Train Acc: 72.01%
	 Val. Loss: 1.509 |  Val. Acc: 38.59% 

108
	Train Loss: 1.183 | Train Acc: 72.16%
	 Val. Loss: 1.518 |  Val. Acc: 37.42% 

109
	Train Loss: 1.180 | Train Acc: 72.05%
	 Val. Loss: 1.517 |  Val. Acc: 37.77% 

110
	Train Loss: 1.178 | Train Acc: 72.11%
	 Val. Loss: 1.514 |  Val. Acc: 38.00% 

111
	Train Loss: 1.180 | Train Acc: 72.25%
	 Val. Loss: 1.511 |  Val. Acc: 38.23% 

112
	Train Loss: 1.176 | Train Acc: 72.58%
	 Val. Loss: 1.507 |  Val. Acc: 38.74% 

113
	Train Loss: 1.175 | Train Acc: 73.09%
	 Val. Loss: 1.507 |  Val. Acc: 38.96% 

114
	Train Loss: 1.173 | Train Acc: 72.93%
	 Val. Loss: 1.505 |  Val. Acc: 39.36% 

115
	Train Loss: 1.169 | Train Acc: 73.51%
	 Val. Loss: 1.507 |  Val. Acc: 38.66% 

116
	Train Loss: 1.171 | Train Acc: 73.49%
	 Val. Loss: 1.512 |  Val. Acc: 37.65% 

117
	Train Loss: 1.169 | Train Acc: 73.89%
	 Val. Loss: 1.508 |  Val. Acc: 38.53% 

118
	Train Loss: 1.165 | Train Acc: 73.61%
	 Val. Loss: 1.510 |  Val. Acc: 38.39% 

119
	Train Loss: 1.159 | Train Acc: 74.85%
	 Val. Loss: 1.506 |  Val. Acc: 38.82% 

120
	Train Loss: 1.163 | Train Acc: 74.38%
	 Val. Loss: 1.507 |  Val. Acc: 38.90% 

label distribution: actual vs pred: {0: {2: 130, 1: 203, 0: 363, 4: 68, 3: 26}, 4: {0: 163, 4: 55, 2: 43, 1: 73, 3: 12}, 1: {1: 444, 4: 11, 2: 82, 3: 142, 0: 106}, 3: {3: 169, 1: 221, 4: 11, 0: 40, 2: 23}, 2: {1: 200, 4: 36, 2: 117, 0: 182, 3: 44}}
	 pred vs actuals: {0: {2: 130, 1: 203, 0: 363, 4: 68, 3: 26}, 4: {0: 163, 4: 55, 2: 43, 1: 73, 3: 12}, 1: {1: 444, 4: 11, 2: 82, 3: 142, 0: 106}, 3: {3: 169, 1: 221, 4: 11, 0: 40, 2: 23}, 2: {1: 200, 4: 36, 2: 117, 0: 182, 3: 44}}
121
	Train Loss: 1.164 | Train Acc: 74.02%
	 Val. Loss: 1.512 |  Val. Acc: 38.20% 

122
	Train Loss: 1.160 | Train Acc: 74.42%
	 Val. Loss: 1.513 |  Val. Acc: 38.33% 

123
	Train Loss: 1.163 | Train Acc: 73.78%
	 Val. Loss: 1.512 |  Val. Acc: 38.20% 

124
	Train Loss: 1.160 | Train Acc: 73.95%
	 Val. Loss: 1.509 |  Val. Acc: 38.43% 

125
	Train Loss: 1.162 | Train Acc: 73.85%
	 Val. Loss: 1.517 |  Val. Acc: 37.79% 

126
	Train Loss: 1.154 | Train Acc: 74.57%
	 Val. Loss: 1.515 |  Val. Acc: 37.69% 

127
	Train Loss: 1.152 | Train Acc: 75.23%
	 Val. Loss: 1.514 |  Val. Acc: 38.32% 

128
	Train Loss: 1.155 | Train Acc: 74.88%
	 Val. Loss: 1.512 |  Val. Acc: 38.46% 

129
	Train Loss: 1.154 | Train Acc: 74.85%
	 Val. Loss: 1.516 |  Val. Acc: 38.06% 

130
	Train Loss: 1.149 | Train Acc: 75.52%
	 Val. Loss: 1.510 |  Val. Acc: 38.52% 

131
	Train Loss: 1.155 | Train Acc: 74.56%
	 Val. Loss: 1.511 |  Val. Acc: 38.52% 

132
	Train Loss: 1.149 | Train Acc: 75.46%
	 Val. Loss: 1.509 |  Val. Acc: 38.38% 

133
	Train Loss: 1.145 | Train Acc: 75.47%
	 Val. Loss: 1.509 |  Val. Acc: 38.72% 

134
	Train Loss: 1.147 | Train Acc: 75.56%
	 Val. Loss: 1.508 |  Val. Acc: 39.03% 

135
	Train Loss: 1.148 | Train Acc: 75.21%
	 Val. Loss: 1.505 |  Val. Acc: 39.18% 

136
	Train Loss: 1.142 | Train Acc: 76.03%
	 Val. Loss: 1.512 |  Val. Acc: 38.56% 

137
	Train Loss: 1.142 | Train Acc: 76.08%
	 Val. Loss: 1.517 |  Val. Acc: 37.73% 

138
	Train Loss: 1.139 | Train Acc: 76.46%
	 Val. Loss: 1.513 |  Val. Acc: 38.46% 

139
	Train Loss: 1.140 | Train Acc: 76.09%
	 Val. Loss: 1.515 |  Val. Acc: 37.99% 

140
	Train Loss: 1.144 | Train Acc: 75.95%
	 Val. Loss: 1.507 |  Val. Acc: 39.16% 

label distribution: actual vs pred: {0: {2: 126, 1: 204, 4: 82, 0: 343, 3: 35}, 4: {4: 71, 2: 37, 1: 80, 0: 150, 3: 8}, 1: {1: 443, 2: 84, 3: 147, 0: 96, 4: 15}, 3: {3: 182, 1: 215, 4: 9, 0: 33, 2: 25}, 2: {1: 209, 2: 117, 0: 165, 3: 45, 4: 43}}
	 pred vs actuals: {0: {2: 126, 1: 204, 4: 82, 0: 343, 3: 35}, 4: {4: 71, 2: 37, 1: 80, 0: 150, 3: 8}, 1: {1: 443, 2: 84, 3: 147, 0: 96, 4: 15}, 3: {3: 182, 1: 215, 4: 9, 0: 33, 2: 25}, 2: {1: 209, 2: 117, 0: 165, 3: 45, 4: 43}}
141
	Train Loss: 1.142 | Train Acc: 75.83%
	 Val. Loss: 1.517 |  Val. Acc: 38.14% 

142
	Train Loss: 1.140 | Train Acc: 76.06%
	 Val. Loss: 1.508 |  Val. Acc: 38.82% 

143
	Train Loss: 1.135 | Train Acc: 76.55%
	 Val. Loss: 1.506 |  Val. Acc: 39.03% 

144
	Train Loss: 1.139 | Train Acc: 76.07%
	 Val. Loss: 1.507 |  Val. Acc: 38.99% 

145
	Train Loss: 1.136 | Train Acc: 76.68%
	 Val. Loss: 1.505 |  Val. Acc: 39.19% 

146
	Train Loss: 1.132 | Train Acc: 77.23%
	 Val. Loss: 1.511 |  Val. Acc: 38.69% 

147
	Train Loss: 1.131 | Train Acc: 77.03%
	 Val. Loss: 1.511 |  Val. Acc: 38.32% 

148
	Train Loss: 1.135 | Train Acc: 76.60%
	 Val. Loss: 1.506 |  Val. Acc: 38.73% 

149
	Train Loss: 1.133 | Train Acc: 77.18%
	 Val. Loss: 1.513 |  Val. Acc: 38.26% 

150
	Train Loss: 1.129 | Train Acc: 77.45%
	 Val. Loss: 1.507 |  Val. Acc: 39.00% 

151
	Train Loss: 1.131 | Train Acc: 77.03%
	 Val. Loss: 1.516 |  Val. Acc: 38.46% 

152
	Train Loss: 1.133 | Train Acc: 77.00%
	 Val. Loss: 1.507 |  Val. Acc: 38.64% 

153
	Train Loss: 1.129 | Train Acc: 77.11%
	 Val. Loss: 1.513 |  Val. Acc: 38.08% 

154
	Train Loss: 1.124 | Train Acc: 77.86%
	 Val. Loss: 1.515 |  Val. Acc: 38.10% 

155
	Train Loss: 1.131 | Train Acc: 77.02%
	 Val. Loss: 1.516 |  Val. Acc: 38.25% 

156
	Train Loss: 1.125 | Train Acc: 77.55%
	 Val. Loss: 1.510 |  Val. Acc: 38.60% 

157
	Train Loss: 1.124 | Train Acc: 77.80%
	 Val. Loss: 1.507 |  Val. Acc: 38.70% 

158
	Train Loss: 1.125 | Train Acc: 77.77%
	 Val. Loss: 1.506 |  Val. Acc: 39.00% 

159
	Train Loss: 1.123 | Train Acc: 77.92%
	 Val. Loss: 1.506 |  Val. Acc: 38.90% 

160
	Train Loss: 1.124 | Train Acc: 77.58%
	 Val. Loss: 1.510 |  Val. Acc: 38.45% 

label distribution: actual vs pred: {0: {2: 156, 1: 183, 4: 91, 0: 315, 3: 45}, 4: {4: 71, 2: 51, 1: 74, 0: 134, 3: 16}, 1: {1: 401, 2: 95, 3: 188, 0: 85, 4: 16}, 3: {3: 216, 1: 178, 4: 13, 0: 24, 2: 33}, 2: {1: 190, 4: 48, 2: 136, 0: 148, 3: 57}}
	 pred vs actuals: {0: {2: 156, 1: 183, 4: 91, 0: 315, 3: 45}, 4: {4: 71, 2: 51, 1: 74, 0: 134, 3: 16}, 1: {1: 401, 2: 95, 3: 188, 0: 85, 4: 16}, 3: {3: 216, 1: 178, 4: 13, 0: 24, 2: 33}, 2: {1: 190, 4: 48, 2: 136, 0: 148, 3: 57}}
161
	Train Loss: 1.118 | Train Acc: 78.42%
	 Val. Loss: 1.510 |  Val. Acc: 38.90% 

162
	Train Loss: 1.121 | Train Acc: 77.67%
	 Val. Loss: 1.512 |  Val. Acc: 38.50% 

163
	Train Loss: 1.120 | Train Acc: 77.99%
	 Val. Loss: 1.510 |  Val. Acc: 38.70% 

164
	Train Loss: 1.120 | Train Acc: 78.16%
	 Val. Loss: 1.507 |  Val. Acc: 38.73% 

165
	Train Loss: 1.118 | Train Acc: 78.75%
	 Val. Loss: 1.507 |  Val. Acc: 38.88% 

166
	Train Loss: 1.116 | Train Acc: 78.57%
	 Val. Loss: 1.504 |  Val. Acc: 39.32% 

167
	Train Loss: 1.115 | Train Acc: 78.53%
	 Val. Loss: 1.507 |  Val. Acc: 39.21% 

168
	Train Loss: 1.114 | Train Acc: 78.61%
	 Val. Loss: 1.504 |  Val. Acc: 39.10% 

169
	Train Loss: 1.115 | Train Acc: 78.61%
	 Val. Loss: 1.514 |  Val. Acc: 38.15% 

170
	Train Loss: 1.117 | Train Acc: 78.28%
	 Val. Loss: 1.508 |  Val. Acc: 38.86% 

171
	Train Loss: 1.115 | Train Acc: 78.28%
	 Val. Loss: 1.513 |  Val. Acc: 38.13% 

172
	Train Loss: 1.114 | Train Acc: 78.56%
	 Val. Loss: 1.511 |  Val. Acc: 38.56% 

173
	Train Loss: 1.109 | Train Acc: 78.95%
	 Val. Loss: 1.508 |  Val. Acc: 38.70% 

174
	Train Loss: 1.109 | Train Acc: 79.15%
	 Val. Loss: 1.516 |  Val. Acc: 38.15% 

175
	Train Loss: 1.110 | Train Acc: 79.02%
	 Val. Loss: 1.513 |  Val. Acc: 38.33% 

176
	Train Loss: 1.108 | Train Acc: 79.30%
	 Val. Loss: 1.510 |  Val. Acc: 38.52% 

177
	Train Loss: 1.107 | Train Acc: 79.16%
	 Val. Loss: 1.514 |  Val. Acc: 37.93% 

178
	Train Loss: 1.106 | Train Acc: 79.42%
	 Val. Loss: 1.513 |  Val. Acc: 38.46% 

179
	Train Loss: 1.106 | Train Acc: 79.22%
	 Val. Loss: 1.514 |  Val. Acc: 38.16% 

180
	Train Loss: 1.107 | Train Acc: 79.26%
	 Val. Loss: 1.518 |  Val. Acc: 37.76% 

label distribution: actual vs pred: {0: {2: 148, 1: 208, 0: 304, 4: 84, 3: 46}, 4: {4: 64, 2: 39, 1: 88, 0: 137, 3: 18}, 1: {1: 407, 2: 96, 3: 176, 0: 87, 4: 19}, 3: {3: 212, 1: 186, 4: 9, 0: 27, 2: 30}, 2: {1: 200, 2: 129, 3: 55, 0: 153, 4: 42}}
	 pred vs actuals: {0: {2: 148, 1: 208, 0: 304, 4: 84, 3: 46}, 4: {4: 64, 2: 39, 1: 88, 0: 137, 3: 18}, 1: {1: 407, 2: 96, 3: 176, 0: 87, 4: 19}, 3: {3: 212, 1: 186, 4: 9, 0: 27, 2: 30}, 2: {1: 200, 2: 129, 3: 55, 0: 153, 4: 42}}
181
	Train Loss: 1.106 | Train Acc: 79.28%
	 Val. Loss: 1.517 |  Val. Acc: 37.96% 

182
	Train Loss: 1.104 | Train Acc: 79.29%
	 Val. Loss: 1.522 |  Val. Acc: 37.50% 

183
	Train Loss: 1.103 | Train Acc: 79.84%
	 Val. Loss: 1.517 |  Val. Acc: 37.73% 

184
	Train Loss: 1.102 | Train Acc: 79.75%
	 Val. Loss: 1.524 |  Val. Acc: 37.02% 

185
	Train Loss: 1.103 | Train Acc: 79.50%
	 Val. Loss: 1.519 |  Val. Acc: 37.68% 

186
	Train Loss: 1.102 | Train Acc: 79.95%
	 Val. Loss: 1.508 |  Val. Acc: 39.02% 

187
	Train Loss: 1.106 | Train Acc: 79.14%
	 Val. Loss: 1.522 |  Val. Acc: 37.46% 

188
	Train Loss: 1.099 | Train Acc: 79.97%
	 Val. Loss: 1.514 |  Val. Acc: 38.56% 

189
	Train Loss: 1.099 | Train Acc: 80.38%
	 Val. Loss: 1.511 |  Val. Acc: 38.69% 

190
	Train Loss: 1.104 | Train Acc: 79.45%
	 Val. Loss: 1.515 |  Val. Acc: 38.38% 

191
	Train Loss: 1.098 | Train Acc: 80.06%
	 Val. Loss: 1.517 |  Val. Acc: 38.32% 

192
	Train Loss: 1.099 | Train Acc: 79.94%
	 Val. Loss: 1.511 |  Val. Acc: 38.56% 

193
	Train Loss: 1.101 | Train Acc: 79.37%
	 Val. Loss: 1.515 |  Val. Acc: 38.22% 

194
	Train Loss: 1.097 | Train Acc: 80.48%
	 Val. Loss: 1.513 |  Val. Acc: 38.28% 

195
	Train Loss: 1.099 | Train Acc: 79.85%
	 Val. Loss: 1.513 |  Val. Acc: 38.42% 

196
	Train Loss: 1.099 | Train Acc: 80.27%
	 Val. Loss: 1.508 |  Val. Acc: 38.62% 

197
	Train Loss: 1.097 | Train Acc: 80.10%
	 Val. Loss: 1.508 |  Val. Acc: 38.96% 

198
	Train Loss: 1.099 | Train Acc: 79.83%
	 Val. Loss: 1.511 |  Val. Acc: 39.06% 

199
	Train Loss: 1.095 | Train Acc: 80.31%
	 Val. Loss: 1.514 |  Val. Acc: 38.27% 

200
	Train Loss: 1.098 | Train Acc: 80.09%
	 Val. Loss: 1.508 |  Val. Acc: 39.30% 

label distribution: actual vs pred: {0: {2: 150, 1: 193, 4: 89, 0: 331, 3: 27}, 4: {4: 68, 2: 47, 1: 77, 0: 143, 3: 11}, 1: {1: 442, 2: 106, 3: 136, 0: 83, 4: 18}, 3: {3: 177, 1: 213, 4: 13, 0: 31, 2: 30}, 2: {1: 196, 2: 142, 0: 152, 3: 47, 4: 42}}
	 pred vs actuals: {0: {2: 150, 1: 193, 4: 89, 0: 331, 3: 27}, 4: {4: 68, 2: 47, 1: 77, 0: 143, 3: 11}, 1: {1: 442, 2: 106, 3: 136, 0: 83, 4: 18}, 3: {3: 177, 1: 213, 4: 13, 0: 31, 2: 30}, 2: {1: 196, 2: 142, 0: 152, 3: 47, 4: 42}}

     [0.21776741368857694, 0.2281477809904917, 0.23060862767641946, 0.23641527474743046, 0.2493816591016778, 0.25682052588898296, 0.26850266366788783, 0.27795677125181784, 0.2863218977571078, 0.29914683974496853, 0.31122201191235893, 0.3262224125263353, 0.336398752312682, 0.3541992109387977, 0.35739105177796593, 0.366951554061071, 0.3723038331558715, 0.3817391652766972, 0.3865719878510253, 0.3998525495126367, 0.3955704758972882, 0.40599715614427717, 0.41426339827171743, 0.4196231875517597, 0.4251882560449104, 0.4376739866385177, 0.44230278183336125, 0.4486964772281037, 0.4526743872524941, 0.46083423454467565, 0.4647858587846364, 0.46896028600326956, 0.47531768203326014, 0.47852829847161626, 0.48905261167108194, 0.4918439078004393, 0.49747907162801314, 0.4969984178673731, 0.5037501002555569, 0.5141930525705695, 0.5213465312300207, 0.5223804374808045, 0.5242266984290729, 0.5360790676177909, 0.5362480473300638, 0.5526052932488864, 0.5436243291042712, 0.5553602900134919, 0.5552351197151288, 0.5688185934062417, 0.5646078668228568, 0.569824962038972, 0.5700753023635307, 0.5759883441337167, 0.5839429124849572, 0.588482836610106, 0.5916671674001163, 0.5951544100835443, 0.59925623843659, 0.6061055535050832, 0.6094588640073663, 0.6129360931104721, 0.6128472223129446, 0.6128747596588309, 0.6194299247166882, 0.6206778717911952, 0.6255933069475165, 0.626554614332713, 0.6358209666051821, 0.630175789197286, 0.6358034425674508, 0.6416801851089686, 0.6474242468947145, 0.6438919430998362, 0.6509552992642198, 0.6536226767927544, 0.6562349796295166, 0.6543348954148489, 0.6701739365107393, 0.6669195106584732, 0.6651458484397087, 0.6721366058745885, 0.6683464512977426, 0.6738126351408762, 0.6778719057775524, 0.6779332392291936, 0.67812975655952, 0.6837386344665806, 0.6805205078429828, 0.6922376934251829, 0.697285808656858, 0.6952530442307528, 0.6968139170511672, 0.6984098374027096, 0.6976788433175108, 0.702271339011519, 0.7005402347812913, 0.7050363494925302, 0.7031362652778625, 0.7055445405990566, 0.7122411480233005, 0.7089779600160852, 0.7120809301938096, 0.7180565568410098, 0.7172366919582838, 0.7204197710507536, 0.7200642873163092, 0.7216151465564014, 0.7204648321622038, 0.7210894316843112, 0.7225338960891445, 0.7257883222135779, 0.7308727369460886, 0.7293293878912381, 0.7350546744860471, 0.7349207423049021, 0.7388898901743431, 0.7360622948163176, 0.7485405152246832, 0.7437602640831307, 0.7402454837816491, 0.744160808656858, 0.7378109228121091, 0.7394869523505642, 0.7384968557858576, 0.7457304433600543, 0.7523469418695529, 0.7487883521541613, 0.7484854402607435, 0.7551832993825277, 0.7456140352166407, 0.7546037612440379, 0.7546938837391056, 0.7555575583079089, 0.7521429143539847, 0.7603290475666795, 0.7607759052215646, 0.764611121182028, 0.760864776019092, 0.7595267064495174, 0.7583413442520246, 0.7605881500461874, 0.7655023330967176, 0.7607483678756783, 0.7667690559064961, 0.7723428862824288, 0.7703176320415653, 0.7660380618212974, 0.7718334434783622, 0.7745183450446281, 0.7702738223554881, 0.769952134998966, 0.7711299867390498, 0.7786214250952141, 0.7702024753235247, 0.7754633802257173, 0.7779692882272207, 0.7777201996002023, 0.7792447729197811, 0.7758126052002928, 0.7841952554711468, 0.7766587560035322, 0.7799494813566339, 0.7816167487401396, 0.7874947429791977, 0.7856660057964935, 0.7853180325194581, 0.7861303874891098, 0.7861479112546738, 0.7828484240187902, 0.7827858388696087, 0.785568373116184, 0.7895462828684071, 0.7914551289658568, 0.7902146920765916, 0.7930147500887309, 0.7915527619183336, 0.7942276496321099, 0.7922474565026967, 0.7926142052428363, 0.7927744233444945, 0.7928983416731499, 0.7984458864007367, 0.7974658034163523, 0.7950224805640304, 0.7995073299974067, 0.7914451153855345, 0.7997213710932971, 0.8038332132988324, 0.794514289457504, 0.8006401206260403, 0.799408445619557, 0.7937106966428017, 0.804832071746321, 0.7985360088958043, 0.8027267083185449, 0.800960556285022, 0.7983394918376452, 0.8030834434783622, 0.8008629236047127]
[0.2680186172749134, 0.2793218087642751, 0.2892952130195942, 0.2869680853600198, 0.2906250002536368, 0.29401595731999014, 0.2790558509370114, 0.30279255357194457, 0.2966755317880752, 0.30478723410596237, 0.31708776602085603, 0.3156250000317046, 0.3219414893934067, 0.3223404256587333, 0.3333776596378773, 0.3386968085740475, 0.3427526595744681, 0.3375000002536368, 0.34541223404255317, 0.34913563855150914, 0.3507978725940623, 0.3582446809778822, 0.362101064083424, 0.3648271275327561, 0.3591755317880752, 0.3686170216570509, 0.3642287235310737, 0.35113031940257294, 0.3634973402987135, 0.36449468072424546, 0.36017287221360716, 0.35418882966041565, 0.36788563842469074, 0.3598404254050965, 0.3695478724672439, 0.3599734046357743, 0.3759973404255319, 0.36961436208258286, 0.367619681231519, 0.3775930854868382, 0.371276596125136, 0.3606382982527956, 0.37101063829787234, 0.3692154256587333, 0.37367021276595747, 0.3767287236578921, 0.37061170250811476, 0.36894946846556154, 0.37566489361702127, 0.374069149189807, 0.3724069151472538, 0.37433510638297873, 0.37433510638297873, 0.375, 0.3763962768493815, 0.3816489361702128, 0.371343085106383, 0.3765957450613062, 0.37666223404255317, 0.375, 0.3846409574468085, 0.3746010642102424, 0.37367021276595747, 0.3755984046357743, 0.3724069151472538, 0.37154255331830777, 0.3762632982527956, 0.3781914894885205, 0.37533244680851063, 0.37898936170212766, 0.38158244718896583, 0.38936170238129636, 0.3876994683387432, 0.37586436182894606, 0.3822473408059871, 0.3819813829787234, 0.37353723416937157, 0.38018617033958435, 0.38231382978723405, 0.3796542553191489, 0.37898936170212766, 0.3815159575736269, 0.3791223402987135, 0.3804521275327561, 0.38091755357194457, 0.3822473408059871, 0.3791223402987135, 0.37945478710722413, 0.38111702114977736, 0.3808510639566056, 0.3775930854868382, 0.3829787234042553, 0.37898936170212766, 0.3762632982527956, 0.3873005319148936, 0.38371010663661553, 0.38477393667748633, 0.38503989387065807, 0.3785904259123701, 0.38091755357194457, 0.3732712769761999, 0.38477393667748633, 0.38430851063829785, 0.3859707446808511, 0.38430851063829785, 0.38390957484854027, 0.3859042556996041, 0.37420212778639284, 0.3776595744680851, 0.37998670212765956, 0.38231382978723405, 0.38736702153023256, 0.3896276595744681, 0.3935505322953488, 0.38663563829787234, 0.3765292554459673, 0.3853058510638298, 0.38390957484854027, 0.38823138335917856, 0.38896276595744683, 0.3819813829787234, 0.38331117021276595, 0.3819813829787234, 0.38430851063829785, 0.3779255322953488, 0.3769281918698169, 0.383244681231519, 0.38457446846556154, 0.3805851067634339, 0.38523936208258286, 0.38523936208258286, 0.3838430852332014, 0.38723404293364666, 0.39029255319148937, 0.3918218086374567, 0.3855718088910935, 0.37726063867832754, 0.38457446846556154, 0.3799202131464126, 0.3916223404255319, 0.381449467958288, 0.38816489374383967, 0.39029255319148937, 0.3898936174017318, 0.3918882982527956, 0.386901596125136, 0.383244681231519, 0.3873005319148936, 0.38257978761449773, 0.3900265959983176, 0.38457446846556154, 0.3863696811047006, 0.3807845743412667, 0.3809840425531915, 0.38251329799915884, 0.38603723429618997, 0.38703457472172187, 0.3900265959983176, 0.38896276595744683, 0.38450797885022264, 0.38896276595744683, 0.3849734042553192, 0.386968085106383, 0.3873005319148936, 0.38882978736086093, 0.3932180854868382, 0.3920877664647204, 0.3910239364238495, 0.3815159575736269, 0.38863031914893614, 0.3812500003804552, 0.38563829787234044, 0.38703457472172187, 0.3815159575736269, 0.38331117021276595, 0.38523936208258286, 0.37925531952939134, 0.3846409574468085, 0.38158244718896583, 0.3775930854868382, 0.379587766337902, 0.375, 0.37726063867832754, 0.3702127660842652, 0.37679521263913907, 0.3902260642102424, 0.3746010642102424, 0.3855718088910935, 0.386901596125136, 0.3838430852332014, 0.3831781916161801, 0.3855718088910935, 0.3822473408059871, 0.3828457448076695, 0.3842420216570509, 0.38623670250811476, 0.3896276595744681, 0.390625, 0.3827127662110836, 0.3929521276595745]




epochs - 300


1
	Train Loss: 1.608 | Train Acc: 21.27%
	 Val. Loss: 1.594 |  Val. Acc: 26.48% 

2
	Train Loss: 1.605 | Train Acc: 22.23%
	 Val. Loss: 1.590 |  Val. Acc: 27.77% 

3
	Train Loss: 1.602 | Train Acc: 23.62%
	 Val. Loss: 1.587 |  Val. Acc: 27.74% 

4
	Train Loss: 1.603 | Train Acc: 23.69%
	 Val. Loss: 1.587 |  Val. Acc: 28.93% 

5
	Train Loss: 1.601 | Train Acc: 23.97%
	 Val. Loss: 1.588 |  Val. Acc: 29.15% 

6
	Train Loss: 1.600 | Train Acc: 24.67%
	 Val. Loss: 1.584 |  Val. Acc: 30.11% 

7
	Train Loss: 1.596 | Train Acc: 26.06%
	 Val. Loss: 1.581 |  Val. Acc: 30.74% 

8
	Train Loss: 1.593 | Train Acc: 26.29%
	 Val. Loss: 1.577 |  Val. Acc: 30.63% 

9
	Train Loss: 1.589 | Train Acc: 27.34%
	 Val. Loss: 1.574 |  Val. Acc: 29.98% 

10
	Train Loss: 1.583 | Train Acc: 27.85%
	 Val. Loss: 1.566 |  Val. Acc: 31.28% 

11
	Train Loss: 1.576 | Train Acc: 29.27%
	 Val. Loss: 1.560 |  Val. Acc: 31.17% 

12
	Train Loss: 1.567 | Train Acc: 30.40%
	 Val. Loss: 1.552 |  Val. Acc: 32.04% 

13
	Train Loss: 1.559 | Train Acc: 31.34%
	 Val. Loss: 1.539 |  Val. Acc: 34.15% 

14
	Train Loss: 1.549 | Train Acc: 33.04%
	 Val. Loss: 1.534 |  Val. Acc: 34.75% 

15
	Train Loss: 1.544 | Train Acc: 32.80%
	 Val. Loss: 1.532 |  Val. Acc: 35.48% 

16
	Train Loss: 1.538 | Train Acc: 33.83%
	 Val. Loss: 1.527 |  Val. Acc: 36.62% 

17
	Train Loss: 1.530 | Train Acc: 35.07%
	 Val. Loss: 1.524 |  Val. Acc: 36.28% 

18
	Train Loss: 1.526 | Train Acc: 34.90%
	 Val. Loss: 1.527 |  Val. Acc: 35.35% 

19
	Train Loss: 1.520 | Train Acc: 35.95%
	 Val. Loss: 1.527 |  Val. Acc: 35.56% 

20
	Train Loss: 1.513 | Train Acc: 37.35%
	 Val. Loss: 1.520 |  Val. Acc: 36.63% 

label distribution: actual vs pred: {0: {2: 58, 0: 508, 3: 56, 4: 18, 1: 150}, 4: {4: 17, 2: 25, 3: 39, 0: 201, 1: 64}, 1: {1: 302, 3: 189, 0: 240, 2: 50, 4: 4}, 3: {3: 188, 0: 82, 4: 5, 2: 24, 1: 165}, 2: {0: 313, 2: 67, 3: 55, 1: 139, 4: 5}}
	 pred vs actuals: {0: {2: 58, 0: 508, 3: 56, 4: 18, 1: 150}, 4: {4: 17, 2: 25, 3: 39, 0: 201, 1: 64}, 1: {1: 302, 3: 189, 0: 240, 2: 50, 4: 4}, 3: {3: 188, 0: 82, 4: 5, 2: 24, 1: 165}, 2: {0: 313, 2: 67, 3: 55, 1: 139, 4: 5}}
21
	Train Loss: 1.509 | Train Acc: 37.42%
	 Val. Loss: 1.521 |  Val. Acc: 36.29% 

22
	Train Loss: 1.504 | Train Acc: 38.03%
	 Val. Loss: 1.521 |  Val. Acc: 36.32% 

23
	Train Loss: 1.502 | Train Acc: 37.99%
	 Val. Loss: 1.525 |  Val. Acc: 35.96% 

24
	Train Loss: 1.493 | Train Acc: 39.14%
	 Val. Loss: 1.521 |  Val. Acc: 36.26% 

25
	Train Loss: 1.491 | Train Acc: 39.39%
	 Val. Loss: 1.513 |  Val. Acc: 37.39% 

26
	Train Loss: 1.488 | Train Acc: 40.10%
	 Val. Loss: 1.518 |  Val. Acc: 36.19% 

27
	Train Loss: 1.482 | Train Acc: 40.76%
	 Val. Loss: 1.515 |  Val. Acc: 37.19% 

28
	Train Loss: 1.476 | Train Acc: 41.14%
	 Val. Loss: 1.516 |  Val. Acc: 37.02% 

29
	Train Loss: 1.476 | Train Acc: 41.21%
	 Val. Loss: 1.515 |  Val. Acc: 37.19% 

30
	Train Loss: 1.467 | Train Acc: 42.15%
	 Val. Loss: 1.515 |  Val. Acc: 36.76% 

31
	Train Loss: 1.464 | Train Acc: 42.43%
	 Val. Loss: 1.517 |  Val. Acc: 36.76% 

32
	Train Loss: 1.459 | Train Acc: 43.19%
	 Val. Loss: 1.514 |  Val. Acc: 37.13% 

33
	Train Loss: 1.452 | Train Acc: 44.14%
	 Val. Loss: 1.521 |  Val. Acc: 36.03% 

34
	Train Loss: 1.453 | Train Acc: 43.47%
	 Val. Loss: 1.520 |  Val. Acc: 36.26% 

35
	Train Loss: 1.448 | Train Acc: 44.34%
	 Val. Loss: 1.519 |  Val. Acc: 36.20% 

36
	Train Loss: 1.438 | Train Acc: 45.64%
	 Val. Loss: 1.513 |  Val. Acc: 36.73% 

37
	Train Loss: 1.439 | Train Acc: 45.34%
	 Val. Loss: 1.517 |  Val. Acc: 36.40% 

38
	Train Loss: 1.436 | Train Acc: 45.42%
	 Val. Loss: 1.514 |  Val. Acc: 37.22% 

39
	Train Loss: 1.431 | Train Acc: 46.05%
	 Val. Loss: 1.515 |  Val. Acc: 36.82% 

40
	Train Loss: 1.429 | Train Acc: 46.57%
	 Val. Loss: 1.518 |  Val. Acc: 36.82% 

label distribution: actual vs pred: {0: {2: 96, 0: 466, 4: 54, 3: 47, 1: 127}, 4: {4: 33, 3: 23, 0: 204, 2: 39, 1: 47}, 1: {1: 298, 3: 189, 0: 188, 2: 93, 4: 17}, 3: {3: 198, 2: 40, 4: 13, 1: 151, 0: 62}, 2: {4: 26, 2: 95, 0: 266, 1: 138, 3: 54}}
	 pred vs actuals: {0: {2: 96, 0: 466, 4: 54, 3: 47, 1: 127}, 4: {4: 33, 3: 23, 0: 204, 2: 39, 1: 47}, 1: {1: 298, 3: 189, 0: 188, 2: 93, 4: 17}, 3: {3: 198, 2: 40, 4: 13, 1: 151, 0: 62}, 2: {4: 26, 2: 95, 0: 266, 1: 138, 3: 54}}
41
	Train Loss: 1.425 | Train Acc: 46.88%
	 Val. Loss: 1.516 |  Val. Acc: 36.79% 

42
	Train Loss: 1.421 | Train Acc: 47.21%
	 Val. Loss: 1.519 |  Val. Acc: 36.46% 

43
	Train Loss: 1.416 | Train Acc: 47.87%
	 Val. Loss: 1.517 |  Val. Acc: 37.06% 

44
	Train Loss: 1.410 | Train Acc: 48.49%
	 Val. Loss: 1.521 |  Val. Acc: 36.46% 

45
	Train Loss: 1.409 | Train Acc: 48.84%
	 Val. Loss: 1.510 |  Val. Acc: 37.97% 

46
	Train Loss: 1.401 | Train Acc: 49.59%
	 Val. Loss: 1.510 |  Val. Acc: 38.06% 

47
	Train Loss: 1.401 | Train Acc: 49.40%
	 Val. Loss: 1.510 |  Val. Acc: 38.16% 

48
	Train Loss: 1.399 | Train Acc: 49.53%
	 Val. Loss: 1.515 |  Val. Acc: 36.86% 

49
	Train Loss: 1.396 | Train Acc: 50.26%
	 Val. Loss: 1.516 |  Val. Acc: 37.21% 

50
	Train Loss: 1.394 | Train Acc: 50.24%
	 Val. Loss: 1.512 |  Val. Acc: 37.59% 

51
	Train Loss: 1.385 | Train Acc: 50.99%
	 Val. Loss: 1.509 |  Val. Acc: 37.93% 

52
	Train Loss: 1.383 | Train Acc: 51.63%
	 Val. Loss: 1.513 |  Val. Acc: 37.12% 

53
	Train Loss: 1.378 | Train Acc: 51.73%
	 Val. Loss: 1.513 |  Val. Acc: 37.02% 

54
	Train Loss: 1.374 | Train Acc: 52.18%
	 Val. Loss: 1.508 |  Val. Acc: 38.09% 

55
	Train Loss: 1.371 | Train Acc: 52.82%
	 Val. Loss: 1.512 |  Val. Acc: 37.15% 

56
	Train Loss: 1.368 | Train Acc: 53.13%
	 Val. Loss: 1.508 |  Val. Acc: 38.18% 

57
	Train Loss: 1.367 | Train Acc: 53.28%
	 Val. Loss: 1.512 |  Val. Acc: 37.48% 

58
	Train Loss: 1.361 | Train Acc: 53.96%
	 Val. Loss: 1.506 |  Val. Acc: 38.42% 

59
	Train Loss: 1.357 | Train Acc: 54.53%
	 Val. Loss: 1.509 |  Val. Acc: 37.96% 

60
	Train Loss: 1.356 | Train Acc: 54.25%
	 Val. Loss: 1.508 |  Val. Acc: 37.84% 

label distribution: actual vs pred: {0: {0: 493, 2: 112, 4: 61, 3: 32, 1: 92}, 4: {4: 47, 3: 20, 0: 206, 2: 41, 1: 32}, 1: {1: 291, 3: 164, 0: 202, 2: 109, 4: 19}, 3: {3: 183, 2: 46, 4: 14, 1: 157, 0: 64}, 2: {4: 34, 2: 100, 0: 279, 3: 48, 1: 118}}
	 pred vs actuals: {0: {0: 493, 2: 112, 4: 61, 3: 32, 1: 92}, 4: {4: 47, 3: 20, 0: 206, 2: 41, 1: 32}, 1: {1: 291, 3: 164, 0: 202, 2: 109, 4: 19}, 3: {3: 183, 2: 46, 4: 14, 1: 157, 0: 64}, 2: {4: 34, 2: 100, 0: 279, 3: 48, 1: 118}}
61
	Train Loss: 1.352 | Train Acc: 54.61%
	 Val. Loss: 1.503 |  Val. Acc: 38.81% 

62
	Train Loss: 1.345 | Train Acc: 55.56%
	 Val. Loss: 1.507 |  Val. Acc: 38.24% 

63
	Train Loss: 1.344 | Train Acc: 56.01%
	 Val. Loss: 1.509 |  Val. Acc: 37.79% 

64
	Train Loss: 1.346 | Train Acc: 55.26%
	 Val. Loss: 1.512 |  Val. Acc: 37.42% 

65
	Train Loss: 1.344 | Train Acc: 55.30%
	 Val. Loss: 1.508 |  Val. Acc: 37.80% 

66
	Train Loss: 1.341 | Train Acc: 55.79%
	 Val. Loss: 1.506 |  Val. Acc: 38.07% 

67
	Train Loss: 1.336 | Train Acc: 56.31%
	 Val. Loss: 1.515 |  Val. Acc: 37.18% 

68
	Train Loss: 1.335 | Train Acc: 56.17%
	 Val. Loss: 1.507 |  Val. Acc: 37.94% 

69
	Train Loss: 1.330 | Train Acc: 56.99%
	 Val. Loss: 1.505 |  Val. Acc: 38.77% 

70
	Train Loss: 1.326 | Train Acc: 57.42%
	 Val. Loss: 1.509 |  Val. Acc: 37.93% 

71
	Train Loss: 1.323 | Train Acc: 57.79%
	 Val. Loss: 1.510 |  Val. Acc: 37.81% 

72
	Train Loss: 1.321 | Train Acc: 57.70%
	 Val. Loss: 1.511 |  Val. Acc: 37.89% 

73
	Train Loss: 1.322 | Train Acc: 58.00%
	 Val. Loss: 1.509 |  Val. Acc: 38.32% 

74
	Train Loss: 1.317 | Train Acc: 58.18%
	 Val. Loss: 1.512 |  Val. Acc: 37.46% 

75
	Train Loss: 1.315 | Train Acc: 58.55%
	 Val. Loss: 1.511 |  Val. Acc: 37.48% 

76
	Train Loss: 1.315 | Train Acc: 58.59%
	 Val. Loss: 1.510 |  Val. Acc: 38.12% 

77
	Train Loss: 1.311 | Train Acc: 58.74%
	 Val. Loss: 1.507 |  Val. Acc: 38.30% 

78
	Train Loss: 1.310 | Train Acc: 59.01%
	 Val. Loss: 1.510 |  Val. Acc: 38.22% 

79
	Train Loss: 1.307 | Train Acc: 59.40%
	 Val. Loss: 1.511 |  Val. Acc: 38.09% 

80
	Train Loss: 1.300 | Train Acc: 60.11%
	 Val. Loss: 1.510 |  Val. Acc: 37.79% 

label distribution: actual vs pred: {0: {0: 453, 2: 124, 3: 26, 4: 75, 1: 112}, 4: {4: 62, 3: 18, 0: 195, 2: 37, 1: 34}, 1: {1: 310, 3: 177, 2: 103, 0: 169, 4: 26}, 3: {3: 182, 4: 17, 2: 46, 1: 168, 0: 51}, 2: {4: 41, 2: 110, 0: 252, 3: 41, 1: 135}}
	 pred vs actuals: {0: {0: 453, 2: 124, 3: 26, 4: 75, 1: 112}, 4: {4: 62, 3: 18, 0: 195, 2: 37, 1: 34}, 1: {1: 310, 3: 177, 2: 103, 0: 169, 4: 26}, 3: {3: 182, 4: 17, 2: 46, 1: 168, 0: 51}, 2: {4: 41, 2: 110, 0: 252, 3: 41, 1: 135}}
81
	Train Loss: 1.301 | Train Acc: 59.62%
	 Val. Loss: 1.511 |  Val. Acc: 38.03% 

82
	Train Loss: 1.296 | Train Acc: 60.55%
	 Val. Loss: 1.516 |  Val. Acc: 37.41% 

83
	Train Loss: 1.290 | Train Acc: 60.93%
	 Val. Loss: 1.511 |  Val. Acc: 37.61% 

84
	Train Loss: 1.297 | Train Acc: 60.36%
	 Val. Loss: 1.511 |  Val. Acc: 37.89% 

85
	Train Loss: 1.292 | Train Acc: 60.75%
	 Val. Loss: 1.511 |  Val. Acc: 37.65% 

86
	Train Loss: 1.289 | Train Acc: 61.18%
	 Val. Loss: 1.505 |  Val. Acc: 38.99% 

87
	Train Loss: 1.283 | Train Acc: 61.94%
	 Val. Loss: 1.509 |  Val. Acc: 37.93% 

88
	Train Loss: 1.285 | Train Acc: 61.61%
	 Val. Loss: 1.506 |  Val. Acc: 38.72% 

89
	Train Loss: 1.279 | Train Acc: 62.20%
	 Val. Loss: 1.507 |  Val. Acc: 38.32% 

90
	Train Loss: 1.282 | Train Acc: 61.71%
	 Val. Loss: 1.508 |  Val. Acc: 38.08% 

91
	Train Loss: 1.278 | Train Acc: 61.75%
	 Val. Loss: 1.509 |  Val. Acc: 38.11% 

92
	Train Loss: 1.278 | Train Acc: 61.91%
	 Val. Loss: 1.506 |  Val. Acc: 38.36% 

93
	Train Loss: 1.273 | Train Acc: 62.65%
	 Val. Loss: 1.512 |  Val. Acc: 38.32% 

94
	Train Loss: 1.273 | Train Acc: 62.63%
	 Val. Loss: 1.507 |  Val. Acc: 38.09% 

95
	Train Loss: 1.272 | Train Acc: 62.85%
	 Val. Loss: 1.510 |  Val. Acc: 37.92% 

96
	Train Loss: 1.269 | Train Acc: 63.04%
	 Val. Loss: 1.510 |  Val. Acc: 38.09% 

97
	Train Loss: 1.266 | Train Acc: 62.94%
	 Val. Loss: 1.514 |  Val. Acc: 37.78% 

98
	Train Loss: 1.264 | Train Acc: 63.22%
	 Val. Loss: 1.503 |  Val. Acc: 38.63% 

99
	Train Loss: 1.260 | Train Acc: 64.23%
	 Val. Loss: 1.510 |  Val. Acc: 38.09% 

100
	Train Loss: 1.259 | Train Acc: 63.61%
	 Val. Loss: 1.512 |  Val. Acc: 38.01% 

label distribution: actual vs pred: {0: {0: 427, 4: 118, 2: 108, 3: 37, 1: 100}, 4: {4: 92, 3: 15, 0: 170, 2: 35, 1: 34}, 1: {2: 104, 3: 194, 1: 304, 0: 145, 4: 38}, 3: {3: 207, 4: 25, 2: 36, 1: 155, 0: 41}, 2: {4: 65, 2: 100, 1: 128, 3: 54, 0: 232}}
	 pred vs actuals: {0: {0: 427, 4: 118, 2: 108, 3: 37, 1: 100}, 4: {4: 92, 3: 15, 0: 170, 2: 35, 1: 34}, 1: {2: 104, 3: 194, 1: 304, 0: 145, 4: 38}, 3: {3: 207, 4: 25, 2: 36, 1: 155, 0: 41}, 2: {4: 65, 2: 100, 1: 128, 3: 54, 0: 232}}
101
	Train Loss: 1.258 | Train Acc: 64.27%
	 Val. Loss: 1.514 |  Val. Acc: 37.84% 

102
	Train Loss: 1.257 | Train Acc: 64.57%
	 Val. Loss: 1.509 |  Val. Acc: 38.31% 

103
	Train Loss: 1.257 | Train Acc: 64.64%
	 Val. Loss: 1.507 |  Val. Acc: 38.49% 

104
	Train Loss: 1.248 | Train Acc: 65.00%
	 Val. Loss: 1.512 |  Val. Acc: 38.42% 

105
	Train Loss: 1.254 | Train Acc: 64.24%
	 Val. Loss: 1.507 |  Val. Acc: 38.73% 

106
	Train Loss: 1.249 | Train Acc: 65.03%
	 Val. Loss: 1.506 |  Val. Acc: 38.56% 

107
	Train Loss: 1.248 | Train Acc: 65.17%
	 Val. Loss: 1.505 |  Val. Acc: 38.76% 

108
	Train Loss: 1.245 | Train Acc: 65.56%
	 Val. Loss: 1.503 |  Val. Acc: 38.66% 

109
	Train Loss: 1.244 | Train Acc: 65.10%
	 Val. Loss: 1.511 |  Val. Acc: 37.99% 

110
	Train Loss: 1.243 | Train Acc: 65.88%
	 Val. Loss: 1.512 |  Val. Acc: 38.22% 

111
	Train Loss: 1.246 | Train Acc: 65.26%
	 Val. Loss: 1.508 |  Val. Acc: 38.05% 

112
	Train Loss: 1.244 | Train Acc: 65.63%
	 Val. Loss: 1.513 |  Val. Acc: 37.99% 

113
	Train Loss: 1.236 | Train Acc: 66.59%
	 Val. Loss: 1.503 |  Val. Acc: 39.04% 

114
	Train Loss: 1.234 | Train Acc: 66.83%
	 Val. Loss: 1.504 |  Val. Acc: 39.24% 

115
	Train Loss: 1.233 | Train Acc: 66.49%
	 Val. Loss: 1.510 |  Val. Acc: 37.96% 

116
	Train Loss: 1.232 | Train Acc: 67.04%
	 Val. Loss: 1.509 |  Val. Acc: 38.58% 

117
	Train Loss: 1.233 | Train Acc: 67.00%
	 Val. Loss: 1.510 |  Val. Acc: 38.29% 

118
	Train Loss: 1.231 | Train Acc: 66.34%
	 Val. Loss: 1.507 |  Val. Acc: 38.40% 

119
	Train Loss: 1.228 | Train Acc: 66.98%
	 Val. Loss: 1.509 |  Val. Acc: 38.32% 

120
	Train Loss: 1.225 | Train Acc: 67.11%
	 Val. Loss: 1.508 |  Val. Acc: 38.42% 

label distribution: actual vs pred: {0: {2: 123, 0: 432, 3: 32, 4: 95, 1: 108}, 4: {4: 85, 0: 173, 2: 36, 1: 37, 3: 15}, 1: {1: 312, 3: 177, 0: 156, 2: 114, 4: 26}, 3: {3: 199, 4: 25, 2: 39, 1: 156, 0: 45}, 2: {4: 51, 2: 110, 1: 132, 3: 50, 0: 236}}
	 pred vs actuals: {0: {2: 123, 0: 432, 3: 32, 4: 95, 1: 108}, 4: {4: 85, 0: 173, 2: 36, 1: 37, 3: 15}, 1: {1: 312, 3: 177, 0: 156, 2: 114, 4: 26}, 3: {3: 199, 4: 25, 2: 39, 1: 156, 0: 45}, 2: {4: 51, 2: 110, 1: 132, 3: 50, 0: 236}}
121
	Train Loss: 1.227 | Train Acc: 67.08%
	 Val. Loss: 1.508 |  Val. Acc: 38.62% 

122
	Train Loss: 1.230 | Train Acc: 66.89%
	 Val. Loss: 1.504 |  Val. Acc: 38.90% 

123
	Train Loss: 1.228 | Train Acc: 67.25%
	 Val. Loss: 1.505 |  Val. Acc: 38.79% 

124
	Train Loss: 1.224 | Train Acc: 67.62%
	 Val. Loss: 1.513 |  Val. Acc: 37.82% 

125
	Train Loss: 1.220 | Train Acc: 67.95%
	 Val. Loss: 1.508 |  Val. Acc: 38.28% 

126
	Train Loss: 1.219 | Train Acc: 67.94%
	 Val. Loss: 1.508 |  Val. Acc: 38.72% 

127
	Train Loss: 1.215 | Train Acc: 67.94%
	 Val. Loss: 1.504 |  Val. Acc: 38.90% 

128
	Train Loss: 1.219 | Train Acc: 67.82%
	 Val. Loss: 1.502 |  Val. Acc: 39.26% 

129
	Train Loss: 1.221 | Train Acc: 67.34%
	 Val. Loss: 1.502 |  Val. Acc: 39.26% 

130
	Train Loss: 1.211 | Train Acc: 68.69%
	 Val. Loss: 1.506 |  Val. Acc: 38.76% 

131
	Train Loss: 1.217 | Train Acc: 68.21%
	 Val. Loss: 1.509 |  Val. Acc: 38.22% 

132
	Train Loss: 1.214 | Train Acc: 68.24%
	 Val. Loss: 1.505 |  Val. Acc: 39.10% 

133
	Train Loss: 1.209 | Train Acc: 68.91%
	 Val. Loss: 1.510 |  Val. Acc: 38.62% 

134
	Train Loss: 1.212 | Train Acc: 68.58%
	 Val. Loss: 1.505 |  Val. Acc: 38.65% 

135
	Train Loss: 1.214 | Train Acc: 68.93%
	 Val. Loss: 1.507 |  Val. Acc: 38.56% 

136
	Train Loss: 1.206 | Train Acc: 68.98%
	 Val. Loss: 1.506 |  Val. Acc: 38.71% 

137
	Train Loss: 1.205 | Train Acc: 69.25%
	 Val. Loss: 1.508 |  Val. Acc: 38.42% 

138
	Train Loss: 1.207 | Train Acc: 68.97%
	 Val. Loss: 1.503 |  Val. Acc: 38.82% 

139
	Train Loss: 1.205 | Train Acc: 69.86%
	 Val. Loss: 1.499 |  Val. Acc: 39.74% 

140
	Train Loss: 1.203 | Train Acc: 69.59%
	 Val. Loss: 1.505 |  Val. Acc: 39.03% 

label distribution: actual vs pred: {0: {2: 106, 0: 428, 3: 33, 4: 106, 1: 117}, 4: {4: 95, 0: 169, 2: 33, 1: 35, 3: 14}, 1: {1: 340, 3: 164, 0: 154, 2: 103, 4: 24}, 3: {3: 185, 4: 20, 2: 33, 1: 181, 0: 45}, 2: {4: 51, 2: 104, 1: 143, 0: 237, 3: 44}}
	 pred vs actuals: {0: {2: 106, 0: 428, 3: 33, 4: 106, 1: 117}, 4: {4: 95, 0: 169, 2: 33, 1: 35, 3: 14}, 1: {1: 340, 3: 164, 0: 154, 2: 103, 4: 24}, 3: {3: 185, 4: 20, 2: 33, 1: 181, 0: 45}, 2: {4: 51, 2: 104, 1: 143, 0: 237, 3: 44}}
141
	Train Loss: 1.199 | Train Acc: 69.84%
	 Val. Loss: 1.505 |  Val. Acc: 39.12% 

142
	Train Loss: 1.200 | Train Acc: 69.71%
	 Val. Loss: 1.504 |  Val. Acc: 39.12% 

143
	Train Loss: 1.202 | Train Acc: 69.41%
	 Val. Loss: 1.504 |  Val. Acc: 39.43% 

144
	Train Loss: 1.195 | Train Acc: 70.14%
	 Val. Loss: 1.504 |  Val. Acc: 38.89% 

145
	Train Loss: 1.196 | Train Acc: 70.52%
	 Val. Loss: 1.496 |  Val. Acc: 39.65% 

146
	Train Loss: 1.199 | Train Acc: 69.84%
	 Val. Loss: 1.502 |  Val. Acc: 39.25% 

147
	Train Loss: 1.194 | Train Acc: 70.63%
	 Val. Loss: 1.498 |  Val. Acc: 39.43% 

148
	Train Loss: 1.194 | Train Acc: 70.00%
	 Val. Loss: 1.494 |  Val. Acc: 39.99% 

149
	Train Loss: 1.197 | Train Acc: 69.92%
	 Val. Loss: 1.498 |  Val. Acc: 39.55% 

150
	Train Loss: 1.192 | Train Acc: 70.68%
	 Val. Loss: 1.504 |  Val. Acc: 38.92% 

151
	Train Loss: 1.188 | Train Acc: 71.24%
	 Val. Loss: 1.497 |  Val. Acc: 40.35% 

152
	Train Loss: 1.185 | Train Acc: 71.63%
	 Val. Loss: 1.506 |  Val. Acc: 38.78% 

153
	Train Loss: 1.190 | Train Acc: 71.02%
	 Val. Loss: 1.501 |  Val. Acc: 39.32% 

154
	Train Loss: 1.186 | Train Acc: 71.28%
	 Val. Loss: 1.502 |  Val. Acc: 39.41% 

155
	Train Loss: 1.187 | Train Acc: 70.75%
	 Val. Loss: 1.497 |  Val. Acc: 39.59% 

156
	Train Loss: 1.180 | Train Acc: 71.61%
	 Val. Loss: 1.496 |  Val. Acc: 39.86% 

157
	Train Loss: 1.183 | Train Acc: 71.43%
	 Val. Loss: 1.496 |  Val. Acc: 40.02% 

158
	Train Loss: 1.184 | Train Acc: 71.05%
	 Val. Loss: 1.499 |  Val. Acc: 39.56% 

159
	Train Loss: 1.187 | Train Acc: 71.09%
	 Val. Loss: 1.499 |  Val. Acc: 39.93% 

160
	Train Loss: 1.185 | Train Acc: 71.25%
	 Val. Loss: 1.500 |  Val. Acc: 39.56% 

label distribution: actual vs pred: {0: {2: 123, 0: 430, 3: 32, 4: 105, 1: 100}, 4: {4: 96, 0: 171, 2: 35, 1: 30, 3: 14}, 1: {1: 321, 3: 164, 0: 151, 2: 122, 4: 27}, 3: {3: 190, 2: 45, 4: 22, 1: 168, 0: 39}, 2: {4: 58, 2: 131, 1: 124, 0: 228, 3: 38}}
	 pred vs actuals: {0: {2: 123, 0: 430, 3: 32, 4: 105, 1: 100}, 4: {4: 96, 0: 171, 2: 35, 1: 30, 3: 14}, 1: {1: 321, 3: 164, 0: 151, 2: 122, 4: 27}, 3: {3: 190, 2: 45, 4: 22, 1: 168, 0: 39}, 2: {4: 58, 2: 131, 1: 124, 0: 228, 3: 38}}
161
	Train Loss: 1.181 | Train Acc: 71.83%
	 Val. Loss: 1.504 |  Val. Acc: 38.63% 

162
	Train Loss: 1.178 | Train Acc: 72.11%
	 Val. Loss: 1.500 |  Val. Acc: 39.59% 

163
	Train Loss: 1.184 | Train Acc: 71.43%
	 Val. Loss: 1.494 |  Val. Acc: 40.23% 

164
	Train Loss: 1.181 | Train Acc: 71.45%
	 Val. Loss: 1.501 |  Val. Acc: 39.32% 

165
	Train Loss: 1.179 | Train Acc: 71.76%
	 Val. Loss: 1.497 |  Val. Acc: 39.79% 

166
	Train Loss: 1.175 | Train Acc: 72.26%
	 Val. Loss: 1.497 |  Val. Acc: 39.73% 

167
	Train Loss: 1.181 | Train Acc: 71.34%
	 Val. Loss: 1.502 |  Val. Acc: 39.03% 

168
	Train Loss: 1.174 | Train Acc: 72.22%
	 Val. Loss: 1.502 |  Val. Acc: 39.26% 

169
	Train Loss: 1.168 | Train Acc: 73.36%
	 Val. Loss: 1.499 |  Val. Acc: 39.79% 

170
	Train Loss: 1.174 | Train Acc: 72.58%
	 Val. Loss: 1.500 |  Val. Acc: 39.52% 

171
	Train Loss: 1.175 | Train Acc: 72.11%
	 Val. Loss: 1.501 |  Val. Acc: 39.09% 

172
	Train Loss: 1.170 | Train Acc: 72.82%
	 Val. Loss: 1.500 |  Val. Acc: 39.02% 

173
	Train Loss: 1.173 | Train Acc: 72.33%
	 Val. Loss: 1.503 |  Val. Acc: 38.98% 

174
	Train Loss: 1.165 | Train Acc: 73.10%
	 Val. Loss: 1.499 |  Val. Acc: 39.65% 

175
	Train Loss: 1.169 | Train Acc: 72.98%
	 Val. Loss: 1.499 |  Val. Acc: 39.55% 

176
	Train Loss: 1.166 | Train Acc: 73.30%
	 Val. Loss: 1.501 |  Val. Acc: 39.22% 

177
	Train Loss: 1.168 | Train Acc: 72.83%
	 Val. Loss: 1.496 |  Val. Acc: 39.92% 

178
	Train Loss: 1.168 | Train Acc: 73.02%
	 Val. Loss: 1.500 |  Val. Acc: 39.51% 

179
	Train Loss: 1.166 | Train Acc: 73.30%
	 Val. Loss: 1.497 |  Val. Acc: 39.72% 

180
	Train Loss: 1.164 | Train Acc: 73.23%
	 Val. Loss: 1.498 |  Val. Acc: 39.70% 

label distribution: actual vs pred: {0: {2: 116, 0: 432, 3: 29, 4: 105, 1: 108}, 4: {4: 94, 0: 170, 2: 27, 1: 37, 3: 18}, 1: {1: 344, 3: 156, 0: 153, 2: 106, 4: 26}, 3: {3: 191, 4: 23, 2: 36, 1: 173, 0: 41}, 2: {4: 57, 2: 109, 1: 135, 3: 47, 0: 231}}
	 pred vs actuals: {0: {2: 116, 0: 432, 3: 29, 4: 105, 1: 108}, 4: {4: 94, 0: 170, 2: 27, 1: 37, 3: 18}, 1: {1: 344, 3: 156, 0: 153, 2: 106, 4: 26}, 3: {3: 191, 4: 23, 2: 36, 1: 173, 0: 41}, 2: {4: 57, 2: 109, 1: 135, 3: 47, 0: 231}}
181
	Train Loss: 1.163 | Train Acc: 73.59%
	 Val. Loss: 1.499 |  Val. Acc: 39.56% 

182
	Train Loss: 1.162 | Train Acc: 73.52%
	 Val. Loss: 1.499 |  Val. Acc: 39.49% 

183
	Train Loss: 1.164 | Train Acc: 73.29%
	 Val. Loss: 1.498 |  Val. Acc: 39.99% 

184
	Train Loss: 1.162 | Train Acc: 73.38%
	 Val. Loss: 1.491 |  Val. Acc: 40.80% 

185
	Train Loss: 1.165 | Train Acc: 73.32%
	 Val. Loss: 1.494 |  Val. Acc: 39.96% 

186
	Train Loss: 1.163 | Train Acc: 73.21%
	 Val. Loss: 1.498 |  Val. Acc: 39.59% 

187
	Train Loss: 1.160 | Train Acc: 73.60%
	 Val. Loss: 1.502 |  Val. Acc: 39.16% 

188
	Train Loss: 1.163 | Train Acc: 72.83%
	 Val. Loss: 1.499 |  Val. Acc: 39.63% 

189
	Train Loss: 1.161 | Train Acc: 73.30%
	 Val. Loss: 1.501 |  Val. Acc: 39.47% 

190
	Train Loss: 1.159 | Train Acc: 73.40%
	 Val. Loss: 1.500 |  Val. Acc: 39.57% 

191
	Train Loss: 1.156 | Train Acc: 74.08%
	 Val. Loss: 1.500 |  Val. Acc: 39.32% 

192
	Train Loss: 1.159 | Train Acc: 73.44%
	 Val. Loss: 1.497 |  Val. Acc: 39.76% 

193
	Train Loss: 1.153 | Train Acc: 74.61%
	 Val. Loss: 1.499 |  Val. Acc: 39.49% 

194
	Train Loss: 1.155 | Train Acc: 74.13%
	 Val. Loss: 1.499 |  Val. Acc: 39.73% 

195
	Train Loss: 1.155 | Train Acc: 73.59%
	 Val. Loss: 1.501 |  Val. Acc: 39.26% 

196
	Train Loss: 1.151 | Train Acc: 74.56%
	 Val. Loss: 1.498 |  Val. Acc: 39.76% 

197
	Train Loss: 1.155 | Train Acc: 73.92%
	 Val. Loss: 1.495 |  Val. Acc: 39.87% 

198
	Train Loss: 1.156 | Train Acc: 73.74%
	 Val. Loss: 1.498 |  Val. Acc: 39.87% 

199
	Train Loss: 1.153 | Train Acc: 74.30%
	 Val. Loss: 1.501 |  Val. Acc: 39.26% 

200
	Train Loss: 1.154 | Train Acc: 74.57%
	 Val. Loss: 1.497 |  Val. Acc: 39.89% 

label distribution: actual vs pred: {0: {2: 121, 0: 438, 3: 24, 4: 96, 1: 111}, 4: {4: 89, 0: 179, 3: 15, 2: 31, 1: 32}, 1: {1: 353, 3: 149, 0: 146, 2: 112, 4: 25}, 3: {3: 187, 4: 19, 2: 38, 1: 172, 0: 48}, 2: {4: 54, 1: 138, 2: 111, 3: 34, 0: 242}}
	 pred vs actuals: {0: {2: 121, 0: 438, 3: 24, 4: 96, 1: 111}, 4: {4: 89, 0: 179, 3: 15, 2: 31, 1: 32}, 1: {1: 353, 3: 149, 0: 146, 2: 112, 4: 25}, 3: {3: 187, 4: 19, 2: 38, 1: 172, 0: 48}, 2: {4: 54, 1: 138, 2: 111, 3: 34, 0: 242}}
201
	Train Loss: 1.149 | Train Acc: 74.43%
	 Val. Loss: 1.497 |  Val. Acc: 39.95% 

202
	Train Loss: 1.154 | Train Acc: 74.27%
	 Val. Loss: 1.495 |  Val. Acc: 40.19% 

203
	Train Loss: 1.156 | Train Acc: 74.42%
	 Val. Loss: 1.497 |  Val. Acc: 39.99% 

204
	Train Loss: 1.150 | Train Acc: 74.60%
	 Val. Loss: 1.498 |  Val. Acc: 39.85% 

205
	Train Loss: 1.152 | Train Acc: 74.33%
	 Val. Loss: 1.496 |  Val. Acc: 39.69% 

206
	Train Loss: 1.149 | Train Acc: 74.60%
	 Val. Loss: 1.493 |  Val. Acc: 40.09% 

207
	Train Loss: 1.148 | Train Acc: 75.13%
	 Val. Loss: 1.494 |  Val. Acc: 40.36% 

208
	Train Loss: 1.151 | Train Acc: 73.99%
	 Val. Loss: 1.501 |  Val. Acc: 39.22% 

209
	Train Loss: 1.148 | Train Acc: 74.61%
	 Val. Loss: 1.495 |  Val. Acc: 40.11% 

210
	Train Loss: 1.150 | Train Acc: 74.60%
	 Val. Loss: 1.494 |  Val. Acc: 39.86% 

211
	Train Loss: 1.147 | Train Acc: 75.21%
	 Val. Loss: 1.498 |  Val. Acc: 39.63% 

212
	Train Loss: 1.146 | Train Acc: 75.34%
	 Val. Loss: 1.496 |  Val. Acc: 39.66% 

213
	Train Loss: 1.146 | Train Acc: 75.07%
	 Val. Loss: 1.497 |  Val. Acc: 39.82% 

214
	Train Loss: 1.145 | Train Acc: 75.20%
	 Val. Loss: 1.498 |  Val. Acc: 39.56% 

215
	Train Loss: 1.145 | Train Acc: 75.15%
	 Val. Loss: 1.496 |  Val. Acc: 40.03% 

216
	Train Loss: 1.144 | Train Acc: 74.72%
	 Val. Loss: 1.502 |  Val. Acc: 39.39% 

217
	Train Loss: 1.141 | Train Acc: 75.42%
	 Val. Loss: 1.502 |  Val. Acc: 39.29% 

218
	Train Loss: 1.138 | Train Acc: 76.16%
	 Val. Loss: 1.504 |  Val. Acc: 39.19% 

219
	Train Loss: 1.142 | Train Acc: 75.64%
	 Val. Loss: 1.500 |  Val. Acc: 39.42% 

220
	Train Loss: 1.141 | Train Acc: 75.31%
	 Val. Loss: 1.498 |  Val. Acc: 39.93% 

label distribution: actual vs pred: {0: {2: 114, 4: 94, 0: 428, 1: 128, 3: 26}, 4: {4: 87, 0: 178, 3: 9, 2: 33, 1: 39}, 1: {1: 375, 3: 126, 0: 143, 2: 123, 4: 18}, 3: {3: 171, 2: 33, 4: 16, 1: 202, 0: 42}, 2: {4: 46, 1: 146, 2: 118, 3: 35, 0: 234}}
	 pred vs actuals: {0: {2: 114, 4: 94, 0: 428, 1: 128, 3: 26}, 4: {4: 87, 0: 178, 3: 9, 2: 33, 1: 39}, 1: {1: 375, 3: 126, 0: 143, 2: 123, 4: 18}, 3: {3: 171, 2: 33, 4: 16, 1: 202, 0: 42}, 2: {4: 46, 1: 146, 2: 118, 3: 35, 0: 234}}
221
	Train Loss: 1.144 | Train Acc: 74.99%
	 Val. Loss: 1.502 |  Val. Acc: 39.08% 

222
	Train Loss: 1.143 | Train Acc: 75.23%
	 Val. Loss: 1.498 |  Val. Acc: 39.74% 

223
	Train Loss: 1.141 | Train Acc: 75.44%
	 Val. Loss: 1.493 |  Val. Acc: 40.26% 

224
	Train Loss: 1.137 | Train Acc: 75.86%
	 Val. Loss: 1.502 |  Val. Acc: 39.39% 

225
	Train Loss: 1.139 | Train Acc: 75.58%
	 Val. Loss: 1.500 |  Val. Acc: 39.49% 

226
	Train Loss: 1.138 | Train Acc: 75.93%
	 Val. Loss: 1.502 |  Val. Acc: 39.43% 

227
	Train Loss: 1.134 | Train Acc: 76.33%
	 Val. Loss: 1.500 |  Val. Acc: 39.42% 

228
	Train Loss: 1.140 | Train Acc: 75.67%
	 Val. Loss: 1.499 |  Val. Acc: 39.83% 

229
	Train Loss: 1.138 | Train Acc: 75.65%
	 Val. Loss: 1.503 |  Val. Acc: 39.29% 

230
	Train Loss: 1.134 | Train Acc: 76.18%
	 Val. Loss: 1.499 |  Val. Acc: 39.65% 

231
	Train Loss: 1.137 | Train Acc: 75.82%
	 Val. Loss: 1.505 |  Val. Acc: 38.68% 

232
	Train Loss: 1.135 | Train Acc: 75.95%
	 Val. Loss: 1.500 |  Val. Acc: 39.39% 

233
	Train Loss: 1.137 | Train Acc: 75.82%
	 Val. Loss: 1.503 |  Val. Acc: 39.15% 

234
	Train Loss: 1.138 | Train Acc: 75.31%
	 Val. Loss: 1.504 |  Val. Acc: 39.29% 

235
	Train Loss: 1.135 | Train Acc: 75.94%
	 Val. Loss: 1.500 |  Val. Acc: 39.72% 

236
	Train Loss: 1.137 | Train Acc: 75.60%
	 Val. Loss: 1.502 |  Val. Acc: 39.55% 

237
	Train Loss: 1.136 | Train Acc: 75.74%
	 Val. Loss: 1.501 |  Val. Acc: 39.52% 

238
	Train Loss: 1.132 | Train Acc: 76.45%
	 Val. Loss: 1.499 |  Val. Acc: 39.55% 

239
	Train Loss: 1.133 | Train Acc: 76.47%
	 Val. Loss: 1.500 |  Val. Acc: 39.75% 

240
	Train Loss: 1.133 | Train Acc: 75.93%
	 Val. Loss: 1.497 |  Val. Acc: 40.05% 

label distribution: actual vs pred: {0: {2: 115, 4: 106, 0: 436, 3: 30, 1: 103}, 4: {4: 96, 0: 172, 3: 13, 2: 31, 1: 34}, 1: {2: 123, 3: 153, 1: 338, 0: 144, 4: 27}, 3: {3: 196, 4: 20, 2: 39, 1: 167, 0: 42}, 2: {4: 55, 1: 131, 2: 119, 3: 41, 0: 233}}
	 pred vs actuals: {0: {2: 115, 4: 106, 0: 436, 3: 30, 1: 103}, 4: {4: 96, 0: 172, 3: 13, 2: 31, 1: 34}, 1: {2: 123, 3: 153, 1: 338, 0: 144, 4: 27}, 3: {3: 196, 4: 20, 2: 39, 1: 167, 0: 42}, 2: {4: 55, 1: 131, 2: 119, 3: 41, 0: 233}}
241
	Train Loss: 1.132 | Train Acc: 75.97%
	 Val. Loss: 1.499 |  Val. Acc: 39.73% 

242
	Train Loss: 1.131 | Train Acc: 76.25%
	 Val. Loss: 1.502 |  Val. Acc: 39.45% 

243
	Train Loss: 1.131 | Train Acc: 76.23%
	 Val. Loss: 1.497 |  Val. Acc: 40.19% 

244
	Train Loss: 1.131 | Train Acc: 76.20%
	 Val. Loss: 1.500 |  Val. Acc: 39.61% 

245
	Train Loss: 1.128 | Train Acc: 76.49%
	 Val. Loss: 1.502 |  Val. Acc: 39.55% 

246
	Train Loss: 1.130 | Train Acc: 76.29%
	 Val. Loss: 1.502 |  Val. Acc: 39.42% 

247
	Train Loss: 1.134 | Train Acc: 75.95%
	 Val. Loss: 1.500 |  Val. Acc: 39.72% 

248
	Train Loss: 1.126 | Train Acc: 76.82%
	 Val. Loss: 1.500 |  Val. Acc: 39.79% 

249
	Train Loss: 1.132 | Train Acc: 76.25%
	 Val. Loss: 1.499 |  Val. Acc: 39.76% 

250
	Train Loss: 1.134 | Train Acc: 76.17%
	 Val. Loss: 1.497 |  Val. Acc: 39.99% 

251
	Train Loss: 1.132 | Train Acc: 76.15%
	 Val. Loss: 1.499 |  Val. Acc: 39.55% 

252
	Train Loss: 1.126 | Train Acc: 76.71%
	 Val. Loss: 1.497 |  Val. Acc: 39.82% 

253
	Train Loss: 1.130 | Train Acc: 76.35%
	 Val. Loss: 1.501 |  Val. Acc: 39.29% 

254
	Train Loss: 1.130 | Train Acc: 76.29%
	 Val. Loss: 1.503 |  Val. Acc: 39.02% 

255
	Train Loss: 1.126 | Train Acc: 77.09%
	 Val. Loss: 1.505 |  Val. Acc: 38.68% 

256
	Train Loss: 1.127 | Train Acc: 76.85%
	 Val. Loss: 1.499 |  Val. Acc: 39.56% 

257
	Train Loss: 1.123 | Train Acc: 77.22%
	 Val. Loss: 1.504 |  Val. Acc: 38.82% 

258
	Train Loss: 1.126 | Train Acc: 77.16%
	 Val. Loss: 1.502 |  Val. Acc: 39.22% 

259
	Train Loss: 1.127 | Train Acc: 76.36%
	 Val. Loss: 1.504 |  Val. Acc: 38.95% 

260
	Train Loss: 1.126 | Train Acc: 77.31%
	 Val. Loss: 1.501 |  Val. Acc: 39.05% 

label distribution: actual vs pred: {0: {2: 120, 4: 111, 0: 400, 3: 24, 1: 135}, 4: {4: 97, 0: 167, 2: 32, 1: 39, 3: 11}, 1: {1: 364, 3: 144, 0: 131, 2: 122, 4: 24}, 3: {3: 175, 4: 18, 2: 35, 1: 200, 0: 36}, 2: {4: 59, 1: 149, 2: 121, 3: 35, 0: 215}}
	 pred vs actuals: {0: {2: 120, 4: 111, 0: 400, 3: 24, 1: 135}, 4: {4: 97, 0: 167, 2: 32, 1: 39, 3: 11}, 1: {1: 364, 3: 144, 0: 131, 2: 122, 4: 24}, 3: {3: 175, 4: 18, 2: 35, 1: 200, 0: 36}, 2: {4: 59, 1: 149, 2: 121, 3: 35, 0: 215}}
261
	Train Loss: 1.128 | Train Acc: 76.68%
	 Val. Loss: 1.501 |  Val. Acc: 39.26% 

262
	Train Loss: 1.121 | Train Acc: 77.46%
	 Val. Loss: 1.497 |  Val. Acc: 39.85% 

263
	Train Loss: 1.124 | Train Acc: 76.94%
	 Val. Loss: 1.508 |  Val. Acc: 38.42% 

264
	Train Loss: 1.122 | Train Acc: 76.98%
	 Val. Loss: 1.498 |  Val. Acc: 39.59% 

265
	Train Loss: 1.123 | Train Acc: 77.17%
	 Val. Loss: 1.500 |  Val. Acc: 39.36% 

266
	Train Loss: 1.125 | Train Acc: 76.91%
	 Val. Loss: 1.500 |  Val. Acc: 39.49% 

267
	Train Loss: 1.126 | Train Acc: 77.02%
	 Val. Loss: 1.498 |  Val. Acc: 39.49% 

268
	Train Loss: 1.127 | Train Acc: 76.71%
	 Val. Loss: 1.497 |  Val. Acc: 39.79% 

269
	Train Loss: 1.121 | Train Acc: 77.59%
	 Val. Loss: 1.495 |  Val. Acc: 40.22% 

270
	Train Loss: 1.120 | Train Acc: 77.29%
	 Val. Loss: 1.500 |  Val. Acc: 39.63% 

271
	Train Loss: 1.120 | Train Acc: 77.12%
	 Val. Loss: 1.497 |  Val. Acc: 39.86% 

272
	Train Loss: 1.118 | Train Acc: 77.98%
	 Val. Loss: 1.497 |  Val. Acc: 39.93% 

273
	Train Loss: 1.118 | Train Acc: 78.18%
	 Val. Loss: 1.503 |  Val. Acc: 38.89% 

274
	Train Loss: 1.122 | Train Acc: 76.85%
	 Val. Loss: 1.500 |  Val. Acc: 39.39% 

275
	Train Loss: 1.119 | Train Acc: 77.88%
	 Val. Loss: 1.504 |  Val. Acc: 39.08% 

276
	Train Loss: 1.116 | Train Acc: 77.88%
	 Val. Loss: 1.498 |  Val. Acc: 39.79% 

277
	Train Loss: 1.116 | Train Acc: 78.26%
	 Val. Loss: 1.497 |  Val. Acc: 39.75% 

278
	Train Loss: 1.115 | Train Acc: 77.92%
	 Val. Loss: 1.495 |  Val. Acc: 39.85% 

279
	Train Loss: 1.121 | Train Acc: 77.34%
	 Val. Loss: 1.499 |  Val. Acc: 39.29% 

280
	Train Loss: 1.115 | Train Acc: 77.68%
	 Val. Loss: 1.500 |  Val. Acc: 39.55% 

label distribution: actual vs pred: {0: {2: 112, 4: 107, 0: 427, 3: 29, 1: 115}, 4: {4: 92, 0: 172, 2: 36, 1: 30, 3: 16}, 1: {2: 114, 3: 149, 1: 357, 0: 140, 4: 25}, 3: {3: 181, 4: 21, 2: 35, 1: 188, 0: 39}, 2: {4: 57, 1: 134, 2: 113, 3: 41, 0: 234}}
	 pred vs actuals: {0: {2: 112, 4: 107, 0: 427, 3: 29, 1: 115}, 4: {4: 92, 0: 172, 2: 36, 1: 30, 3: 16}, 1: {2: 114, 3: 149, 1: 357, 0: 140, 4: 25}, 3: {3: 181, 4: 21, 2: 35, 1: 188, 0: 39}, 2: {4: 57, 1: 134, 2: 113, 3: 41, 0: 234}}
281
	Train Loss: 1.116 | Train Acc: 77.78%
	 Val. Loss: 1.497 |  Val. Acc: 39.76% 

282
	Train Loss: 1.121 | Train Acc: 77.34%
	 Val. Loss: 1.499 |  Val. Acc: 39.59% 

283
	Train Loss: 1.116 | Train Acc: 77.89%
	 Val. Loss: 1.501 |  Val. Acc: 39.26% 

284
	Train Loss: 1.118 | Train Acc: 77.96%
	 Val. Loss: 1.502 |  Val. Acc: 39.05% 

285
	Train Loss: 1.115 | Train Acc: 77.55%
	 Val. Loss: 1.501 |  Val. Acc: 39.65% 

286
	Train Loss: 1.114 | Train Acc: 77.83%
	 Val. Loss: 1.500 |  Val. Acc: 39.39% 

287
	Train Loss: 1.113 | Train Acc: 78.04%
	 Val. Loss: 1.500 |  Val. Acc: 39.42% 

288
	Train Loss: 1.114 | Train Acc: 78.04%
	 Val. Loss: 1.499 |  Val. Acc: 39.85% 

289
	Train Loss: 1.114 | Train Acc: 77.89%
	 Val. Loss: 1.498 |  Val. Acc: 40.05% 

290
	Train Loss: 1.118 | Train Acc: 77.60%
	 Val. Loss: 1.499 |  Val. Acc: 39.79% 

291
	Train Loss: 1.117 | Train Acc: 77.60%
	 Val. Loss: 1.503 |  Val. Acc: 39.32% 

292
	Train Loss: 1.111 | Train Acc: 78.32%
	 Val. Loss: 1.500 |  Val. Acc: 39.20% 

293
	Train Loss: 1.114 | Train Acc: 77.77%
	 Val. Loss: 1.502 |  Val. Acc: 39.12% 

294
	Train Loss: 1.112 | Train Acc: 78.35%
	 Val. Loss: 1.502 |  Val. Acc: 39.19% 

295
	Train Loss: 1.113 | Train Acc: 78.31%
	 Val. Loss: 1.500 |  Val. Acc: 39.85% 

296
	Train Loss: 1.113 | Train Acc: 77.81%
	 Val. Loss: 1.500 |  Val. Acc: 39.29% 

297
	Train Loss: 1.112 | Train Acc: 78.36%
	 Val. Loss: 1.504 |  Val. Acc: 38.86% 

298
	Train Loss: 1.111 | Train Acc: 77.98%
	 Val. Loss: 1.500 |  Val. Acc: 39.45% 

299
	Train Loss: 1.115 | Train Acc: 77.52%
	 Val. Loss: 1.497 |  Val. Acc: 39.89% 

300
	Train Loss: 1.113 | Train Acc: 78.24%
	 Val. Loss: 1.499 |  Val. Acc: 39.66% 

label distribution: actual vs pred: {0: {2: 127, 4: 125, 0: 366, 3: 35, 1: 137}, 4: {4: 107, 0: 151, 3: 18, 2: 31, 1: 39}, 1: {2: 111, 3: 163, 1: 378, 0: 100, 4: 33}, 3: {3: 194, 4: 22, 2: 33, 1: 190, 0: 25}, 2: {4: 60, 1: 148, 2: 126, 3: 51, 0: 194}}
	 pred vs actuals: {0: {2: 127, 4: 125, 0: 366, 3: 35, 1: 137}, 4: {4: 107, 0: 151, 3: 18, 2: 31, 1: 39}, 1: {2: 111, 3: 163, 1: 378, 0: 100, 4: 33}, 3: {3: 194, 4: 22, 2: 33, 1: 190, 0: 25}, 2: {4: 60, 1: 148, 2: 126, 3: 51, 0: 194}}


[0.21265671310359485, 0.22230608632031096, 0.2361561724040062, 0.23688716648920485, 0.2396972382177501, 0.24672429501738177, 0.2605668709158353, 0.2628937855567018, 0.2733630237241858, 0.27854507127308953, 0.2926630217190747, 0.30398091401683686, 0.31339747053847467, 0.3303605403802166, 0.3279973263065565, 0.3382975846937258, 0.35068568247094, 0.34896459182103473, 0.35952395255162833, 0.37349795723614626, 0.3742389650377509, 0.38026716338989397, 0.3799367138784226, 0.39139730037619536, 0.3939294941621284, 0.4010028639068342, 0.4075755527302555, 0.4113556938628628, 0.4121242390103536, 0.42148697240167554, 0.42432332977856674, 0.4318585776847247, 0.4413665083989705, 0.43470494877802185, 0.4433642252939477, 0.45637567103181254, 0.4533790956893468, 0.454172674923727, 0.46047875108239855, 0.4656507851870637, 0.468791306154913, 0.472080779946558, 0.47868976813473113, 0.4849219940024424, 0.48837544058011545, 0.4958919128870855, 0.4939930806421254, 0.4953211366313778, 0.5026260715097053, 0.5023506969621737, 0.5098859450044153, 0.516279640263074, 0.5172872605933446, 0.5217645998414793, 0.5282133702001616, 0.5313251021245843, 0.5327520427638537, 0.5396013578323469, 0.5452991068091022, 0.5425003004945033, 0.5461289855443179, 0.5555655692266003, 0.5600867178886448, 0.5525865176496985, 0.5530058379586973, 0.5578661980149953, 0.5630570075827647, 0.5617101758582407, 0.5698875471881536, 0.574187144569066, 0.5778796664656025, 0.5769609169328593, 0.580010063572017, 0.5818212769891573, 0.5855137988856939, 0.5859043298790988, 0.587412631130654, 0.5900875191165976, 0.593957782608189, 0.6010662001561901, 0.5962058398277248, 0.6055347772493754, 0.6092623466770398, 0.6035645979724519, 0.6075324941443526, 0.6117594927957614, 0.619411149253584, 0.6160841243996468, 0.622033465398501, 0.6171468196938571, 0.6175198269216982, 0.6191069856626258, 0.6264745054179675, 0.6262792397851813, 0.6285348074621262, 0.6304348914046266, 0.6294260193768157, 0.6322273290864954, 0.6423047846310759, 0.636097592850254, 0.642669029976135, 0.6457482176284267, 0.6463890892185576, 0.6500465633117989, 0.6424387165400536, 0.6503319514396528, 0.6516687695838544, 0.6556016182246274, 0.65100036037567, 0.6588122346629836, 0.6525962809993796, 0.6563426258901482, 0.6659106386306624, 0.668318913679689, 0.664948079411842, 0.6703616919582838, 0.6700324941443526, 0.6633609209430816, 0.66980843946814, 0.6710664003951364, 0.6707547263467692, 0.6689259894362324, 0.672547164028638, 0.6762471961104162, 0.6794590642463126, 0.6794227650176444, 0.6794140031348624, 0.6781748176709702, 0.6734471380982769, 0.6868954279107046, 0.6821064148863701, 0.6824281022428922, 0.6891434851302404, 0.6857638889796114, 0.6893400024605668, 0.6898481935670931, 0.6925143193980875, 0.689661689817089, 0.6985888306953046, 0.6958676299003705, 0.6984373747485958, 0.6971080670618031, 0.6941377773676833, 0.7013525894787758, 0.7052416287056388, 0.6984098374027096, 0.7063381198334368, 0.6999882339886879, 0.6991671174084215, 0.7067574401424356, 0.7124113794331137, 0.7162816431968724, 0.7101558120283362, 0.7127606044076893, 0.7075059582653656, 0.716067602100982, 0.7143202255305634, 0.710467485804536, 0.7108517583102396, 0.7125365497314766, 0.7182981355549538, 0.7210531321834756, 0.7143465111789093, 0.7145255047436718, 0.7175834135377788, 0.7226227668866719, 0.7134277619183336, 0.7221683990465452, 0.7336377474270999, 0.7257532744102826, 0.7210794181039889, 0.7281878353798226, 0.7232648901743431, 0.7310066691272334, 0.7298288171149824, 0.7330481957082879, 0.7282854683322993, 0.7302306136584173, 0.7329768486763244, 0.7323359768140262, 0.7358933148318774, 0.7351798445122427, 0.7329242771074652, 0.7337616660279226, 0.7331646038517016, 0.7320580991435813, 0.7359997096671361, 0.7283305294437495, 0.7329505627558112, 0.7340470541557765, 0.7408074981545749, 0.7344213130811578, 0.7461410017862712, 0.7413069273783192, 0.7358757910663134, 0.7455877492961274, 0.7392466253341605, 0.7374266503064055, 0.7429917187995563, 0.7456678582108729, 0.7443122646035669, 0.7426800450233564, 0.7442496797265528, 0.7460158314879082, 0.7433221683110276, 0.7460333555256395, 0.751304273735987, 0.7398524493931635, 0.746113464440385, 0.7460158314879082, 0.7520715673220212, 0.7534446846948911, 0.7507422596352286, 0.7519726829441715, 0.7515446007523907, 0.7472011936854009, 0.7542207401637073, 0.7616495933706902, 0.7563511375422891, 0.7530791876522918, 0.7498860952516669, 0.7523294181039889, 0.7543809582653656, 0.758564147230697, 0.7557903748669036, 0.7593214272364089, 0.7633443983722495, 0.7566640632881966, 0.7564938316062161, 0.7618185733551304, 0.7581886366077754, 0.7595004205290041, 0.7581898883053156, 0.753114235455587, 0.7594102980339363, 0.7559693684316662, 0.7574401184848455, 0.7644671754205608, 0.7647087538623374, 0.7592763658527915, 0.7597232235076765, 0.7624519347600197, 0.7622917166583614, 0.7619612674190573, 0.7648702233893686, 0.7628712550690185, 0.7595179445667354, 0.7682135203113295, 0.7625157713345742, 0.7616571035559319, 0.7615156614617126, 0.767063205917132, 0.7634595550902902, 0.7628624929140692, 0.7709422315636726, 0.7685439698228008, 0.7721726548726155, 0.7716381778455761, 0.7636110110369991, 0.7730738806397948, 0.7668041037097914, 0.7746259913052598, 0.7694001342063626, 0.7697919171694751, 0.7717445726808347, 0.7690509092317869, 0.7701924617432024, 0.7671257910663134, 0.7759189997633843, 0.7729311865758678, 0.7711825583079089, 0.7798067872927069, 0.7818057558852244, 0.7684989084391833, 0.7787816431968724, 0.7787891533821141, 0.7825630358909363, 0.7791559021222537, 0.7733868061135348, 0.7768014500674592, 0.7777740225944345, 0.7733592687676486, 0.7788792758771818, 0.779637807580434, 0.7754721421084992, 0.7782634382378565, 0.7803775635484147, 0.7804138627770829, 0.7789330991435813, 0.7759715713322435, 0.7760441700617472, 0.7832314448269535, 0.7776663763338028, 0.7835443703006936, 0.7831250499916947, 0.7781032204083657, 0.7836244792154391, 0.7797980254099249, 0.7751604683322993, 0.7824215932523824]
[0.26476063817105394, 0.2777260637029688, 0.2773936168944582, 0.2892952130195942, 0.29148936176553686, 0.30113031921234534, 0.30738031927575454, 0.3062500002536368, 0.2998005319783028, 0.3127659575102177, 0.31170212778639284, 0.32041223410596237, 0.34148936195576446, 0.3474734045089559, 0.35478723429618997, 0.3661569147667986, 0.36283244668169223, 0.3535239360433944, 0.35558510650979713, 0.36628989399747647, 0.3628989362970312, 0.3632313831055418, 0.3595744682119248, 0.3625664894885205, 0.37393617059322115, 0.3619015958714992, 0.3719414897421573, 0.3702127660842652, 0.3719414897421573, 0.367619681231519, 0.367619681231519, 0.371276596125136, 0.36030585144428495, 0.36263297910385944, 0.3619680854868382, 0.36728723442300837, 0.363962766337902, 0.37220744693532903, 0.3681515956178625, 0.3682180852332014, 0.36788563842469074, 0.36456117033958435, 0.37061170250811476, 0.36462765995492324, 0.3796542553191489, 0.3805851067634339, 0.3816489361702128, 0.368550532041712, 0.37214095731999014, 0.37593085144428495, 0.37925531952939134, 0.37121010650979713, 0.3702127660842652, 0.38091755357194457, 0.3714760637029688, 0.3817819147667986, 0.3748005317880752, 0.3842420216570509, 0.379587766337902, 0.3783909577004453, 0.3880984047625927, 0.38238031940257294, 0.3779255322953488, 0.37420212778639284, 0.37799202127659576, 0.3807180853600198, 0.3718085105114795, 0.3793882981259772, 0.3876994683387432, 0.3793218085106383, 0.3781249998731816, 0.3789228727208807, 0.3831781916161801, 0.3746010642102424, 0.3748005317880752, 0.38118351076511625, 0.3830452130195942, 0.3821808511906482, 0.3808510639566056, 0.3779255322953488, 0.38025265995492324, 0.37413563817105394, 0.37606383004087085, 0.3788563831055418, 0.37646276583062843, 0.3898936174017318, 0.3793218085106383, 0.38716755331830777, 0.383244681231519, 0.3807845743412667, 0.3810505321685304, 0.38357712804002964, 0.383244681231519, 0.38091755357194457, 0.37918882991405245, 0.38091755357194457, 0.37779255306467097, 0.3863031914893617, 0.3808510639566056, 0.3800531917429985, 0.3783909577004453, 0.38311170200084116, 0.3849069152740722, 0.384175532041712, 0.3873005319148936, 0.3855718088910935, 0.38763297872340424, 0.38663563829787234, 0.3798537235310737, 0.3821808511906482, 0.380518617148095, 0.3798537235310737, 0.39035904280682826, 0.3923537236578921, 0.379587766337902, 0.3858377660842652, 0.38291223442300837, 0.38404255344512617, 0.383244681231519, 0.384175532041712, 0.38623670250811476, 0.38896276595744683, 0.3878989365506679, 0.3781914894885205, 0.3828457448076695, 0.38716755331830777, 0.38896276595744683, 0.3925531918698169, 0.3925531918698169, 0.38763297872340424, 0.3821808511906482, 0.39095744680851063, 0.38617021289277587, 0.3865026597012865, 0.3855718088910935, 0.3871010637029688, 0.384175532041712, 0.38816489374383967, 0.39740691540089057, 0.39029255319148937, 0.3911569150204354, 0.3911569150204354, 0.394281914893617, 0.3888962769761999, 0.39654255357194457, 0.39248670225447796, 0.394281914893617, 0.3998670216570509, 0.3954787235310737, 0.38916223416937157, 0.4035239365506679, 0.38783244693532903, 0.3932180854868382, 0.3941489362970312, 0.3959441489361702, 0.3986037234042553, 0.40019946846556154, 0.39561170212765956, 0.3992686170212766, 0.39561170212765956, 0.3863031914893617, 0.3959441489361702, 0.40232712791321124, 0.3932180854868382, 0.3978723408059871, 0.3972739361702128, 0.39029255319148937, 0.39261968085106386, 0.3978723408059871, 0.395212766337902, 0.3908909578272637, 0.3902260642102424, 0.38982712778639284, 0.39654255357194457, 0.3954787235310737, 0.3922207450613062, 0.39920212804002964, 0.3951462767225631, 0.39720744718896583, 0.39700797897704104, 0.39561170212765956, 0.3949468085106383, 0.3998670216570509, 0.408045213273231, 0.3996010638297872, 0.3959441489361702, 0.3916223404255319, 0.3962765957446808, 0.39468085131746655, 0.3956781917429985, 0.3932180854868382, 0.3976063829787234, 0.3949468085106383, 0.3972739361702128, 0.3925531918698169, 0.3976063829787234, 0.3986702130195942, 0.3986702130195942, 0.39261968085106386, 0.39893617021276595, 0.39953457484854027, 0.40186170250811476, 0.3998670216570509, 0.39853723442300837, 0.3968750003804552, 0.4009308510638298, 0.4035904255319149, 0.3921542554459673, 0.40113031927575454, 0.3986037234042553, 0.3962765957446808, 0.3966090425531915, 0.39820478761449773, 0.39561170212765956, 0.4002659574468085, 0.39388297910385944, 0.39288563867832754, 0.3918882982527956, 0.3942154259123701, 0.3992686170212766, 0.3908244682119248, 0.39740691540089057, 0.402593085106383, 0.39388297910385944, 0.3949468085106383, 0.394281914893617, 0.3942154259123701, 0.3982712765957447, 0.39288563867832754, 0.39654255357194457, 0.3867686168944582, 0.39388297910385944, 0.39148936182894606, 0.39288563867832754, 0.39720744718896583, 0.3954787235310737, 0.395212766337902, 0.3954787235310737, 0.3974734043821375, 0.4005319152740722, 0.3972739361702128, 0.3945478727208807, 0.40186170250811476, 0.396143617148095, 0.3954787235310737, 0.3942154259123701, 0.39720744718896583, 0.3978723408059871, 0.3976063829787234, 0.3998670216570509, 0.3955452131464126, 0.39820478761449773, 0.39288563867832754, 0.3902260642102424, 0.38683510650979713, 0.39561170212765956, 0.38816489374383967, 0.3922207450613062, 0.3894946809778822, 0.39049202140341416, 0.39261968085106386, 0.39853723442300837, 0.384175532041712, 0.39587765995492324, 0.3935505322953488, 0.3949468085106383, 0.3949468085106383, 0.39793882978723405, 0.4021941493166254, 0.3962765957446808, 0.3986037234042553, 0.3992686170212766, 0.3888962769761999, 0.3939494680851064, 0.3908244682119248, 0.3978723408059871, 0.39753989399747647, 0.39853723442300837, 0.39288563867832754, 0.3955452131464126, 0.3976063829787234, 0.3959441489361702, 0.3925531918698169, 0.39049202140341416, 0.39654255357194457, 0.39388297910385944, 0.3942154259123701, 0.39853723442300837, 0.4005319152740722, 0.39793882978723405, 0.3932180854868382, 0.39195478723404253, 0.3912234046357743, 0.3918882982527956, 0.39853723442300837, 0.39288563867832754, 0.3885638301676892, 0.3945478727208807, 0.398869681231519, 0.3966090425531915]

