
size_of_vocab = len(Review.vocab)
embedding_dim = 300
num_hidden_nodes = 100
num_output_nodes = 5
num_layers = 2
dropout = 0.3
lr = 1e-4
 

1
	Train Loss: 1.605 | Train Acc: 22.91%
	 Val. Loss: 1.591 |  Val. Acc: 26.57% 

2
	Train Loss: 1.600 | Train Acc: 24.03%
	 Val. Loss: 1.588 |  Val. Acc: 25.98% 

3
	Train Loss: 1.598 | Train Acc: 24.48%
	 Val. Loss: 1.587 |  Val. Acc: 27.16% 

4
	Train Loss: 1.598 | Train Acc: 25.04%
	 Val. Loss: 1.586 |  Val. Acc: 28.03% 

5
	Train Loss: 1.596 | Train Acc: 25.74%
	 Val. Loss: 1.585 |  Val. Acc: 28.31% 

6
	Train Loss: 1.593 | Train Acc: 26.96%
	 Val. Loss: 1.583 |  Val. Acc: 27.93% 

7
	Train Loss: 1.590 | Train Acc: 26.90%
	 Val. Loss: 1.581 |  Val. Acc: 29.05% 

8
	Train Loss: 1.588 | Train Acc: 26.81%
	 Val. Loss: 1.579 |  Val. Acc: 29.57% 

9
	Train Loss: 1.583 | Train Acc: 27.78%
	 Val. Loss: 1.576 |  Val. Acc: 30.24% 

10
	Train Loss: 1.576 | Train Acc: 29.38%
	 Val. Loss: 1.570 |  Val. Acc: 30.52% 

11
	Train Loss: 1.568 | Train Acc: 30.27%
	 Val. Loss: 1.564 |  Val. Acc: 30.29% 

12
	Train Loss: 1.559 | Train Acc: 31.67%
	 Val. Loss: 1.558 |  Val. Acc: 30.82% 

13
	Train Loss: 1.551 | Train Acc: 33.42%
	 Val. Loss: 1.555 |  Val. Acc: 31.47% 

14
	Train Loss: 1.542 | Train Acc: 34.15%
	 Val. Loss: 1.551 |  Val. Acc: 31.93% 

15
	Train Loss: 1.531 | Train Acc: 35.25%
	 Val. Loss: 1.548 |  Val. Acc: 32.03% 

16
	Train Loss: 1.526 | Train Acc: 36.01%
	 Val. Loss: 1.543 |  Val. Acc: 32.19% 

17
	Train Loss: 1.520 | Train Acc: 36.98%
	 Val. Loss: 1.538 |  Val. Acc: 33.43% 

18
	Train Loss: 1.515 | Train Acc: 37.33%
	 Val. Loss: 1.536 |  Val. Acc: 33.70% 

19
	Train Loss: 1.502 | Train Acc: 39.12%
	 Val. Loss: 1.537 |  Val. Acc: 34.02% 

20
	Train Loss: 1.501 | Train Acc: 38.72%
	 Val. Loss: 1.534 |  Val. Acc: 34.06% 

label distribution: actual vs pred: {2: {0: 319, 1: 113, 2: 60, 3: 63, 4: 1}, 0: {2: 45, 4: 1, 0: 519, 1: 151, 3: 60}, 1: {1: 282, 2: 49, 0: 269, 3: 152}, 4: {1: 68, 0: 275, 3: 36, 2: 22}, 3: {0: 104, 1: 193, 3: 157, 2: 25}}
	 pred vs actuals: {2: {0: 319, 1: 113, 2: 60, 3: 63, 4: 1}, 0: {2: 45, 4: 1, 0: 519, 1: 151, 3: 60}, 1: {1: 282, 2: 49, 0: 269, 3: 152}, 4: {1: 68, 0: 275, 3: 36, 2: 22}, 3: {0: 104, 1: 193, 3: 157, 2: 25}}
21
	Train Loss: 1.494 | Train Acc: 39.61%
	 Val. Loss: 1.533 |  Val. Acc: 34.76% 

22
	Train Loss: 1.490 | Train Acc: 40.52%
	 Val. Loss: 1.531 |  Val. Acc: 35.03% 

23
	Train Loss: 1.482 | Train Acc: 41.73%
	 Val. Loss: 1.530 |  Val. Acc: 35.07% 

24
	Train Loss: 1.477 | Train Acc: 41.76%
	 Val. Loss: 1.532 |  Val. Acc: 35.00% 

25
	Train Loss: 1.473 | Train Acc: 42.34%
	 Val. Loss: 1.530 |  Val. Acc: 35.20% 

26
	Train Loss: 1.464 | Train Acc: 43.46%
	 Val. Loss: 1.530 |  Val. Acc: 35.76% 

27
	Train Loss: 1.465 | Train Acc: 43.32%
	 Val. Loss: 1.528 |  Val. Acc: 35.60% 

28
	Train Loss: 1.458 | Train Acc: 43.89%
	 Val. Loss: 1.525 |  Val. Acc: 36.16% 

29
	Train Loss: 1.456 | Train Acc: 44.55%
	 Val. Loss: 1.526 |  Val. Acc: 36.16% 

30
	Train Loss: 1.447 | Train Acc: 45.66%
	 Val. Loss: 1.522 |  Val. Acc: 36.56% 

31
	Train Loss: 1.442 | Train Acc: 45.68%
	 Val. Loss: 1.522 |  Val. Acc: 36.56% 

32
	Train Loss: 1.434 | Train Acc: 46.87%
	 Val. Loss: 1.524 |  Val. Acc: 36.03% 

33
	Train Loss: 1.433 | Train Acc: 47.18%
	 Val. Loss: 1.524 |  Val. Acc: 36.16% 

34
	Train Loss: 1.433 | Train Acc: 47.16%
	 Val. Loss: 1.522 |  Val. Acc: 36.73% 

35
	Train Loss: 1.423 | Train Acc: 48.37%
	 Val. Loss: 1.522 |  Val. Acc: 36.56% 

36
	Train Loss: 1.417 | Train Acc: 48.72%
	 Val. Loss: 1.524 |  Val. Acc: 36.06% 

37
	Train Loss: 1.410 | Train Acc: 49.70%
	 Val. Loss: 1.522 |  Val. Acc: 36.46% 

38
	Train Loss: 1.407 | Train Acc: 49.79%
	 Val. Loss: 1.522 |  Val. Acc: 35.96% 

39
	Train Loss: 1.409 | Train Acc: 49.15%
	 Val. Loss: 1.521 |  Val. Acc: 36.54% 

40
	Train Loss: 1.401 | Train Acc: 50.79%
	 Val. Loss: 1.523 |  Val. Acc: 36.40% 

label distribution: actual vs pred: {2: {0: 295, 2: 63, 3: 45, 1: 144, 4: 9}, 0: {2: 47, 4: 6, 0: 539, 1: 145, 3: 39}, 1: {1: 342, 0: 226, 2: 49, 4: 9, 3: 126}, 4: {1: 72, 0: 290, 3: 20, 2: 18, 4: 1}, 3: {0: 91, 3: 141, 1: 227, 2: 17, 4: 3}}
	 pred vs actuals: {2: {0: 295, 2: 63, 3: 45, 1: 144, 4: 9}, 0: {2: 47, 4: 6, 0: 539, 1: 145, 3: 39}, 1: {1: 342, 0: 226, 2: 49, 4: 9, 3: 126}, 4: {1: 72, 0: 290, 3: 20, 2: 18, 4: 1}, 3: {0: 91, 3: 141, 1: 227, 2: 17, 4: 3}}
41
	Train Loss: 1.395 | Train Acc: 50.85%
	 Val. Loss: 1.521 |  Val. Acc: 36.50% 

42
	Train Loss: 1.396 | Train Acc: 50.98%
	 Val. Loss: 1.522 |  Val. Acc: 36.21% 

43
	Train Loss: 1.388 | Train Acc: 51.89%
	 Val. Loss: 1.520 |  Val. Acc: 36.61% 

44
	Train Loss: 1.388 | Train Acc: 52.35%
	 Val. Loss: 1.520 |  Val. Acc: 36.34% 

45
	Train Loss: 1.384 | Train Acc: 52.70%
	 Val. Loss: 1.519 |  Val. Acc: 36.85% 

46
	Train Loss: 1.379 | Train Acc: 52.76%
	 Val. Loss: 1.521 |  Val. Acc: 36.03% 

47
	Train Loss: 1.376 | Train Acc: 53.29%
	 Val. Loss: 1.522 |  Val. Acc: 36.30% 

48
	Train Loss: 1.371 | Train Acc: 54.29%
	 Val. Loss: 1.516 |  Val. Acc: 36.85% 

49
	Train Loss: 1.363 | Train Acc: 54.53%
	 Val. Loss: 1.521 |  Val. Acc: 36.50% 

50
	Train Loss: 1.366 | Train Acc: 53.66%
	 Val. Loss: 1.520 |  Val. Acc: 36.44% 

51
	Train Loss: 1.357 | Train Acc: 55.39%
	 Val. Loss: 1.523 |  Val. Acc: 35.96% 

52
	Train Loss: 1.358 | Train Acc: 55.14%
	 Val. Loss: 1.523 |  Val. Acc: 36.26% 

53
	Train Loss: 1.351 | Train Acc: 56.33%
	 Val. Loss: 1.523 |  Val. Acc: 36.50% 

54
	Train Loss: 1.349 | Train Acc: 56.10%
	 Val. Loss: 1.521 |  Val. Acc: 37.07% 

55
	Train Loss: 1.343 | Train Acc: 57.27%
	 Val. Loss: 1.521 |  Val. Acc: 36.40% 

56
	Train Loss: 1.347 | Train Acc: 56.02%
	 Val. Loss: 1.521 |  Val. Acc: 36.77% 

57
	Train Loss: 1.336 | Train Acc: 57.56%
	 Val. Loss: 1.516 |  Val. Acc: 37.29% 

58
	Train Loss: 1.339 | Train Acc: 57.19%
	 Val. Loss: 1.518 |  Val. Acc: 36.80% 

59
	Train Loss: 1.332 | Train Acc: 57.56%
	 Val. Loss: 1.520 |  Val. Acc: 37.11% 

60
	Train Loss: 1.331 | Train Acc: 58.04%
	 Val. Loss: 1.522 |  Val. Acc: 36.97% 

label distribution: actual vs pred: {2: {0: 243, 4: 21, 2: 91, 3: 56, 1: 145}, 0: {2: 86, 1: 147, 0: 478, 4: 20, 3: 45}, 1: {1: 352, 2: 62, 4: 9, 3: 161, 0: 168}, 4: {1: 66, 0: 264, 3: 25, 4: 18, 2: 28}, 3: {1: 222, 4: 11, 3: 162, 2: 26, 0: 58}}
	 pred vs actuals: {2: {0: 243, 4: 21, 2: 91, 3: 56, 1: 145}, 0: {2: 86, 1: 147, 0: 478, 4: 20, 3: 45}, 1: {1: 352, 2: 62, 4: 9, 3: 161, 0: 168}, 4: {1: 66, 0: 264, 3: 25, 4: 18, 2: 28}, 3: {1: 222, 4: 11, 3: 162, 2: 26, 0: 58}}
61
	Train Loss: 1.324 | Train Acc: 58.82%
	 Val. Loss: 1.525 |  Val. Acc: 36.24% 

62
	Train Loss: 1.320 | Train Acc: 59.08%
	 Val. Loss: 1.514 |  Val. Acc: 37.53% 

63
	Train Loss: 1.322 | Train Acc: 59.16%
	 Val. Loss: 1.520 |  Val. Acc: 36.47% 

64
	Train Loss: 1.318 | Train Acc: 59.16%
	 Val. Loss: 1.526 |  Val. Acc: 35.71% 

65
	Train Loss: 1.310 | Train Acc: 60.60%
	 Val. Loss: 1.519 |  Val. Acc: 36.34% 

66
	Train Loss: 1.310 | Train Acc: 60.79%
	 Val. Loss: 1.522 |  Val. Acc: 36.44% 

67
	Train Loss: 1.305 | Train Acc: 60.86%
	 Val. Loss: 1.524 |  Val. Acc: 35.70% 

68
	Train Loss: 1.303 | Train Acc: 61.04%
	 Val. Loss: 1.519 |  Val. Acc: 36.87% 

69
	Train Loss: 1.301 | Train Acc: 61.54%
	 Val. Loss: 1.523 |  Val. Acc: 36.54% 

70
	Train Loss: 1.298 | Train Acc: 61.59%
	 Val. Loss: 1.520 |  Val. Acc: 36.68% 

71
	Train Loss: 1.293 | Train Acc: 61.79%
	 Val. Loss: 1.523 |  Val. Acc: 36.37% 

72
	Train Loss: 1.294 | Train Acc: 62.05%
	 Val. Loss: 1.522 |  Val. Acc: 36.78% 

73
	Train Loss: 1.288 | Train Acc: 61.88%
	 Val. Loss: 1.523 |  Val. Acc: 36.87% 

74
	Train Loss: 1.288 | Train Acc: 61.87%
	 Val. Loss: 1.520 |  Val. Acc: 36.77% 

75
	Train Loss: 1.284 | Train Acc: 63.11%
	 Val. Loss: 1.515 |  Val. Acc: 37.25% 

76
	Train Loss: 1.283 | Train Acc: 62.94%
	 Val. Loss: 1.520 |  Val. Acc: 36.70% 

77
	Train Loss: 1.275 | Train Acc: 63.87%
	 Val. Loss: 1.520 |  Val. Acc: 37.07% 

78
	Train Loss: 1.282 | Train Acc: 63.32%
	 Val. Loss: 1.515 |  Val. Acc: 37.48% 

79
	Train Loss: 1.271 | Train Acc: 64.29%
	 Val. Loss: 1.520 |  Val. Acc: 36.93% 

80
	Train Loss: 1.271 | Train Acc: 64.25%
	 Val. Loss: 1.520 |  Val. Acc: 37.09% 

label distribution: actual vs pred: {2: {0: 255, 4: 24, 2: 97, 3: 44, 1: 136}, 0: {2: 95, 1: 138, 0: 485, 4: 29, 3: 29}, 1: {1: 351, 2: 72, 4: 16, 3: 145, 0: 168}, 4: {1: 62, 0: 263, 3: 25, 4: 21, 2: 30}, 3: {1: 226, 4: 10, 3: 153, 0: 63, 2: 27}}
	 pred vs actuals: {2: {0: 255, 4: 24, 2: 97, 3: 44, 1: 136}, 0: {2: 95, 1: 138, 0: 485, 4: 29, 3: 29}, 1: {1: 351, 2: 72, 4: 16, 3: 145, 0: 168}, 4: {1: 62, 0: 263, 3: 25, 4: 21, 2: 30}, 3: {1: 226, 4: 10, 3: 153, 0: 63, 2: 27}}
81
	Train Loss: 1.270 | Train Acc: 64.18%
	 Val. Loss: 1.518 |  Val. Acc: 37.27% 

82
	Train Loss: 1.263 | Train Acc: 65.09%
	 Val. Loss: 1.518 |  Val. Acc: 36.93% 

83
	Train Loss: 1.261 | Train Acc: 64.93%
	 Val. Loss: 1.520 |  Val. Acc: 36.61% 

84
	Train Loss: 1.263 | Train Acc: 64.82%
	 Val. Loss: 1.520 |  Val. Acc: 37.03% 

85
	Train Loss: 1.259 | Train Acc: 65.10%
	 Val. Loss: 1.522 |  Val. Acc: 36.51% 

86
	Train Loss: 1.258 | Train Acc: 65.52%
	 Val. Loss: 1.522 |  Val. Acc: 36.80% 

87
	Train Loss: 1.254 | Train Acc: 65.54%
	 Val. Loss: 1.518 |  Val. Acc: 36.91% 

88
	Train Loss: 1.250 | Train Acc: 66.16%
	 Val. Loss: 1.516 |  Val. Acc: 37.57% 

89
	Train Loss: 1.252 | Train Acc: 65.96%
	 Val. Loss: 1.515 |  Val. Acc: 37.71% 

90
	Train Loss: 1.242 | Train Acc: 67.08%
	 Val. Loss: 1.514 |  Val. Acc: 37.75% 

91
	Train Loss: 1.249 | Train Acc: 66.46%
	 Val. Loss: 1.519 |  Val. Acc: 36.94% 

92
	Train Loss: 1.244 | Train Acc: 66.70%
	 Val. Loss: 1.515 |  Val. Acc: 38.08% 

93
	Train Loss: 1.245 | Train Acc: 66.59%
	 Val. Loss: 1.518 |  Val. Acc: 37.57% 

94
	Train Loss: 1.240 | Train Acc: 67.05%
	 Val. Loss: 1.522 |  Val. Acc: 36.93% 

95
	Train Loss: 1.239 | Train Acc: 67.48%
	 Val. Loss: 1.521 |  Val. Acc: 37.10% 

96
	Train Loss: 1.235 | Train Acc: 67.13%
	 Val. Loss: 1.520 |  Val. Acc: 37.29% 

97
	Train Loss: 1.230 | Train Acc: 68.00%
	 Val. Loss: 1.515 |  Val. Acc: 38.02% 

98
	Train Loss: 1.231 | Train Acc: 68.05%
	 Val. Loss: 1.518 |  Val. Acc: 37.04% 

99
	Train Loss: 1.231 | Train Acc: 67.90%
	 Val. Loss: 1.518 |  Val. Acc: 37.04% 

100
	Train Loss: 1.231 | Train Acc: 67.97%
	 Val. Loss: 1.522 |  Val. Acc: 36.93% 

label distribution: actual vs pred: {2: {4: 33, 2: 111, 3: 42, 1: 138, 0: 232}, 0: {2: 111, 4: 40, 0: 470, 1: 122, 3: 33}, 1: {1: 333, 2: 97, 4: 26, 3: 135, 0: 161}, 4: {1: 50, 4: 39, 0: 256, 3: 17, 2: 39}, 3: {1: 224, 4: 14, 3: 147, 2: 32, 0: 62}}
	 pred vs actuals: {2: {4: 33, 2: 111, 3: 42, 1: 138, 0: 232}, 0: {2: 111, 4: 40, 0: 470, 1: 122, 3: 33}, 1: {1: 333, 2: 97, 4: 26, 3: 135, 0: 161}, 4: {1: 50, 4: 39, 0: 256, 3: 17, 2: 39}, 3: {1: 224, 4: 14, 3: 147, 2: 32, 0: 62}}
101
	Train Loss: 1.227 | Train Acc: 68.49%
	 Val. Loss: 1.517 |  Val. Acc: 37.58% 

102
	Train Loss: 1.221 | Train Acc: 69.09%
	 Val. Loss: 1.518 |  Val. Acc: 37.01% 

103
	Train Loss: 1.219 | Train Acc: 69.43%
	 Val. Loss: 1.523 |  Val. Acc: 36.90% 

104
	Train Loss: 1.222 | Train Acc: 68.72%
	 Val. Loss: 1.521 |  Val. Acc: 37.02% 

105
	Train Loss: 1.221 | Train Acc: 69.19%
	 Val. Loss: 1.522 |  Val. Acc: 36.81% 

106
	Train Loss: 1.217 | Train Acc: 69.22%
	 Val. Loss: 1.522 |  Val. Acc: 37.05% 

107
	Train Loss: 1.219 | Train Acc: 68.91%
	 Val. Loss: 1.523 |  Val. Acc: 37.00% 

108
	Train Loss: 1.207 | Train Acc: 70.48%
	 Val. Loss: 1.517 |  Val. Acc: 37.71% 

109
	Train Loss: 1.208 | Train Acc: 70.38%
	 Val. Loss: 1.517 |  Val. Acc: 37.46% 

110
	Train Loss: 1.213 | Train Acc: 69.79%
	 Val. Loss: 1.519 |  Val. Acc: 37.61% 

111
	Train Loss: 1.208 | Train Acc: 70.41%
	 Val. Loss: 1.521 |  Val. Acc: 36.77% 

112
	Train Loss: 1.206 | Train Acc: 70.41%
	 Val. Loss: 1.520 |  Val. Acc: 37.00% 

113
	Train Loss: 1.203 | Train Acc: 71.23%
	 Val. Loss: 1.519 |  Val. Acc: 37.37% 

114
	Train Loss: 1.200 | Train Acc: 71.33%
	 Val. Loss: 1.517 |  Val. Acc: 37.51% 

115
	Train Loss: 1.201 | Train Acc: 71.16%
	 Val. Loss: 1.524 |  Val. Acc: 36.60% 

116
	Train Loss: 1.197 | Train Acc: 71.49%
	 Val. Loss: 1.518 |  Val. Acc: 37.27% 

117
	Train Loss: 1.193 | Train Acc: 71.65%
	 Val. Loss: 1.521 |  Val. Acc: 37.07% 

118
	Train Loss: 1.199 | Train Acc: 70.87%
	 Val. Loss: 1.520 |  Val. Acc: 37.43% 

119
	Train Loss: 1.193 | Train Acc: 71.85%
	 Val. Loss: 1.514 |  Val. Acc: 38.31% 

120
	Train Loss: 1.188 | Train Acc: 72.20%
	 Val. Loss: 1.520 |  Val. Acc: 37.17% 

label distribution: actual vs pred: {2: {4: 48, 2: 104, 3: 40, 1: 147, 0: 217}, 0: {2: 110, 4: 55, 0: 443, 1: 141, 3: 27}, 1: {1: 358, 2: 81, 4: 36, 3: 140, 0: 137}, 4: {4: 56, 0: 227, 3: 20, 1: 56, 2: 42}, 3: {4: 21, 1: 232, 3: 146, 2: 30, 0: 50}}
	 pred vs actuals: {2: {4: 48, 2: 104, 3: 40, 1: 147, 0: 217}, 0: {2: 110, 4: 55, 0: 443, 1: 141, 3: 27}, 1: {1: 358, 2: 81, 4: 36, 3: 140, 0: 137}, 4: {4: 56, 0: 227, 3: 20, 1: 56, 2: 42}, 3: {4: 21, 1: 232, 3: 146, 2: 30, 0: 50}}
121
	Train Loss: 1.192 | Train Acc: 71.97%
	 Val. Loss: 1.520 |  Val. Acc: 37.80% 

122
	Train Loss: 1.190 | Train Acc: 72.32%
	 Val. Loss: 1.519 |  Val. Acc: 37.31% 

123
	Train Loss: 1.189 | Train Acc: 72.07%
	 Val. Loss: 1.521 |  Val. Acc: 37.21% 

124
	Train Loss: 1.187 | Train Acc: 72.31%
	 Val. Loss: 1.519 |  Val. Acc: 37.13% 

125
	Train Loss: 1.187 | Train Acc: 72.31%
	 Val. Loss: 1.515 |  Val. Acc: 37.71% 

126
	Train Loss: 1.185 | Train Acc: 72.39%
	 Val. Loss: 1.513 |  Val. Acc: 38.18% 

127
	Train Loss: 1.183 | Train Acc: 72.31%
	 Val. Loss: 1.515 |  Val. Acc: 37.65% 

128
	Train Loss: 1.185 | Train Acc: 72.61%
	 Val. Loss: 1.516 |  Val. Acc: 37.75% 

129
	Train Loss: 1.180 | Train Acc: 73.18%
	 Val. Loss: 1.520 |  Val. Acc: 37.60% 

130
	Train Loss: 1.182 | Train Acc: 73.08%
	 Val. Loss: 1.520 |  Val. Acc: 37.63% 

131
	Train Loss: 1.177 | Train Acc: 73.79%
	 Val. Loss: 1.521 |  Val. Acc: 37.17% 

132
	Train Loss: 1.178 | Train Acc: 73.12%
	 Val. Loss: 1.524 |  Val. Acc: 36.64% 

133
	Train Loss: 1.170 | Train Acc: 74.00%
	 Val. Loss: 1.519 |  Val. Acc: 37.47% 

134
	Train Loss: 1.176 | Train Acc: 73.23%
	 Val. Loss: 1.518 |  Val. Acc: 37.73% 

135
	Train Loss: 1.170 | Train Acc: 73.88%
	 Val. Loss: 1.522 |  Val. Acc: 37.21% 

136
	Train Loss: 1.167 | Train Acc: 74.52%
	 Val. Loss: 1.523 |  Val. Acc: 36.76% 

137
	Train Loss: 1.164 | Train Acc: 75.07%
	 Val. Loss: 1.523 |  Val. Acc: 36.99% 

138
	Train Loss: 1.168 | Train Acc: 74.55%
	 Val. Loss: 1.523 |  Val. Acc: 36.60% 

139
	Train Loss: 1.165 | Train Acc: 74.39%
	 Val. Loss: 1.520 |  Val. Acc: 37.55% 

140
	Train Loss: 1.167 | Train Acc: 73.83%
	 Val. Loss: 1.523 |  Val. Acc: 37.03% 

label distribution: actual vs pred: {2: {4: 42, 2: 113, 3: 47, 1: 151, 0: 203}, 0: {2: 123, 4: 50, 0: 425, 1: 140, 3: 38}, 1: {1: 338, 2: 95, 3: 168, 0: 121, 4: 30}, 4: {4: 55, 0: 223, 3: 22, 2: 42, 1: 59}, 3: {1: 211, 2: 32, 3: 174, 0: 45, 4: 17}}
	 pred vs actuals: {2: {4: 42, 2: 113, 3: 47, 1: 151, 0: 203}, 0: {2: 123, 4: 50, 0: 425, 1: 140, 3: 38}, 1: {1: 338, 2: 95, 3: 168, 0: 121, 4: 30}, 4: {4: 55, 0: 223, 3: 22, 2: 42, 1: 59}, 3: {1: 211, 2: 32, 3: 174, 0: 45, 4: 17}}
141
	Train Loss: 1.168 | Train Acc: 73.89%
	 Val. Loss: 1.520 |  Val. Acc: 37.67% 

142
	Train Loss: 1.165 | Train Acc: 74.17%
	 Val. Loss: 1.519 |  Val. Acc: 37.49% 

143
	Train Loss: 1.162 | Train Acc: 75.03%
	 Val. Loss: 1.523 |  Val. Acc: 37.39% 

144
	Train Loss: 1.162 | Train Acc: 74.38%
	 Val. Loss: 1.523 |  Val. Acc: 37.16% 

145
	Train Loss: 1.157 | Train Acc: 75.07%
	 Val. Loss: 1.520 |  Val. Acc: 37.53% 

146
	Train Loss: 1.161 | Train Acc: 74.75%
	 Val. Loss: 1.524 |  Val. Acc: 36.89% 

147
	Train Loss: 1.157 | Train Acc: 75.16%
	 Val. Loss: 1.518 |  Val. Acc: 37.61% 

148
	Train Loss: 1.153 | Train Acc: 75.54%
	 Val. Loss: 1.524 |  Val. Acc: 36.56% 

149
	Train Loss: 1.153 | Train Acc: 75.34%
	 Val. Loss: 1.521 |  Val. Acc: 36.90% 

150
	Train Loss: 1.158 | Train Acc: 75.07%
	 Val. Loss: 1.517 |  Val. Acc: 37.39% 

151
	Train Loss: 1.154 | Train Acc: 75.60%
	 Val. Loss: 1.518 |  Val. Acc: 37.30% 

152
	Train Loss: 1.154 | Train Acc: 75.19%
	 Val. Loss: 1.520 |  Val. Acc: 37.22% 

153
	Train Loss: 1.152 | Train Acc: 75.58%
	 Val. Loss: 1.521 |  Val. Acc: 37.31% 

154
	Train Loss: 1.148 | Train Acc: 75.90%
	 Val. Loss: 1.521 |  Val. Acc: 37.33% 

155
	Train Loss: 1.146 | Train Acc: 76.34%
	 Val. Loss: 1.523 |  Val. Acc: 37.12% 

156
	Train Loss: 1.148 | Train Acc: 76.26%
	 Val. Loss: 1.524 |  Val. Acc: 36.79% 

157
	Train Loss: 1.148 | Train Acc: 76.37%
	 Val. Loss: 1.523 |  Val. Acc: 37.27% 

158
	Train Loss: 1.145 | Train Acc: 76.18%
	 Val. Loss: 1.526 |  Val. Acc: 36.36% 

159
	Train Loss: 1.142 | Train Acc: 76.87%
	 Val. Loss: 1.521 |  Val. Acc: 37.23% 

160
	Train Loss: 1.141 | Train Acc: 76.74%
	 Val. Loss: 1.524 |  Val. Acc: 37.17% 

label distribution: actual vs pred: {2: {4: 36, 2: 125, 3: 36, 1: 153, 0: 206}, 0: {2: 122, 4: 71, 0: 412, 1: 142, 3: 29}, 1: {1: 366, 2: 89, 3: 136, 0: 134, 4: 27}, 4: {4: 57, 1: 59, 0: 222, 3: 18, 2: 45}, 3: {1: 235, 2: 36, 3: 147, 0: 45, 4: 16}}
	 pred vs actuals: {2: {4: 36, 2: 125, 3: 36, 1: 153, 0: 206}, 0: {2: 122, 4: 71, 0: 412, 1: 142, 3: 29}, 1: {1: 366, 2: 89, 3: 136, 0: 134, 4: 27}, 4: {4: 57, 1: 59, 0: 222, 3: 18, 2: 45}, 3: {1: 235, 2: 36, 3: 147, 0: 45, 4: 16}}
161
	Train Loss: 1.143 | Train Acc: 76.36%
	 Val. Loss: 1.525 |  Val. Acc: 36.49% 

162
	Train Loss: 1.140 | Train Acc: 76.63%
	 Val. Loss: 1.525 |  Val. Acc: 36.73% 

163
	Train Loss: 1.138 | Train Acc: 76.97%
	 Val. Loss: 1.520 |  Val. Acc: 37.19% 

164
	Train Loss: 1.141 | Train Acc: 76.62%
	 Val. Loss: 1.518 |  Val. Acc: 37.23% 

165
	Train Loss: 1.141 | Train Acc: 76.72%
	 Val. Loss: 1.524 |  Val. Acc: 36.76% 

166
	Train Loss: 1.141 | Train Acc: 76.49%
	 Val. Loss: 1.522 |  Val. Acc: 36.95% 

167
	Train Loss: 1.133 | Train Acc: 77.24%
	 Val. Loss: 1.525 |  Val. Acc: 36.95% 

168
	Train Loss: 1.133 | Train Acc: 77.29%
	 Val. Loss: 1.523 |  Val. Acc: 36.56% 

169
	Train Loss: 1.135 | Train Acc: 77.63%
	 Val. Loss: 1.524 |  Val. Acc: 36.60% 

170
	Train Loss: 1.134 | Train Acc: 76.99%
	 Val. Loss: 1.529 |  Val. Acc: 36.13% 

171
	Train Loss: 1.132 | Train Acc: 77.41%
	 Val. Loss: 1.520 |  Val. Acc: 37.37% 

172
	Train Loss: 1.134 | Train Acc: 77.59%
	 Val. Loss: 1.524 |  Val. Acc: 36.87% 

173
	Train Loss: 1.134 | Train Acc: 78.07%
	 Val. Loss: 1.518 |  Val. Acc: 37.51% 

174
	Train Loss: 1.130 | Train Acc: 77.60%
	 Val. Loss: 1.521 |  Val. Acc: 37.49% 

175
	Train Loss: 1.128 | Train Acc: 78.07%
	 Val. Loss: 1.518 |  Val. Acc: 37.68% 

176
	Train Loss: 1.129 | Train Acc: 77.59%
	 Val. Loss: 1.518 |  Val. Acc: 37.70% 

177
	Train Loss: 1.131 | Train Acc: 77.43%
	 Val. Loss: 1.522 |  Val. Acc: 36.93% 

178
	Train Loss: 1.130 | Train Acc: 77.76%
	 Val. Loss: 1.519 |  Val. Acc: 37.97% 

179
	Train Loss: 1.126 | Train Acc: 77.98%
	 Val. Loss: 1.522 |  Val. Acc: 36.96% 

180
	Train Loss: 1.126 | Train Acc: 77.63%
	 Val. Loss: 1.519 |  Val. Acc: 37.50% 

label distribution: actual vs pred: {2: {4: 42, 2: 121, 3: 40, 1: 149, 0: 204}, 0: {2: 121, 4: 66, 0: 431, 1: 135, 3: 23}, 1: {1: 360, 2: 97, 3: 141, 0: 124, 4: 30}, 4: {4: 55, 1: 53, 0: 230, 2: 44, 3: 19}, 3: {1: 225, 2: 42, 0: 41, 3: 150, 4: 21}}
	 pred vs actuals: {2: {4: 42, 2: 121, 3: 40, 1: 149, 0: 204}, 0: {2: 121, 4: 66, 0: 431, 1: 135, 3: 23}, 1: {1: 360, 2: 97, 3: 141, 0: 124, 4: 30}, 4: {4: 55, 1: 53, 0: 230, 2: 44, 3: 19}, 3: {1: 225, 2: 42, 0: 41, 3: 150, 4: 21}}
181
	Train Loss: 1.132 | Train Acc: 77.43%
	 Val. Loss: 1.519 |  Val. Acc: 37.72% 

182
	Train Loss: 1.123 | Train Acc: 78.21%
	 Val. Loss: 1.517 |  Val. Acc: 38.07% 

183
	Train Loss: 1.123 | Train Acc: 78.43%
	 Val. Loss: 1.516 |  Val. Acc: 37.89% 

184
	Train Loss: 1.125 | Train Acc: 78.20%
	 Val. Loss: 1.525 |  Val. Acc: 36.99% 

185
	Train Loss: 1.125 | Train Acc: 78.18%
	 Val. Loss: 1.526 |  Val. Acc: 36.75% 

186
	Train Loss: 1.123 | Train Acc: 78.51%
	 Val. Loss: 1.519 |  Val. Acc: 37.61% 

187
	Train Loss: 1.118 | Train Acc: 78.90%
	 Val. Loss: 1.523 |  Val. Acc: 37.17% 

188
	Train Loss: 1.118 | Train Acc: 78.95%
	 Val. Loss: 1.521 |  Val. Acc: 37.51% 

189
	Train Loss: 1.115 | Train Acc: 79.05%
	 Val. Loss: 1.522 |  Val. Acc: 37.21% 

190
	Train Loss: 1.116 | Train Acc: 79.18%
	 Val. Loss: 1.522 |  Val. Acc: 36.87% 

191
	Train Loss: 1.122 | Train Acc: 78.44%
	 Val. Loss: 1.521 |  Val. Acc: 37.34% 

192
	Train Loss: 1.119 | Train Acc: 78.59%
	 Val. Loss: 1.523 |  Val. Acc: 37.20% 

193
	Train Loss: 1.113 | Train Acc: 79.01%
	 Val. Loss: 1.519 |  Val. Acc: 37.63% 

194
	Train Loss: 1.119 | Train Acc: 78.55%
	 Val. Loss: 1.520 |  Val. Acc: 37.29% 

195
	Train Loss: 1.114 | Train Acc: 79.30%
	 Val. Loss: 1.524 |  Val. Acc: 36.61% 

196
	Train Loss: 1.111 | Train Acc: 79.42%
	 Val. Loss: 1.526 |  Val. Acc: 36.89% 

197
	Train Loss: 1.115 | Train Acc: 78.90%
	 Val. Loss: 1.523 |  Val. Acc: 36.67% 

198
	Train Loss: 1.115 | Train Acc: 79.01%
	 Val. Loss: 1.523 |  Val. Acc: 36.91% 

199
	Train Loss: 1.116 | Train Acc: 78.83%
	 Val. Loss: 1.520 |  Val. Acc: 37.33% 

200
	Train Loss: 1.111 | Train Acc: 79.12%
	 Val. Loss: 1.521 |  Val. Acc: 37.28% 

label distribution: actual vs pred: {2: {2: 93, 4: 47, 3: 35, 1: 157, 0: 224}, 0: {2: 111, 4: 71, 0: 430, 1: 139, 3: 25}, 1: {1: 371, 2: 90, 3: 126, 0: 133, 4: 32}, 4: {4: 69, 1: 53, 0: 222, 2: 40, 3: 17}, 3: {1: 232, 2: 36, 3: 143, 0: 48, 4: 20}}
	 pred vs actuals: {2: {2: 93, 4: 47, 3: 35, 1: 157, 0: 224}, 0: {2: 111, 4: 71, 0: 430, 1: 139, 3: 25}, 1: {1: 371, 2: 90, 3: 126, 0: 133, 4: 32}, 4: {4: 69, 1: 53, 0: 222, 2: 40, 3: 17}, 3: {1: 232, 2: 36, 3: 143, 0: 48, 4: 20}}


[0.22911734540354123, 0.2402639678594741, 0.24483112381263214, 0.2504241635853594, 0.2573883364993063, 0.269649621268565, 0.2689788510853594, 0.2681009312244979, 0.2777679135853594, 0.29380721277133987, 0.3026949180798097, 0.3166528567671776, 0.33424084599722514, 0.3415206755426797, 0.35254892673004756, 0.3600556344979189, 0.3698409880426797, 0.37333293877203355, 0.39124644886363635, 0.3872020991349762, 0.3961391255936839, 0.40516493063081394, 0.4172979798167944, 0.41761363636363635, 0.42343355444344605, 0.4345703125, 0.43317945081401954, 0.43889086185531184, 0.4455196496776559, 0.45658735795454547, 0.4568043718622489, 0.46870067868043075, 0.471798058768565, 0.4716402305798097, 0.4836845011873679, 0.48716658777133987, 0.4969519414007664, 0.4979186396707188, 0.49150686558674683, 0.5078618214889006, 0.5085227272727273, 0.5097952178594741, 0.5188506154174154, 0.5234572284600951, 0.527028093453158, 0.5275706283070825, 0.5328677398237315, 0.5429292930798097, 0.5453460385853593, 0.5366260260343552, 0.5538687658580866, 0.5513829704035412, 0.563279277221723, 0.5610499527644027, 0.5727193812755021, 0.5601917613636364, 0.5756096118553118, 0.5718907829035412, 0.5756194760853593, 0.580403645946221, 0.588216145946221, 0.5908400409600951, 0.5915897254916754, 0.591569996862249, 0.6060310131446882, 0.607855902815407, 0.6085661300881342, 0.6103712909600951, 0.6153724748979915, 0.6158755523237315, 0.6179174557328224, 0.6204920295964588, 0.6187559186734937, 0.6187164614146407, 0.6311454386873678, 0.629448784684593, 0.6387014676901427, 0.6332366635853593, 0.6428740529174154, 0.6425090750510042, 0.6418185765770349, 0.6509430239146407, 0.6492956914007664, 0.6482303504916754, 0.6510219380936839, 0.6552438448098573, 0.6553819443691861, 0.6616161616349762, 0.659613715315407, 0.6707504733719609, 0.6645655777644027, 0.6669823232699524, 0.6659071181308139, 0.6705334596335888, 0.6748342801901427, 0.6713127368553118, 0.6800327494063161, 0.680466777221723, 0.6790463226762685, 0.6797170926901427, 0.6848563761873678, 0.6908834437755021, 0.6942767517810519, 0.6871547505936839, 0.6918797347355973, 0.6922249841419134, 0.6890782829035412, 0.7048216540027749, 0.703776041553779, 0.6978969380936839, 0.7040916982699524, 0.7041311551901427, 0.7122593119063161, 0.7132654670964588, 0.7116378630426797, 0.7148733426901427, 0.7164516255936839, 0.7086884470825846, 0.7184639363126322, 0.7220446653664112, 0.7197068340399049, 0.7232086489146407, 0.7207327176901427, 0.7231100062755021, 0.7230804135853593, 0.7238596908070825, 0.7231198705055497, 0.7261482006446882, 0.7317511049861257, 0.7307844067161734, 0.7378669506446882, 0.731248027221723, 0.7399581755426797, 0.7322640466419134, 0.7387941919944503, 0.7452256943691861, 0.75073981996287, 0.7455018938265063, 0.7438940182328224, 0.7382516573098573, 0.7388829704035412, 0.7417238794944503, 0.7503057924861257, 0.7437657829035412, 0.750651041553779, 0.747504340315407, 0.7515881471335888, 0.7554056187244979, 0.7534130368043076, 0.750651041553779, 0.7559777460992336, 0.7518939392810519, 0.7558100538497622, 0.7589567550881342, 0.7633956755426797, 0.7626459913497622, 0.7636620107699524, 0.7618371210992336, 0.7686631943691861, 0.767380839721723, 0.7636126892810519, 0.7663154988126322, 0.7697383995083246, 0.7661773989146407, 0.7672032829035412, 0.7649049084972251, 0.7723524306308139, 0.7729344222355973, 0.7762685448608615, 0.7698666351762685, 0.774088541553779, 0.7758641097355973, 0.7806581438265063, 0.7760022096335888, 0.7807469222355973, 0.7759430239146407, 0.7742562341419134, 0.7775607640770349, 0.7798098169944503, 0.7762685448608615, 0.7743055556308139, 0.7820588699118658, 0.7842980585992336, 0.7819503630426797, 0.7818023989146407, 0.7850970642810519, 0.7890427715399049, 0.789496527815407, 0.7905224118043076, 0.79175544496287, 0.784376972778277, 0.7859256629916754, 0.790078519758853, 0.7855310920964588, 0.793008207258853, 0.7942313761873678, 0.7889539931308139, 0.7901179766790434, 0.7882930870083246, 0.791212910278277]
[0.2656914896153389, 0.2597739360433944, 0.2716090425531915, 0.2802526595744681, 0.28311170225447796, 0.27925531914893614, 0.29049202134000496, 0.2957446809778822, 0.302393617148095, 0.30518617021276595, 0.3028590425531915, 0.3081781914893617, 0.3146941490629886, 0.3193484043821375, 0.32027925535085355, 0.3219414893934067, 0.33430851076511625, 0.3370345745314943, 0.34022606386149185, 0.3406250001268184, 0.3476063831055418, 0.35033244687191983, 0.35066489368043047, 0.3500000000634092, 0.351994680914473, 0.3576462766591539, 0.35598404261660066, 0.36163563836128154, 0.36163563836128154, 0.3656250000634092, 0.36555851076511625, 0.360305851127239, 0.36163563836128154, 0.36728723410596237, 0.3656250000634092, 0.3605718086374567, 0.3646276596378773, 0.3596409575102177, 0.3653590425531915, 0.36396276602085603, 0.3650265957446808, 0.362101064083424, 0.3660904257855517, 0.36336436170212766, 0.3684840424263731, 0.360305851127239, 0.3629654255953241, 0.3684840424263731, 0.3650265957446808, 0.36436170212765956, 0.3596409575102177, 0.36263297878681344, 0.3650265957446808, 0.3706781914893617, 0.3640292553191489, 0.36768617021276595, 0.3728723405523503, 0.3680186170212766, 0.37107712791321124, 0.3696808510638298, 0.36236702127659576, 0.3752659578272637, 0.3646941489361702, 0.35711436195576446, 0.36336436170212766, 0.3644281917429985, 0.3570478723404255, 0.36868351063829785, 0.3653590425531915, 0.36675531940257294, 0.3636968085106383, 0.36775265982810484, 0.36868351063829785, 0.36768617021276595, 0.3724734041285008, 0.3670212765957447, 0.3707446811047006, 0.3748005317880752, 0.3693484042553192, 0.3709441489995794, 0.37273936195576446, 0.3692819149570262, 0.3660904257855517, 0.3703457446808511, 0.3650930853600198, 0.3680186170212766, 0.3691489360433944, 0.3757313832323602, 0.3771276594476497, 0.3775265958714992, 0.36941489387065807, 0.3807845743412667, 0.3757313832323602, 0.3693484042553192, 0.37101063829787234, 0.37293882985064325, 0.38018617033958435, 0.37041223429618997, 0.37041223429618997, 0.3693484042553192, 0.37579787221360716, 0.37007978748767933, 0.3690159574468085, 0.3702127660842652, 0.36808510663661553, 0.3704787232774369, 0.37001329787234044, 0.37706117046640275, 0.3746010642102424, 0.37606383004087085, 0.36768617021276595, 0.37001329787234044, 0.37367021276595747, 0.3750664896153389, 0.36595744687191983, 0.37273936195576446, 0.3707446811047006, 0.3742686170846858, 0.38311170200084116, 0.3716755319148936, 0.37799202127659576, 0.3730718087642751, 0.3720744683387432, 0.371343085106383, 0.37706117046640275, 0.3817819147667986, 0.37646276583062843, 0.3775265958714992, 0.3759973404255319, 0.37632978723404253, 0.3716755319148936, 0.3663563829787234, 0.37473404280682826, 0.3773271276595745, 0.3721409574785131, 0.367619680914473, 0.3699468085740475, 0.3660239361702128, 0.3754654254050965, 0.3702792553825581, 0.3767287236578921, 0.37493351070170705, 0.37393617027617515, 0.37160904261660066, 0.3752659575102177, 0.3689494681485156, 0.37606383004087085, 0.3656250000634092, 0.3690159574468085, 0.37393617027617515, 0.37300531914893614, 0.37220744693532903, 0.373138297904045, 0.3732712766591539, 0.37121010650979713, 0.36788563842469074, 0.37273936195576446, 0.36363031921234534, 0.3723404255319149, 0.3716755319148936, 0.364893617148095, 0.36728723410596237, 0.37194148942511135, 0.372273936233622, 0.3675531916161801, 0.3695478724672439, 0.369481383010428, 0.3656250000634092, 0.36595744687191983, 0.3613031915527709, 0.37367021276595747, 0.36868351063829785, 0.3750664896153389, 0.37493351070170705, 0.37679521263913907, 0.37699468085106386, 0.3693484042553192, 0.3797207449344879, 0.36961436176553686, 0.375, 0.3771941490629886, 0.3807180853600198, 0.3788563831055418, 0.3699468085740475, 0.3674867021593642, 0.37606383004087085, 0.3716755319148936, 0.3750664896153389, 0.3720744683387432, 0.36868351063829785, 0.3734042555727857, 0.37200797872340424, 0.37632978723404253, 0.3728723405523503, 0.3660904257855517, 0.3689494681485156, 0.36668882978723405, 0.36908244706214743, 0.3732712766591539, 0.3728058509370114]     








size_of_vocab = len(Review.vocab)
embedding_dim = 256
num_hidden_nodes = 50
num_output_nodes = 5
num_layers = 2
dropout = 0.4
lr = 1e-4



	Train Loss: 1.606 | Train Acc: 22.73%
	 Val. Loss: 1.596 |  Val. Acc: 26.08% 

2
	Train Loss: 1.602 | Train Acc: 24.36%
	 Val. Loss: 1.590 |  Val. Acc: 26.07% 

3
	Train Loss: 1.600 | Train Acc: 23.93%
	 Val. Loss: 1.589 |  Val. Acc: 26.04% 

4
	Train Loss: 1.600 | Train Acc: 24.28%
	 Val. Loss: 1.588 |  Val. Acc: 27.31% 

5
	Train Loss: 1.598 | Train Acc: 24.33%
	 Val. Loss: 1.587 |  Val. Acc: 27.36% 

6
	Train Loss: 1.598 | Train Acc: 24.86%
	 Val. Loss: 1.587 |  Val. Acc: 27.39% 

7
	Train Loss: 1.597 | Train Acc: 24.61%
	 Val. Loss: 1.586 |  Val. Acc: 27.53% 

8
	Train Loss: 1.595 | Train Acc: 25.72%
	 Val. Loss: 1.585 |  Val. Acc: 27.77% 

9
	Train Loss: 1.594 | Train Acc: 25.72%
	 Val. Loss: 1.585 |  Val. Acc: 27.90% 

10
	Train Loss: 1.593 | Train Acc: 25.79%
	 Val. Loss: 1.583 |  Val. Acc: 27.83% 

11
	Train Loss: 1.592 | Train Acc: 26.14%
	 Val. Loss: 1.583 |  Val. Acc: 28.63% 

12
	Train Loss: 1.589 | Train Acc: 27.08%
	 Val. Loss: 1.582 |  Val. Acc: 28.26% 

13
	Train Loss: 1.588 | Train Acc: 27.21%
	 Val. Loss: 1.582 |  Val. Acc: 28.92% 

14
	Train Loss: 1.586 | Train Acc: 27.55%
	 Val. Loss: 1.580 |  Val. Acc: 28.72% 

15
	Train Loss: 1.583 | Train Acc: 28.28%
	 Val. Loss: 1.579 |  Val. Acc: 29.39% 

16
	Train Loss: 1.579 | Train Acc: 28.82%
	 Val. Loss: 1.577 |  Val. Acc: 29.95% 

17
	Train Loss: 1.577 | Train Acc: 29.37%
	 Val. Loss: 1.576 |  Val. Acc: 29.95% 

18
	Train Loss: 1.573 | Train Acc: 29.61%
	 Val. Loss: 1.573 |  Val. Acc: 29.78% 

19
	Train Loss: 1.569 | Train Acc: 30.70%
	 Val. Loss: 1.572 |  Val. Acc: 30.28% 

20
	Train Loss: 1.565 | Train Acc: 31.64%
	 Val. Loss: 1.569 |  Val. Acc: 30.72% 

label distribution: actual vs pred: {2: {0: 272, 1: 207, 2: 33, 3: 42, 4: 2}, 0: {2: 37, 0: 431, 1: 248, 3: 57, 4: 3}, 1: {2: 25, 0: 279, 1: 371, 3: 74, 4: 3}, 4: {1: 146, 0: 212, 3: 35, 2: 7, 4: 1}, 3: {3: 77, 1: 239, 0: 151, 2: 11, 4: 1}}
	 pred vs actuals: {2: {0: 272, 1: 207, 2: 33, 3: 42, 4: 2}, 0: {2: 37, 0: 431, 1: 248, 3: 57, 4: 3}, 1: {2: 25, 0: 279, 1: 371, 3: 74, 4: 3}, 4: {1: 146, 0: 212, 3: 35, 2: 7, 4: 1}, 3: {3: 77, 1: 239, 0: 151, 2: 11, 4: 1}}
21
	Train Loss: 1.562 | Train Acc: 31.71%
	 Val. Loss: 1.566 |  Val. Acc: 31.11% 

22
	Train Loss: 1.557 | Train Acc: 31.51%
	 Val. Loss: 1.563 |  Val. Acc: 30.98% 

23
	Train Loss: 1.551 | Train Acc: 33.16%
	 Val. Loss: 1.560 |  Val. Acc: 31.39% 

24
	Train Loss: 1.548 | Train Acc: 33.56%
	 Val. Loss: 1.557 |  Val. Acc: 31.66% 

25
	Train Loss: 1.542 | Train Acc: 33.90%
	 Val. Loss: 1.554 |  Val. Acc: 32.12% 

26
	Train Loss: 1.536 | Train Acc: 34.96%
	 Val. Loss: 1.552 |  Val. Acc: 32.27% 

27
	Train Loss: 1.531 | Train Acc: 35.20%
	 Val. Loss: 1.549 |  Val. Acc: 32.71% 

28
	Train Loss: 1.527 | Train Acc: 36.14%
	 Val. Loss: 1.547 |  Val. Acc: 32.88% 

29
	Train Loss: 1.522 | Train Acc: 36.50%
	 Val. Loss: 1.545 |  Val. Acc: 33.14% 

30
	Train Loss: 1.523 | Train Acc: 36.53%
	 Val. Loss: 1.545 |  Val. Acc: 33.20% 

31
	Train Loss: 1.514 | Train Acc: 37.07%
	 Val. Loss: 1.542 |  Val. Acc: 33.25% 

32
	Train Loss: 1.511 | Train Acc: 37.55%
	 Val. Loss: 1.541 |  Val. Acc: 33.25% 

33
	Train Loss: 1.510 | Train Acc: 37.62%
	 Val. Loss: 1.541 |  Val. Acc: 33.08% 

34
	Train Loss: 1.503 | Train Acc: 38.73%
	 Val. Loss: 1.540 |  Val. Acc: 33.31% 

35
	Train Loss: 1.499 | Train Acc: 39.48%
	 Val. Loss: 1.539 |  Val. Acc: 33.50% 

36
	Train Loss: 1.497 | Train Acc: 39.39%
	 Val. Loss: 1.537 |  Val. Acc: 33.34% 

37
	Train Loss: 1.491 | Train Acc: 39.99%
	 Val. Loss: 1.536 |  Val. Acc: 33.61% 

38
	Train Loss: 1.490 | Train Acc: 40.40%
	 Val. Loss: 1.536 |  Val. Acc: 33.57% 

39
	Train Loss: 1.485 | Train Acc: 40.67%
	 Val. Loss: 1.535 |  Val. Acc: 33.94% 

40
	Train Loss: 1.481 | Train Acc: 41.53%
	 Val. Loss: 1.535 |  Val. Acc: 33.64% 

label distribution: actual vs pred: {2: {0: 278, 2: 54, 3: 45, 1: 177, 4: 2}, 0: {2: 62, 4: 5, 0: 453, 1: 189, 3: 67}, 1: {2: 49, 0: 223, 1: 340, 3: 138, 4: 2}, 4: {3: 36, 0: 259, 2: 19, 4: 3, 1: 84}, 3: {0: 94, 1: 218, 3: 151, 2: 15, 4: 1}}
	 pred vs actuals: {2: {0: 278, 2: 54, 3: 45, 1: 177, 4: 2}, 0: {2: 62, 4: 5, 0: 453, 1: 189, 3: 67}, 1: {2: 49, 0: 223, 1: 340, 3: 138, 4: 2}, 4: {3: 36, 0: 259, 2: 19, 4: 3, 1: 84}, 3: {0: 94, 1: 218, 3: 151, 2: 15, 4: 1}}
41
	Train Loss: 1.480 | Train Acc: 41.87%
	 Val. Loss: 1.534 |  Val. Acc: 34.31% 

42
	Train Loss: 1.475 | Train Acc: 41.58%
	 Val. Loss: 1.533 |  Val. Acc: 34.27% 

43
	Train Loss: 1.471 | Train Acc: 42.68%
	 Val. Loss: 1.532 |  Val. Acc: 34.28% 

44
	Train Loss: 1.466 | Train Acc: 42.55%
	 Val. Loss: 1.530 |  Val. Acc: 34.67% 

45
	Train Loss: 1.468 | Train Acc: 42.35%
	 Val. Loss: 1.531 |  Val. Acc: 34.64% 

46
	Train Loss: 1.464 | Train Acc: 42.98%
	 Val. Loss: 1.529 |  Val. Acc: 34.67% 

47
	Train Loss: 1.460 | Train Acc: 43.90%
	 Val. Loss: 1.530 |  Val. Acc: 34.67% 

48
	Train Loss: 1.456 | Train Acc: 43.75%
	 Val. Loss: 1.528 |  Val. Acc: 34.44% 

49
	Train Loss: 1.454 | Train Acc: 44.30%
	 Val. Loss: 1.529 |  Val. Acc: 34.34% 

50
	Train Loss: 1.452 | Train Acc: 44.51%
	 Val. Loss: 1.529 |  Val. Acc: 34.67% 

51
	Train Loss: 1.450 | Train Acc: 44.32%
	 Val. Loss: 1.528 |  Val. Acc: 34.97% 

52
	Train Loss: 1.447 | Train Acc: 45.33%
	 Val. Loss: 1.528 |  Val. Acc: 34.81% 

53
	Train Loss: 1.441 | Train Acc: 45.72%
	 Val. Loss: 1.528 |  Val. Acc: 34.61% 

54
	Train Loss: 1.438 | Train Acc: 46.08%
	 Val. Loss: 1.528 |  Val. Acc: 35.30% 

55
	Train Loss: 1.439 | Train Acc: 45.63%
	 Val. Loss: 1.528 |  Val. Acc: 35.43% 

56
	Train Loss: 1.436 | Train Acc: 45.83%
	 Val. Loss: 1.527 |  Val. Acc: 35.50% 

57
	Train Loss: 1.430 | Train Acc: 47.03%
	 Val. Loss: 1.527 |  Val. Acc: 35.70% 

58
	Train Loss: 1.430 | Train Acc: 46.87%
	 Val. Loss: 1.526 |  Val. Acc: 35.17% 

59
	Train Loss: 1.433 | Train Acc: 46.32%
	 Val. Loss: 1.527 |  Val. Acc: 35.93% 

60
	Train Loss: 1.426 | Train Acc: 46.95%
	 Val. Loss: 1.526 |  Val. Acc: 35.80% 

label distribution: actual vs pred: {2: {0: 295, 2: 72, 3: 34, 4: 3, 1: 152}, 0: {2: 60, 4: 9, 0: 483, 1: 175, 3: 49}, 1: {2: 62, 0: 208, 1: 362, 3: 115, 4: 5}, 4: {3: 31, 0: 273, 2: 24, 4: 5, 1: 68}, 3: {0: 84, 1: 227, 3: 144, 2: 20, 4: 4}}
	 pred vs actuals: {2: {0: 295, 2: 72, 3: 34, 4: 3, 1: 152}, 0: {2: 60, 4: 9, 0: 483, 1: 175, 3: 49}, 1: {2: 62, 0: 208, 1: 362, 3: 115, 4: 5}, 4: {3: 31, 0: 273, 2: 24, 4: 5, 1: 68}, 3: {0: 84, 1: 227, 3: 144, 2: 20, 4: 4}}
61
	Train Loss: 1.421 | Train Acc: 47.31%
	 Val. Loss: 1.526 |  Val. Acc: 35.70% 

62
	Train Loss: 1.424 | Train Acc: 47.78%
	 Val. Loss: 1.525 |  Val. Acc: 35.81% 

63
	Train Loss: 1.418 | Train Acc: 48.55%
	 Val. Loss: 1.524 |  Val. Acc: 35.80% 

64
	Train Loss: 1.417 | Train Acc: 48.61%
	 Val. Loss: 1.524 |  Val. Acc: 35.74% 

65
	Train Loss: 1.415 | Train Acc: 48.86%
	 Val. Loss: 1.524 |  Val. Acc: 35.64% 

66
	Train Loss: 1.410 | Train Acc: 49.23%
	 Val. Loss: 1.524 |  Val. Acc: 35.80% 

67
	Train Loss: 1.406 | Train Acc: 49.49%
	 Val. Loss: 1.524 |  Val. Acc: 35.94% 

68
	Train Loss: 1.403 | Train Acc: 50.06%
	 Val. Loss: 1.523 |  Val. Acc: 36.18% 

69
	Train Loss: 1.400 | Train Acc: 50.29%
	 Val. Loss: 1.524 |  Val. Acc: 35.61% 

70
	Train Loss: 1.397 | Train Acc: 50.86%
	 Val. Loss: 1.524 |  Val. Acc: 35.58% 

71
	Train Loss: 1.400 | Train Acc: 50.22%
	 Val. Loss: 1.525 |  Val. Acc: 35.51% 

72
	Train Loss: 1.393 | Train Acc: 50.91%
	 Val. Loss: 1.524 |  Val. Acc: 35.71% 

73
	Train Loss: 1.387 | Train Acc: 52.34%
	 Val. Loss: 1.524 |  Val. Acc: 35.81% 

74
	Train Loss: 1.386 | Train Acc: 51.73%
	 Val. Loss: 1.525 |  Val. Acc: 35.84% 

75
	Train Loss: 1.389 | Train Acc: 51.45%
	 Val. Loss: 1.525 |  Val. Acc: 35.64% 

76
	Train Loss: 1.383 | Train Acc: 52.49%
	 Val. Loss: 1.524 |  Val. Acc: 35.61% 

77
	Train Loss: 1.378 | Train Acc: 52.90%
	 Val. Loss: 1.525 |  Val. Acc: 35.31% 

78
	Train Loss: 1.383 | Train Acc: 52.13%
	 Val. Loss: 1.526 |  Val. Acc: 35.44% 

79
	Train Loss: 1.376 | Train Acc: 53.20%
	 Val. Loss: 1.524 |  Val. Acc: 35.61% 

80
	Train Loss: 1.374 | Train Acc: 53.12%
	 Val. Loss: 1.524 |  Val. Acc: 36.39% 

label distribution: actual vs pred: {2: {0: 295, 2: 63, 3: 32, 1: 154, 4: 12}, 0: {2: 59, 4: 27, 0: 491, 1: 146, 3: 53}, 1: {2: 62, 0: 198, 1: 364, 3: 116, 4: 12}, 4: {3: 27, 0: 275, 4: 16, 2: 28, 1: 55}, 3: {0: 81, 1: 227, 3: 143, 2: 20, 4: 8}}
	 pred vs actuals: {2: {0: 295, 2: 63, 3: 32, 1: 154, 4: 12}, 0: {2: 59, 4: 27, 0: 491, 1: 146, 3: 53}, 1: {2: 62, 0: 198, 1: 364, 3: 116, 4: 12}, 4: {3: 27, 0: 275, 4: 16, 2: 28, 1: 55}, 3: {0: 81, 1: 227, 3: 143, 2: 20, 4: 8}}
81
	Train Loss: 1.373 | Train Acc: 53.72%
	 Val. Loss: 1.525 |  Val. Acc: 35.78% 

82
	Train Loss: 1.372 | Train Acc: 53.49%
	 Val. Loss: 1.524 |  Val. Acc: 36.02% 

83
	Train Loss: 1.371 | Train Acc: 53.82%
	 Val. Loss: 1.524 |  Val. Acc: 36.28% 

84
	Train Loss: 1.366 | Train Acc: 53.97%
	 Val. Loss: 1.522 |  Val. Acc: 36.66% 

85
	Train Loss: 1.363 | Train Acc: 54.76%
	 Val. Loss: 1.524 |  Val. Acc: 36.18% 

86
	Train Loss: 1.364 | Train Acc: 54.78%
	 Val. Loss: 1.524 |  Val. Acc: 36.28% 

87
	Train Loss: 1.359 | Train Acc: 55.20%
	 Val. Loss: 1.523 |  Val. Acc: 36.25% 

88
	Train Loss: 1.359 | Train Acc: 54.93%
	 Val. Loss: 1.524 |  Val. Acc: 36.04% 

89
	Train Loss: 1.360 | Train Acc: 54.40%
	 Val. Loss: 1.523 |  Val. Acc: 36.38% 

90
	Train Loss: 1.356 | Train Acc: 54.88%
	 Val. Loss: 1.524 |  Val. Acc: 36.21% 

91
	Train Loss: 1.350 | Train Acc: 55.80%
	 Val. Loss: 1.524 |  Val. Acc: 36.18% 

92
	Train Loss: 1.348 | Train Acc: 55.99%
	 Val. Loss: 1.522 |  Val. Acc: 36.99% 

93
	Train Loss: 1.348 | Train Acc: 56.00%
	 Val. Loss: 1.524 |  Val. Acc: 36.08% 

94
	Train Loss: 1.346 | Train Acc: 56.18%
	 Val. Loss: 1.525 |  Val. Acc: 36.05% 

95
	Train Loss: 1.344 | Train Acc: 56.29%
	 Val. Loss: 1.523 |  Val. Acc: 36.42% 

96
	Train Loss: 1.340 | Train Acc: 56.87%
	 Val. Loss: 1.523 |  Val. Acc: 36.39% 

97
	Train Loss: 1.337 | Train Acc: 57.23%
	 Val. Loss: 1.524 |  Val. Acc: 36.16% 

98
	Train Loss: 1.337 | Train Acc: 57.12%
	 Val. Loss: 1.524 |  Val. Acc: 36.29% 

99
	Train Loss: 1.334 | Train Acc: 57.98%
	 Val. Loss: 1.524 |  Val. Acc: 36.22% 

100
	Train Loss: 1.334 | Train Acc: 57.61%
	 Val. Loss: 1.523 |  Val. Acc: 36.46% 

label distribution: actual vs pred: {2: {0: 303, 2: 65, 3: 31, 1: 144, 4: 13}, 0: {2: 68, 4: 32, 0: 500, 1: 130, 3: 46}, 1: {2: 65, 0: 204, 1: 353, 3: 115, 4: 15}, 4: {3: 23, 0: 279, 4: 19, 2: 34, 1: 46}, 3: {0: 83, 1: 221, 3: 142, 2: 24, 4: 9}}
	 pred vs actuals: {2: {0: 303, 2: 65, 3: 31, 1: 144, 4: 13}, 0: {2: 68, 4: 32, 0: 500, 1: 130, 3: 46}, 1: {2: 65, 0: 204, 1: 353, 3: 115, 4: 15}, 4: {3: 23, 0: 279, 4: 19, 2: 34, 1: 46}, 3: {0: 83, 1: 221, 3: 142, 2: 24, 4: 9}}
101
	Train Loss: 1.335 | Train Acc: 57.23%
	 Val. Loss: 1.523 |  Val. Acc: 36.28% 

102
	Train Loss: 1.332 | Train Acc: 57.57%
	 Val. Loss: 1.524 |  Val. Acc: 36.45% 

103
	Train Loss: 1.331 | Train Acc: 57.92%
	 Val. Loss: 1.522 |  Val. Acc: 36.82% 

104
	Train Loss: 1.327 | Train Acc: 58.29%
	 Val. Loss: 1.524 |  Val. Acc: 36.32% 

105
	Train Loss: 1.322 | Train Acc: 58.41%
	 Val. Loss: 1.525 |  Val. Acc: 35.98% 

106
	Train Loss: 1.320 | Train Acc: 58.71%
	 Val. Loss: 1.523 |  Val. Acc: 36.12% 

107
	Train Loss: 1.322 | Train Acc: 58.83%
	 Val. Loss: 1.524 |  Val. Acc: 36.45% 

108
	Train Loss: 1.317 | Train Acc: 59.48%
	 Val. Loss: 1.523 |  Val. Acc: 36.45% 

109
	Train Loss: 1.313 | Train Acc: 59.64%
	 Val. Loss: 1.526 |  Val. Acc: 35.91% 

110
	Train Loss: 1.310 | Train Acc: 59.94%
	 Val. Loss: 1.522 |  Val. Acc: 36.79% 

111
	Train Loss: 1.308 | Train Acc: 60.51%
	 Val. Loss: 1.523 |  Val. Acc: 36.38% 

112
	Train Loss: 1.311 | Train Acc: 59.72%
	 Val. Loss: 1.525 |  Val. Acc: 36.24% 

113
	Train Loss: 1.310 | Train Acc: 60.47%
	 Val. Loss: 1.523 |  Val. Acc: 36.52% 

114
	Train Loss: 1.306 | Train Acc: 60.47%
	 Val. Loss: 1.525 |  Val. Acc: 36.18% 

115
	Train Loss: 1.309 | Train Acc: 60.14%
	 Val. Loss: 1.523 |  Val. Acc: 36.32% 

116
	Train Loss: 1.302 | Train Acc: 60.78%
	 Val. Loss: 1.523 |  Val. Acc: 36.35% 

117
	Train Loss: 1.305 | Train Acc: 60.26%
	 Val. Loss: 1.522 |  Val. Acc: 36.22% 

118
	Train Loss: 1.303 | Train Acc: 61.00%
	 Val. Loss: 1.524 |  Val. Acc: 36.12% 

119
	Train Loss: 1.300 | Train Acc: 60.87%
	 Val. Loss: 1.524 |  Val. Acc: 36.52% 

120
	Train Loss: 1.298 | Train Acc: 61.16%
	 Val. Loss: 1.525 |  Val. Acc: 35.98% 

label distribution: actual vs pred: {2: {0: 287, 2: 63, 3: 36, 1: 157, 4: 13}, 0: {2: 67, 4: 34, 0: 471, 3: 57, 1: 147}, 1: {2: 60, 0: 179, 1: 356, 3: 141, 4: 16}, 4: {3: 27, 0: 261, 4: 24, 1: 59, 2: 30}, 3: {0: 66, 1: 223, 3: 153, 2: 26, 4: 11}}
	 pred vs actuals: {2: {0: 287, 2: 63, 3: 36, 1: 157, 4: 13}, 0: {2: 67, 4: 34, 0: 471, 3: 57, 1: 147}, 1: {2: 60, 0: 179, 1: 356, 3: 141, 4: 16}, 4: {3: 27, 0: 261, 4: 24, 1: 59, 2: 30}, 3: {0: 66, 1: 223, 3: 153, 2: 26, 4: 11}}
121
	Train Loss: 1.299 | Train Acc: 61.37%
	 Val. Loss: 1.522 |  Val. Acc: 36.31% 

122
	Train Loss: 1.296 | Train Acc: 61.71%
	 Val. Loss: 1.522 |  Val. Acc: 36.16% 

123
	Train Loss: 1.293 | Train Acc: 61.93%
	 Val. Loss: 1.524 |  Val. Acc: 36.39% 

124
	Train Loss: 1.293 | Train Acc: 61.81%
	 Val. Loss: 1.526 |  Val. Acc: 35.67% 

125
	Train Loss: 1.287 | Train Acc: 62.52%
	 Val. Loss: 1.525 |  Val. Acc: 35.92% 

126
	Train Loss: 1.287 | Train Acc: 62.14%
	 Val. Loss: 1.524 |  Val. Acc: 36.08% 

127
	Train Loss: 1.282 | Train Acc: 62.80%
	 Val. Loss: 1.523 |  Val. Acc: 36.25% 

128
	Train Loss: 1.286 | Train Acc: 62.38%
	 Val. Loss: 1.523 |  Val. Acc: 36.46% 

129
	Train Loss: 1.280 | Train Acc: 63.29%
	 Val. Loss: 1.524 |  Val. Acc: 36.05% 

130
	Train Loss: 1.281 | Train Acc: 63.03%
	 Val. Loss: 1.524 |  Val. Acc: 36.42% 

131
	Train Loss: 1.280 | Train Acc: 63.58%
	 Val. Loss: 1.523 |  Val. Acc: 36.56% 

132
	Train Loss: 1.278 | Train Acc: 63.02%
	 Val. Loss: 1.523 |  Val. Acc: 36.49% 

133
	Train Loss: 1.273 | Train Acc: 63.83%
	 Val. Loss: 1.524 |  Val. Acc: 36.48% 

134
	Train Loss: 1.275 | Train Acc: 63.56%
	 Val. Loss: 1.523 |  Val. Acc: 36.48% 

135
	Train Loss: 1.275 | Train Acc: 63.54%
	 Val. Loss: 1.525 |  Val. Acc: 36.18% 

136
	Train Loss: 1.276 | Train Acc: 63.46%
	 Val. Loss: 1.525 |  Val. Acc: 36.32% 

137
	Train Loss: 1.274 | Train Acc: 63.87%
	 Val. Loss: 1.525 |  Val. Acc: 36.38% 

138
	Train Loss: 1.271 | Train Acc: 63.97%
	 Val. Loss: 1.524 |  Val. Acc: 36.28% 

139
	Train Loss: 1.262 | Train Acc: 65.17%
	 Val. Loss: 1.524 |  Val. Acc: 36.11% 

140
	Train Loss: 1.266 | Train Acc: 64.41%
	 Val. Loss: 1.523 |  Val. Acc: 36.58% 

label distribution: actual vs pred: {2: {0: 295, 2: 66, 3: 33, 1: 146, 4: 16}, 0: {2: 78, 4: 37, 0: 483, 3: 49, 1: 129}, 1: {2: 63, 0: 183, 1: 366, 3: 120, 4: 20}, 4: {1: 54, 0: 267, 3: 17, 4: 30, 2: 33}, 3: {0: 71, 1: 228, 3: 140, 2: 26, 4: 14}}
	 pred vs actuals: {2: {0: 295, 2: 66, 3: 33, 1: 146, 4: 16}, 0: {2: 78, 4: 37, 0: 483, 3: 49, 1: 129}, 1: {2: 63, 0: 183, 1: 366, 3: 120, 4: 20}, 4: {1: 54, 0: 267, 3: 17, 4: 30, 2: 33}, 3: {0: 71, 1: 228, 3: 140, 2: 26, 4: 14}}
141
	Train Loss: 1.264 | Train Acc: 65.12%
	 Val. Loss: 1.523 |  Val. Acc: 36.34% 

142
	Train Loss: 1.267 | Train Acc: 64.69%
	 Val. Loss: 1.524 |  Val. Acc: 36.75% 

143
	Train Loss: 1.263 | Train Acc: 64.91%
	 Val. Loss: 1.525 |  Val. Acc: 36.20% 

144
	Train Loss: 1.262 | Train Acc: 64.85%
	 Val. Loss: 1.523 |  Val. Acc: 36.38% 

145
	Train Loss: 1.260 | Train Acc: 64.95%
	 Val. Loss: 1.522 |  Val. Acc: 36.78% 

146
	Train Loss: 1.255 | Train Acc: 64.91%
	 Val. Loss: 1.521 |  Val. Acc: 36.84% 

147
	Train Loss: 1.258 | Train Acc: 65.30%
	 Val. Loss: 1.523 |  Val. Acc: 36.65% 

148
	Train Loss: 1.255 | Train Acc: 66.04%
	 Val. Loss: 1.521 |  Val. Acc: 36.45% 

149
	Train Loss: 1.252 | Train Acc: 65.62%
	 Val. Loss: 1.519 |  Val. Acc: 36.95% 

150
	Train Loss: 1.249 | Train Acc: 65.99%
	 Val. Loss: 1.521 |  Val. Acc: 36.88% 

151
	Train Loss: 1.248 | Train Acc: 66.48%
	 Val. Loss: 1.521 |  Val. Acc: 36.88% 

152
	Train Loss: 1.247 | Train Acc: 66.50%
	 Val. Loss: 1.520 |  Val. Acc: 37.28% 

153
	Train Loss: 1.253 | Train Acc: 65.67%
	 Val. Loss: 1.520 |  Val. Acc: 37.01% 

154
	Train Loss: 1.244 | Train Acc: 66.40%
	 Val. Loss: 1.520 |  Val. Acc: 37.05% 

155
	Train Loss: 1.249 | Train Acc: 66.35%
	 Val. Loss: 1.520 |  Val. Acc: 36.95% 

156
	Train Loss: 1.245 | Train Acc: 66.94%
	 Val. Loss: 1.522 |  Val. Acc: 36.50% 

157
	Train Loss: 1.247 | Train Acc: 66.30%
	 Val. Loss: 1.521 |  Val. Acc: 36.88% 

158
	Train Loss: 1.243 | Train Acc: 66.92%
	 Val. Loss: 1.522 |  Val. Acc: 36.52% 

159
	Train Loss: 1.238 | Train Acc: 67.02%
	 Val. Loss: 1.521 |  Val. Acc: 36.85% 

160
	Train Loss: 1.239 | Train Acc: 66.78%
	 Val. Loss: 1.520 |  Val. Acc: 36.95% 

label distribution: actual vs pred: {2: {4: 21, 2: 66, 3: 36, 0: 288, 1: 145}, 0: {2: 70, 4: 49, 0: 480, 3: 49, 1: 128}, 1: {2: 58, 0: 179, 1: 358, 3: 133, 4: 24}, 4: {4: 37, 0: 264, 3: 16, 1: 50, 2: 34}, 3: {0: 66, 1: 214, 2: 25, 3: 155, 4: 19}}
	 pred vs actuals: {2: {4: 21, 2: 66, 3: 36, 0: 288, 1: 145}, 0: {2: 70, 4: 49, 0: 480, 3: 49, 1: 128}, 1: {2: 58, 0: 179, 1: 358, 3: 133, 4: 24}, 4: {4: 37, 0: 264, 3: 16, 1: 50, 2: 34}, 3: {0: 66, 1: 214, 2: 25, 3: 155, 4: 19}}
161
	Train Loss: 1.240 | Train Acc: 67.09%
	 Val. Loss: 1.521 |  Val. Acc: 37.15% 

162
	Train Loss: 1.235 | Train Acc: 67.65%
	 Val. Loss: 1.521 |  Val. Acc: 36.98% 

163
	Train Loss: 1.236 | Train Acc: 67.21%
	 Val. Loss: 1.521 |  Val. Acc: 36.95% 

164
	Train Loss: 1.236 | Train Acc: 67.35%
	 Val. Loss: 1.520 |  Val. Acc: 37.21% 

165
	Train Loss: 1.230 | Train Acc: 68.30%
	 Val. Loss: 1.522 |  Val. Acc: 37.18% 

166
	Train Loss: 1.234 | Train Acc: 67.56%
	 Val. Loss: 1.522 |  Val. Acc: 37.15% 

167
	Train Loss: 1.234 | Train Acc: 67.62%
	 Val. Loss: 1.524 |  Val. Acc: 36.48% 

168
	Train Loss: 1.230 | Train Acc: 68.01%
	 Val. Loss: 1.520 |  Val. Acc: 37.11% 

169
	Train Loss: 1.231 | Train Acc: 67.99%
	 Val. Loss: 1.520 |  Val. Acc: 37.01% 

170
	Train Loss: 1.227 | Train Acc: 68.48%
	 Val. Loss: 1.521 |  Val. Acc: 37.17% 

171
	Train Loss: 1.224 | Train Acc: 68.15%
	 Val. Loss: 1.522 |  Val. Acc: 36.93% 

172
	Train Loss: 1.226 | Train Acc: 68.45%
	 Val. Loss: 1.522 |  Val. Acc: 36.94% 

173
	Train Loss: 1.229 | Train Acc: 68.38%
	 Val. Loss: 1.521 |  Val. Acc: 37.05% 

174
	Train Loss: 1.219 | Train Acc: 69.29%
	 Val. Loss: 1.521 |  Val. Acc: 37.00% 

175
	Train Loss: 1.222 | Train Acc: 68.69%
	 Val. Loss: 1.521 |  Val. Acc: 36.70% 

176
	Train Loss: 1.221 | Train Acc: 69.17%
	 Val. Loss: 1.521 |  Val. Acc: 36.94% 

177
	Train Loss: 1.221 | Train Acc: 68.87%
	 Val. Loss: 1.521 |  Val. Acc: 36.85% 

178
	Train Loss: 1.218 | Train Acc: 69.00%
	 Val. Loss: 1.521 |  Val. Acc: 36.84% 

179
	Train Loss: 1.216 | Train Acc: 69.44%
	 Val. Loss: 1.521 |  Val. Acc: 36.80% 

180
	Train Loss: 1.215 | Train Acc: 69.56%
	 Val. Loss: 1.522 |  Val. Acc: 36.47% 

label distribution: actual vs pred: {2: {4: 28, 2: 76, 3: 33, 0: 283, 1: 136}, 0: {2: 78, 4: 55, 0: 472, 3: 46, 1: 125}, 1: {2: 75, 0: 177, 1: 341, 3: 131, 4: 28}, 4: {4: 40, 0: 264, 3: 14, 1: 45, 2: 38}, 3: {0: 65, 1: 211, 2: 28, 3: 157, 4: 18}}
	 pred vs actuals: {2: {4: 28, 2: 76, 3: 33, 0: 283, 1: 136}, 0: {2: 78, 4: 55, 0: 472, 3: 46, 1: 125}, 1: {2: 75, 0: 177, 1: 341, 3: 131, 4: 28}, 4: {4: 40, 0: 264, 3: 14, 1: 45, 2: 38}, 3: {0: 65, 1: 211, 2: 28, 3: 157, 4: 18}}
181
	Train Loss: 1.216 | Train Acc: 69.48%
	 Val. Loss: 1.523 |  Val. Acc: 36.44% 

182
	Train Loss: 1.215 | Train Acc: 69.64%
	 Val. Loss: 1.524 |  Val. Acc: 36.50% 

183
	Train Loss: 1.215 | Train Acc: 69.55%
	 Val. Loss: 1.524 |  Val. Acc: 36.34% 

184
	Train Loss: 1.210 | Train Acc: 70.02%
	 Val. Loss: 1.522 |  Val. Acc: 36.70% 

185
	Train Loss: 1.209 | Train Acc: 70.27%
	 Val. Loss: 1.520 |  Val. Acc: 36.91% 

186
	Train Loss: 1.212 | Train Acc: 69.62%
	 Val. Loss: 1.520 |  Val. Acc: 36.97% 

187
	Train Loss: 1.212 | Train Acc: 69.50%
	 Val. Loss: 1.521 |  Val. Acc: 36.84% 

188
	Train Loss: 1.204 | Train Acc: 70.80%
	 Val. Loss: 1.522 |  Val. Acc: 36.93% 

189
	Train Loss: 1.202 | Train Acc: 70.86%
	 Val. Loss: 1.523 |  Val. Acc: 37.07% 

190
	Train Loss: 1.206 | Train Acc: 70.10%
	 Val. Loss: 1.522 |  Val. Acc: 36.88% 

191
	Train Loss: 1.208 | Train Acc: 69.74%
	 Val. Loss: 1.523 |  Val. Acc: 36.90% 

192
	Train Loss: 1.209 | Train Acc: 70.23%
	 Val. Loss: 1.523 |  Val. Acc: 36.88% 

193
	Train Loss: 1.202 | Train Acc: 70.39%
	 Val. Loss: 1.522 |  Val. Acc: 37.00% 

194
	Train Loss: 1.205 | Train Acc: 70.22%
	 Val. Loss: 1.521 |  Val. Acc: 36.81% 

195
	Train Loss: 1.200 | Train Acc: 71.07%
	 Val. Loss: 1.522 |  Val. Acc: 36.77% 

196
	Train Loss: 1.200 | Train Acc: 70.85%
	 Val. Loss: 1.523 |  Val. Acc: 36.80% 

197
	Train Loss: 1.201 | Train Acc: 70.70%
	 Val. Loss: 1.523 |  Val. Acc: 36.93% 

198
	Train Loss: 1.198 | Train Acc: 71.35%
	 Val. Loss: 1.523 |  Val. Acc: 36.87% 

199
	Train Loss: 1.195 | Train Acc: 71.74%
	 Val. Loss: 1.520 |  Val. Acc: 37.07% 

200
	Train Loss: 1.196 | Train Acc: 71.01%
	 Val. Loss: 1.522 |  Val. Acc: 37.24% 

label distribution: actual vs pred: {2: {4: 33, 2: 82, 3: 38, 0: 260, 1: 143}, 0: {2: 86, 4: 61, 0: 448, 3: 43, 1: 138}, 1: {2: 74, 0: 148, 1: 377, 3: 124, 4: 29}, 4: {4: 52, 0: 246, 3: 10, 1: 53, 2: 40}, 3: {0: 58, 1: 231, 2: 23, 3: 148, 4: 19}}
	 pred vs actuals: {2: {4: 33, 2: 82, 3: 38, 0: 260, 1: 143}, 0: {2: 86, 4: 61, 0: 448, 3: 43, 1: 138}, 1: {2: 74, 0: 148, 1: 377, 3: 124, 4: 29}, 4: {4: 52, 0: 246, 3: 10, 1: 53, 2: 40}, 3: {0: 58, 1: 231, 2: 23, 3: 148, 4: 19}}

[0.22733191290700977, 0.24363754740492863, 0.2392874053594741, 0.2427990846335888, 0.24332189085808667, 0.24855981699445032, 0.24608388576995244, 0.2572305082258853, 0.2572107796811245, 0.25788154994899576, 0.2614425505426797, 0.27084319767626847, 0.27214528095315804, 0.27551886049861257, 0.2827986900440671, 0.28821417299861257, 0.2936691130426797, 0.29609572286294267, 0.3069858744063161, 0.31639638576995244, 0.31714607013220136, 0.3151436238126321, 0.3316268150440671, 0.3356119792231105, 0.3389559659090909, 0.34963896785947407, 0.35203598490492866, 0.3614267676391385, 0.3650074968622489, 0.3653330176391385, 0.3706695864993063, 0.37547348490492866, 0.3761935764077035, 0.38728101331401954, 0.39475812830708246, 0.3938604798167944, 0.3998678187755021, 0.4039516255936839, 0.40666429935531184, 0.4152560764077035, 0.4187184343622489, 0.41584793241186574, 0.4268367266790433, 0.42546559345315804, 0.42347301136363635, 0.42981573549861257, 0.43900923295454547, 0.43750986423004756, 0.44296480444344605, 0.44514480741186574, 0.4431818181818182, 0.45325323549861257, 0.4571693498979915, 0.4608092646707188, 0.4563210227272727, 0.4582840119573203, 0.47034801136363635, 0.4686809501356699, 0.46320628171617334, 0.4694996843622489, 0.4730508207258853, 0.4778448548167944, 0.4855389835482294, 0.4861209754916755, 0.48859690671617334, 0.4922762784090909, 0.49486071650277486, 0.5006214488636364, 0.5029099590399049, 0.5085819129916754, 0.5022095959972251, 0.5090652619573203, 0.5234375, 0.5172723328525369, 0.5145399306308139, 0.5248579545454546, 0.5290305397727273, 0.5213068181818182, 0.532029277221723, 0.5311809500510042, 0.5371685607189481, 0.5348899147727273, 0.5381747159090909, 0.5397332703525369, 0.5476345487616279, 0.5477627840909091, 0.5519846908070825, 0.5493312027644027, 0.5440439551391385, 0.5488478534600951, 0.5580413510853593, 0.5599056976762685, 0.5599846118553118, 0.5618095011873678, 0.5628551136363636, 0.5686750317161734, 0.5722952176901427, 0.5712200125510042, 0.5797920613126322, 0.5761422823098573, 0.5723248107189481, 0.5756589330055497, 0.5791903409090909, 0.5828894414007664, 0.5840731534090909, 0.5870620266280391, 0.5882556028664112, 0.5948350693691861, 0.5963936238126322, 0.599382496862249, 0.6050741794434461, 0.597222222184593, 0.60472893003713, 0.60472893003713, 0.6013553504916754, 0.6078263101252642, 0.6026475693691861, 0.6099964488636364, 0.6086746369573203, 0.6116437817161734, 0.6136955493553118, 0.6170789931308139, 0.6192589960992336, 0.6180851483209566, 0.625197285278277, 0.6214094067161734, 0.6279987374489958, 0.623826152221723, 0.6329308710992336, 0.6303267045454546, 0.6358309659090909, 0.6302280619063161, 0.6383266255936839, 0.6355744949118658, 0.635416666553779, 0.6346472539007664, 0.6387409249489958, 0.6396681659600951, 0.6517321653664112, 0.6441070864146407, 0.6512093591419134, 0.6468888101252642, 0.649088541553779, 0.6485065499489958, 0.6495324335992336, 0.6491181345825846, 0.6529849273237315, 0.660353535278277, 0.6562302715399049, 0.6599491004916754, 0.6647924557328224, 0.6649798767810519, 0.6567333491349762, 0.6640329073098573, 0.6634509153664112, 0.6694385260343552, 0.6629774306308139, 0.6692017834972251, 0.670247395946221, 0.6677911931818182, 0.6709280301901427, 0.6764717488126322, 0.67211174253713, 0.6734828755936839, 0.6830018938265063, 0.6756234216419134, 0.6761955493553118, 0.6800623420964588, 0.6799439709972251, 0.6847577335482294, 0.6815025254406712, 0.6845012625510042, 0.6838403567671776, 0.6928562972355973, 0.6869081438265063, 0.6916528567671776, 0.688673847778277, 0.6899562027644027, 0.6944148516790434, 0.6955689709972251, 0.6947601010853593, 0.6963975693691861, 0.695509785278277, 0.7001755840399049, 0.7027008364146407, 0.6962397413497622, 0.6950363005426797, 0.7079782198098573, 0.7086292613636364, 0.700974589721723, 0.6973839960992336, 0.7023062658580866, 0.7038746845315803, 0.7021681659600951, 0.7106514363126322, 0.7085207544944503, 0.7070411142300476, 0.713541666553779, 0.7173690027133985, 0.710089173167944]
[0.26077127662744926, 0.2607047872498949, 0.2604388298189386, 0.2730718085423429, 0.2736037234042553, 0.273869680914473, 0.2752659574468085, 0.27765957472172187, 0.27898936195576446, 0.27825797872340424, 0.2863031917429985, 0.2826462768493815, 0.2892287234042553, 0.2872340425531915, 0.2939494683387432, 0.2995345744680851, 0.2994680851697922, 0.297805851127239, 0.30279255325489857, 0.3071808510638298, 0.3111037234676645, 0.309773936233622, 0.3138962768493815, 0.31655585131746655, 0.32121010663661553, 0.32273936176553686, 0.3271276595744681, 0.32878989361702127, 0.3314494680851064, 0.3320478724038347, 0.3325132981259772, 0.3325132981259772, 0.3307845744680851, 0.33311170212765956, 0.33503989368043047, 0.3333776596378773, 0.3361037234042553, 0.33570478729745173, 0.33936170219106876, 0.33643617021276595, 0.34308510638297873, 0.34268617027617515, 0.3427526595744681, 0.34674202127659576, 0.3464095744680851, 0.34674202127659576, 0.34674202127659576, 0.34441489361702127, 0.34341755319148937, 0.34674202127659576, 0.3497340425531915, 0.3480718085106383, 0.3460771276595745, 0.35299202134000496, 0.3543218085740475, 0.35498670219106876, 0.3569813830421326, 0.35166223410596237, 0.35930851070170705, 0.35804521276595747, 0.3570478723404255, 0.35811170238129636, 0.35804521276595747, 0.35738031914893614, 0.3564494683387432, 0.35804521276595747, 0.359375, 0.36183510625616033, 0.3560505319148936, 0.35578457472172187, 0.3551196811047006, 0.35711436195576446, 0.35811170238129636, 0.358444149189807, 0.35638297872340424, 0.35611702153023256, 0.35305851063829785, 0.35438829787234044, 0.35611702153023256, 0.3638962767225631, 0.3577792555727857, 0.36023936182894606, 0.36283244668169223, 0.3665558511906482, 0.36183510625616033, 0.3627659577004453, 0.3624999998731816, 0.36043883004087085, 0.3637632981259772, 0.362101064083424, 0.36183510625616033, 0.36988031927575454, 0.3607712768493815, 0.3605053190221178, 0.3642287235310737, 0.3638962767225631, 0.3615691490629886, 0.3628989362970312, 0.36216755306467097, 0.36456117033958435, 0.3627659577004453, 0.36449468072424546, 0.3681515956178625, 0.36316489349020287, 0.3598404254050965, 0.36117021263913907, 0.36449468072424546, 0.36449468072424546, 0.35910904280682826, 0.36788563842469074, 0.36382978710722413, 0.36243351089193465, 0.3652260639566056, 0.36183510625616033, 0.3632313831055418, 0.3634973402987135, 0.36216755306467097, 0.36117021263913907, 0.3651595743412667, 0.3598404254050965, 0.3630984045089559, 0.3615691490629886, 0.3638962767225631, 0.3567154255319149, 0.3591755317880752, 0.3607712768493815, 0.3624999998731816, 0.36456117033958435, 0.3605053190221178, 0.3642287235310737, 0.36555851076511625, 0.364893617148095, 0.3648271275327561, 0.3648271275327561, 0.36183510625616033, 0.36316489349020287, 0.36382978710722413, 0.36283244668169223, 0.3611037236578921, 0.365824467958288, 0.36343085131746655, 0.36748670200084116, 0.3620345744680851, 0.36382978710722413, 0.3678191488093518, 0.36841755344512617, 0.36648936157530926, 0.36449468072424546, 0.369481382851905, 0.36881648923488375, 0.36881648923488375, 0.3728058509370114, 0.3701462764689263, 0.3704787232774369, 0.369481382851905, 0.3650265957446808, 0.36881648923488375, 0.3651595743412667, 0.3684840424263731, 0.369481382851905, 0.3714760637029688, 0.36981382966041565, 0.369481382851905, 0.3720744683387432, 0.3718085105114795, 0.3714760637029688, 0.36476063855150914, 0.3711436168944582, 0.3701462764689263, 0.3716755319148936, 0.3693484042553192, 0.36941489387065807, 0.3704787232774369, 0.37001329787234044, 0.3670212765957447, 0.36941489387065807, 0.3684840424263731, 0.3683510638297872, 0.3680186170212766, 0.3646941489361702, 0.36436170212765956, 0.3650265957446808, 0.36336436170212766, 0.3670212765957447, 0.36908244706214743, 0.3697473406791687, 0.3683510638297872, 0.3693484042553192, 0.3706781914893617, 0.3687500002536368, 0.3690159574468085, 0.36881648923488375, 0.37001329787234044, 0.36808510663661553, 0.36768617021276595, 0.3680186170212766, 0.3693484042553192, 0.36868351063829785, 0.3707446811047006, 0.3724069151472538]



embedding_dim = 400
num_hidden_nodes = 150
num_output_nodes = 5
num_layers = 2
dropout = 0.4
lr = 1e-4
 



1
	Train Loss: 1.603 | Train Acc: 23.12%
	 Val. Loss: 1.589 |  Val. Acc: 25.51% 

2
	Train Loss: 1.600 | Train Acc: 24.15%
	 Val. Loss: 1.589 |  Val. Acc: 26.72% 

3
	Train Loss: 1.599 | Train Acc: 24.74%
	 Val. Loss: 1.588 |  Val. Acc: 26.70% 

4
	Train Loss: 1.596 | Train Acc: 25.64%
	 Val. Loss: 1.586 |  Val. Acc: 28.19% 

5
	Train Loss: 1.593 | Train Acc: 25.93%
	 Val. Loss: 1.583 |  Val. Acc: 27.87% 

6
	Train Loss: 1.588 | Train Acc: 27.46%
	 Val. Loss: 1.582 |  Val. Acc: 28.80% 

7
	Train Loss: 1.584 | Train Acc: 28.09%
	 Val. Loss: 1.576 |  Val. Acc: 30.26% 

8
	Train Loss: 1.576 | Train Acc: 28.92%
	 Val. Loss: 1.570 |  Val. Acc: 30.23% 

9
	Train Loss: 1.569 | Train Acc: 30.17%
	 Val. Loss: 1.567 |  Val. Acc: 30.82% 

10
	Train Loss: 1.554 | Train Acc: 32.65%
	 Val. Loss: 1.568 |  Val. Acc: 28.41% 

11
	Train Loss: 1.549 | Train Acc: 32.30%
	 Val. Loss: 1.561 |  Val. Acc: 30.74% 

12
	Train Loss: 1.536 | Train Acc: 34.51%
	 Val. Loss: 1.552 |  Val. Acc: 32.21% 

13
	Train Loss: 1.532 | Train Acc: 35.02%
	 Val. Loss: 1.546 |  Val. Acc: 32.74% 

14
	Train Loss: 1.521 | Train Acc: 36.35%
	 Val. Loss: 1.552 |  Val. Acc: 31.11% 

15
	Train Loss: 1.511 | Train Acc: 37.40%
	 Val. Loss: 1.546 |  Val. Acc: 32.42% 

16
	Train Loss: 1.507 | Train Acc: 38.24%
	 Val. Loss: 1.548 |  Val. Acc: 32.19% 

17
	Train Loss: 1.502 | Train Acc: 38.98%
	 Val. Loss: 1.544 |  Val. Acc: 33.06% 

18
	Train Loss: 1.492 | Train Acc: 39.76%
	 Val. Loss: 1.544 |  Val. Acc: 32.93% 

19
	Train Loss: 1.488 | Train Acc: 40.19%
	 Val. Loss: 1.549 |  Val. Acc: 32.46% 

20
	Train Loss: 1.476 | Train Acc: 42.09%
	 Val. Loss: 1.537 |  Val. Acc: 34.32% 

label distribution: actual vs pred: {2: {0: 205, 3: 92, 2: 124, 4: 3, 1: 132}, 0: {2: 126, 3: 110, 0: 384, 1: 152, 4: 4}, 1: {2: 85, 3: 243, 0: 152, 1: 270, 4: 2}, 4: {1: 52, 2: 71, 3: 67, 0: 206, 4: 5}, 3: {3: 247, 1: 133, 0: 66, 2: 31, 4: 2}}
	 pred vs actuals: {2: {0: 205, 3: 92, 2: 124, 4: 3, 1: 132}, 0: {2: 126, 3: 110, 0: 384, 1: 152, 4: 4}, 1: {2: 85, 3: 243, 0: 152, 1: 270, 4: 2}, 4: {1: 52, 2: 71, 3: 67, 0: 206, 4: 5}, 3: {3: 247, 1: 133, 0: 66, 2: 31, 4: 2}}
21
	Train Loss: 1.475 | Train Acc: 42.07%
	 Val. Loss: 1.536 |  Val. Acc: 34.29% 

22
	Train Loss: 1.467 | Train Acc: 43.16%
	 Val. Loss: 1.537 |  Val. Acc: 34.39% 

23
	Train Loss: 1.461 | Train Acc: 43.12%
	 Val. Loss: 1.542 |  Val. Acc: 33.46% 

24
	Train Loss: 1.460 | Train Acc: 43.41%
	 Val. Loss: 1.534 |  Val. Acc: 34.93% 

25
	Train Loss: 1.454 | Train Acc: 44.20%
	 Val. Loss: 1.543 |  Val. Acc: 34.10% 

26
	Train Loss: 1.445 | Train Acc: 45.31%
	 Val. Loss: 1.530 |  Val. Acc: 34.79% 

27
	Train Loss: 1.442 | Train Acc: 45.46%
	 Val. Loss: 1.531 |  Val. Acc: 35.16% 

28
	Train Loss: 1.435 | Train Acc: 46.32%
	 Val. Loss: 1.541 |  Val. Acc: 34.19% 

29
	Train Loss: 1.429 | Train Acc: 46.89%
	 Val. Loss: 1.536 |  Val. Acc: 34.69% 

30
	Train Loss: 1.423 | Train Acc: 47.68%
	 Val. Loss: 1.527 |  Val. Acc: 35.86% 

31
	Train Loss: 1.421 | Train Acc: 48.25%
	 Val. Loss: 1.537 |  Val. Acc: 34.32% 

32
	Train Loss: 1.410 | Train Acc: 49.33%
	 Val. Loss: 1.533 |  Val. Acc: 35.09% 

33
	Train Loss: 1.408 | Train Acc: 49.41%
	 Val. Loss: 1.524 |  Val. Acc: 35.84% 

34
	Train Loss: 1.404 | Train Acc: 49.73%
	 Val. Loss: 1.521 |  Val. Acc: 36.41% 

35
	Train Loss: 1.403 | Train Acc: 49.80%
	 Val. Loss: 1.527 |  Val. Acc: 35.60% 

36
	Train Loss: 1.398 | Train Acc: 50.66%
	 Val. Loss: 1.526 |  Val. Acc: 36.26% 

37
	Train Loss: 1.392 | Train Acc: 51.43%
	 Val. Loss: 1.525 |  Val. Acc: 36.26% 

38
	Train Loss: 1.382 | Train Acc: 52.31%
	 Val. Loss: 1.520 |  Val. Acc: 36.89% 

39
	Train Loss: 1.384 | Train Acc: 52.20%
	 Val. Loss: 1.520 |  Val. Acc: 36.76% 

40
	Train Loss: 1.376 | Train Acc: 53.42%
	 Val. Loss: 1.524 |  Val. Acc: 36.28% 

label distribution: actual vs pred: {2: {4: 30, 2: 135, 3: 69, 0: 159, 1: 163}, 0: {2: 148, 4: 38, 0: 348, 1: 158, 3: 84}, 1: {2: 93, 3: 214, 0: 95, 1: 336, 4: 14}, 4: {1: 62, 2: 60, 4: 33, 3: 45, 0: 201}, 3: {3: 235, 1: 163, 2: 28, 4: 11, 0: 42}}
	 pred vs actuals: {2: {4: 30, 2: 135, 3: 69, 0: 159, 1: 163}, 0: {2: 148, 4: 38, 0: 348, 1: 158, 3: 84}, 1: {2: 93, 3: 214, 0: 95, 1: 336, 4: 14}, 4: {1: 62, 2: 60, 4: 33, 3: 45, 0: 201}, 3: {3: 235, 1: 163, 2: 28, 4: 11, 0: 42}}
41
	Train Loss: 1.367 | Train Acc: 53.71%
	 Val. Loss: 1.521 |  Val. Acc: 36.83% 

42
	Train Loss: 1.369 | Train Acc: 53.48%
	 Val. Loss: 1.525 |  Val. Acc: 36.20% 

43
	Train Loss: 1.362 | Train Acc: 54.38%
	 Val. Loss: 1.520 |  Val. Acc: 36.70% 

44
	Train Loss: 1.360 | Train Acc: 54.80%
	 Val. Loss: 1.528 |  Val. Acc: 35.06% 

45
	Train Loss: 1.357 | Train Acc: 55.04%
	 Val. Loss: 1.520 |  Val. Acc: 36.72% 

46
	Train Loss: 1.351 | Train Acc: 55.34%
	 Val. Loss: 1.529 |  Val. Acc: 35.45% 

47
	Train Loss: 1.349 | Train Acc: 56.04%
	 Val. Loss: 1.525 |  Val. Acc: 35.96% 

48
	Train Loss: 1.344 | Train Acc: 56.48%
	 Val. Loss: 1.517 |  Val. Acc: 36.68% 

49
	Train Loss: 1.344 | Train Acc: 56.57%
	 Val. Loss: 1.522 |  Val. Acc: 37.16% 

50
	Train Loss: 1.335 | Train Acc: 57.69%
	 Val. Loss: 1.526 |  Val. Acc: 36.10% 

51
	Train Loss: 1.330 | Train Acc: 57.87%
	 Val. Loss: 1.529 |  Val. Acc: 35.60% 

52
	Train Loss: 1.331 | Train Acc: 57.99%
	 Val. Loss: 1.525 |  Val. Acc: 36.20% 

53
	Train Loss: 1.324 | Train Acc: 58.31%
	 Val. Loss: 1.522 |  Val. Acc: 36.55% 

54
	Train Loss: 1.325 | Train Acc: 58.08%
	 Val. Loss: 1.521 |  Val. Acc: 36.47% 

55
	Train Loss: 1.321 | Train Acc: 58.41%
	 Val. Loss: 1.520 |  Val. Acc: 36.82% 

56
	Train Loss: 1.321 | Train Acc: 58.91%
	 Val. Loss: 1.524 |  Val. Acc: 36.27% 

57
	Train Loss: 1.309 | Train Acc: 59.69%
	 Val. Loss: 1.521 |  Val. Acc: 36.37% 

58
	Train Loss: 1.308 | Train Acc: 59.99%
	 Val. Loss: 1.529 |  Val. Acc: 36.08% 

59
	Train Loss: 1.309 | Train Acc: 60.13%
	 Val. Loss: 1.514 |  Val. Acc: 37.77% 

60
	Train Loss: 1.301 | Train Acc: 61.15%
	 Val. Loss: 1.525 |  Val. Acc: 36.04% 

label distribution: actual vs pred: {2: {4: 32, 2: 146, 3: 64, 0: 154, 1: 160}, 0: {2: 178, 4: 55, 0: 334, 3: 67, 1: 142}, 1: {2: 109, 0: 100, 1: 327, 3: 191, 4: 25}, 4: {1: 59, 2: 66, 4: 55, 3: 35, 0: 186}, 3: {3: 209, 1: 188, 2: 30, 4: 10, 0: 42}}
	 pred vs actuals: {2: {4: 32, 2: 146, 3: 64, 0: 154, 1: 160}, 0: {2: 178, 4: 55, 0: 334, 3: 67, 1: 142}, 1: {2: 109, 0: 100, 1: 327, 3: 191, 4: 25}, 4: {1: 59, 2: 66, 4: 55, 3: 35, 0: 186}, 3: {3: 209, 1: 188, 2: 30, 4: 10, 0: 42}}
61
	Train Loss: 1.298 | Train Acc: 61.24%
	 Val. Loss: 1.532 |  Val. Acc: 35.51% 

62
	Train Loss: 1.299 | Train Acc: 61.19%
	 Val. Loss: 1.527 |  Val. Acc: 36.07% 

63
	Train Loss: 1.293 | Train Acc: 61.62%
	 Val. Loss: 1.522 |  Val. Acc: 36.51% 

64
	Train Loss: 1.293 | Train Acc: 61.33%
	 Val. Loss: 1.526 |  Val. Acc: 35.88% 

65
	Train Loss: 1.287 | Train Acc: 62.18%
	 Val. Loss: 1.522 |  Val. Acc: 36.72% 

66
	Train Loss: 1.283 | Train Acc: 62.76%
	 Val. Loss: 1.526 |  Val. Acc: 36.06% 

67
	Train Loss: 1.286 | Train Acc: 62.38%
	 Val. Loss: 1.527 |  Val. Acc: 36.24% 

68
	Train Loss: 1.283 | Train Acc: 62.48%
	 Val. Loss: 1.530 |  Val. Acc: 35.61% 

69
	Train Loss: 1.276 | Train Acc: 63.74%
	 Val. Loss: 1.518 |  Val. Acc: 36.82% 

70
	Train Loss: 1.273 | Train Acc: 64.15%
	 Val. Loss: 1.533 |  Val. Acc: 35.81% 

71
	Train Loss: 1.269 | Train Acc: 64.23%
	 Val. Loss: 1.542 |  Val. Acc: 34.63% 

72
	Train Loss: 1.271 | Train Acc: 63.68%
	 Val. Loss: 1.529 |  Val. Acc: 36.21% 

73
	Train Loss: 1.271 | Train Acc: 64.02%
	 Val. Loss: 1.528 |  Val. Acc: 35.99% 

74
	Train Loss: 1.268 | Train Acc: 64.08%
	 Val. Loss: 1.529 |  Val. Acc: 35.95% 

75
	Train Loss: 1.264 | Train Acc: 64.44%
	 Val. Loss: 1.527 |  Val. Acc: 36.04% 

76
	Train Loss: 1.262 | Train Acc: 64.34%
	 Val. Loss: 1.528 |  Val. Acc: 36.46% 

77
	Train Loss: 1.257 | Train Acc: 65.45%
	 Val. Loss: 1.529 |  Val. Acc: 35.82% 

78
	Train Loss: 1.252 | Train Acc: 65.77%
	 Val. Loss: 1.532 |  Val. Acc: 35.82% 

79
	Train Loss: 1.251 | Train Acc: 65.64%
	 Val. Loss: 1.530 |  Val. Acc: 36.14% 

80
	Train Loss: 1.251 | Train Acc: 65.69%
	 Val. Loss: 1.522 |  Val. Acc: 36.64% 

label distribution: actual vs pred: {2: {4: 30, 2: 154, 3: 55, 0: 148, 1: 169}, 0: {2: 189, 4: 68, 0: 320, 1: 146, 3: 53}, 1: {2: 117, 1: 352, 3: 174, 4: 20, 0: 89}, 4: {1: 57, 2: 74, 3: 29, 4: 60, 0: 181}, 3: {3: 203, 1: 187, 2: 41, 4: 12, 0: 36}}
	 pred vs actuals: {2: {4: 30, 2: 154, 3: 55, 0: 148, 1: 169}, 0: {2: 189, 4: 68, 0: 320, 1: 146, 3: 53}, 1: {2: 117, 1: 352, 3: 174, 4: 20, 0: 89}, 4: {1: 57, 2: 74, 3: 29, 4: 60, 0: 181}, 3: {3: 203, 1: 187, 2: 41, 4: 12, 0: 36}}
81
	Train Loss: 1.247 | Train Acc: 66.41%
	 Val. Loss: 1.529 |  Val. Acc: 36.14% 

82
	Train Loss: 1.247 | Train Acc: 66.43%
	 Val. Loss: 1.526 |  Val. Acc: 36.21% 

83
	Train Loss: 1.246 | Train Acc: 66.30%
	 Val. Loss: 1.523 |  Val. Acc: 36.58% 

84
	Train Loss: 1.243 | Train Acc: 66.49%
	 Val. Loss: 1.526 |  Val. Acc: 36.08% 

85
	Train Loss: 1.241 | Train Acc: 67.06%
	 Val. Loss: 1.520 |  Val. Acc: 37.41% 

86
	Train Loss: 1.242 | Train Acc: 66.31%
	 Val. Loss: 1.533 |  Val. Acc: 35.44% 

87
	Train Loss: 1.233 | Train Acc: 67.16%
	 Val. Loss: 1.524 |  Val. Acc: 36.32% 

88
	Train Loss: 1.237 | Train Acc: 67.32%
	 Val. Loss: 1.525 |  Val. Acc: 36.58% 

89
	Train Loss: 1.230 | Train Acc: 67.71%
	 Val. Loss: 1.525 |  Val. Acc: 36.51% 

90
	Train Loss: 1.228 | Train Acc: 68.21%
	 Val. Loss: 1.527 |  Val. Acc: 36.21% 

91
	Train Loss: 1.227 | Train Acc: 67.82%
	 Val. Loss: 1.526 |  Val. Acc: 36.38% 

92
	Train Loss: 1.229 | Train Acc: 67.66%
	 Val. Loss: 1.530 |  Val. Acc: 35.98% 

93
	Train Loss: 1.224 | Train Acc: 68.46%
	 Val. Loss: 1.528 |  Val. Acc: 35.78% 

94
	Train Loss: 1.218 | Train Acc: 69.32%
	 Val. Loss: 1.533 |  Val. Acc: 35.51% 

95
	Train Loss: 1.217 | Train Acc: 68.98%
	 Val. Loss: 1.536 |  Val. Acc: 34.93% 

96
	Train Loss: 1.216 | Train Acc: 69.05%
	 Val. Loss: 1.534 |  Val. Acc: 35.45% 

97
	Train Loss: 1.220 | Train Acc: 68.62%
	 Val. Loss: 1.533 |  Val. Acc: 35.31% 

98
	Train Loss: 1.208 | Train Acc: 69.88%
	 Val. Loss: 1.532 |  Val. Acc: 35.84% 

99
	Train Loss: 1.214 | Train Acc: 69.41%
	 Val. Loss: 1.531 |  Val. Acc: 36.05% 

100
	Train Loss: 1.210 | Train Acc: 69.76%
	 Val. Loss: 1.530 |  Val. Acc: 35.88% 

label distribution: actual vs pred: {2: {2: 155, 3: 52, 0: 157, 4: 40, 1: 152}, 0: {2: 190, 4: 88, 0: 309, 1: 140, 3: 49}, 1: {2: 143, 1: 327, 3: 162, 4: 23, 0: 97}, 4: {4: 82, 2: 73, 3: 30, 0: 173, 1: 43}, 3: {3: 191, 1: 184, 2: 50, 4: 18, 0: 36}}
	 pred vs actuals: {2: {2: 155, 3: 52, 0: 157, 4: 40, 1: 152}, 0: {2: 190, 4: 88, 0: 309, 1: 140, 3: 49}, 1: {2: 143, 1: 327, 3: 162, 4: 23, 0: 97}, 4: {4: 82, 2: 73, 3: 30, 0: 173, 1: 43}, 3: {3: 191, 1: 184, 2: 50, 4: 18, 0: 36}}
101
	Train Loss: 1.209 | Train Acc: 69.45%
	 Val. Loss: 1.532 |  Val. Acc: 35.88% 

102
	Train Loss: 1.214 | Train Acc: 69.53%
	 Val. Loss: 1.531 |  Val. Acc: 36.25% 

103
	Train Loss: 1.204 | Train Acc: 70.13%
	 Val. Loss: 1.527 |  Val. Acc: 36.25% 

104
	Train Loss: 1.203 | Train Acc: 70.20%
	 Val. Loss: 1.529 |  Val. Acc: 36.11% 

105
	Train Loss: 1.200 | Train Acc: 70.50%
	 Val. Loss: 1.528 |  Val. Acc: 36.32% 

106
	Train Loss: 1.207 | Train Acc: 70.66%
	 Val. Loss: 1.528 |  Val. Acc: 36.22% 

107
	Train Loss: 1.192 | Train Acc: 71.45%
	 Val. Loss: 1.530 |  Val. Acc: 36.22% 

108
	Train Loss: 1.202 | Train Acc: 70.24%
	 Val. Loss: 1.523 |  Val. Acc: 36.76% 

109
	Train Loss: 1.197 | Train Acc: 70.98%
	 Val. Loss: 1.525 |  Val. Acc: 36.75% 

110
	Train Loss: 1.201 | Train Acc: 70.40%
	 Val. Loss: 1.528 |  Val. Acc: 36.51% 

111
	Train Loss: 1.189 | Train Acc: 71.79%
	 Val. Loss: 1.528 |  Val. Acc: 36.45% 

112
	Train Loss: 1.188 | Train Acc: 71.79%
	 Val. Loss: 1.531 |  Val. Acc: 35.94% 

113
	Train Loss: 1.193 | Train Acc: 71.25%
	 Val. Loss: 1.527 |  Val. Acc: 36.01% 

114
	Train Loss: 1.193 | Train Acc: 71.13%
	 Val. Loss: 1.529 |  Val. Acc: 36.14% 

115
	Train Loss: 1.190 | Train Acc: 71.32%
	 Val. Loss: 1.534 |  Val. Acc: 35.70% 

116
	Train Loss: 1.186 | Train Acc: 71.73%
	 Val. Loss: 1.531 |  Val. Acc: 35.94% 

117
	Train Loss: 1.185 | Train Acc: 72.08%
	 Val. Loss: 1.531 |  Val. Acc: 35.94% 

118
	Train Loss: 1.182 | Train Acc: 72.16%
	 Val. Loss: 1.534 |  Val. Acc: 35.64% 

119
	Train Loss: 1.181 | Train Acc: 72.27%
	 Val. Loss: 1.524 |  Val. Acc: 36.97% 

120
	Train Loss: 1.181 | Train Acc: 71.92%
	 Val. Loss: 1.530 |  Val. Acc: 35.98% 

label distribution: actual vs pred: {2: {2: 150, 3: 47, 0: 140, 4: 40, 1: 179}, 0: {2: 186, 4: 87, 0: 304, 1: 154, 3: 45}, 1: {2: 119, 1: 361, 3: 162, 4: 22, 0: 88}, 4: {4: 73, 2: 72, 3: 26, 0: 171, 1: 59}, 3: {3: 181, 1: 207, 2: 45, 4: 15, 0: 31}}
	 pred vs actuals: {2: {2: 150, 3: 47, 0: 140, 4: 40, 1: 179}, 0: {2: 186, 4: 87, 0: 304, 1: 154, 3: 45}, 1: {2: 119, 1: 361, 3: 162, 4: 22, 0: 88}, 4: {4: 73, 2: 72, 3: 26, 0: 171, 1: 59}, 3: {3: 181, 1: 207, 2: 45, 4: 15, 0: 31}}
121
	Train Loss: 1.179 | Train Acc: 72.71%
	 Val. Loss: 1.533 |  Val. Acc: 35.23% 

122
	Train Loss: 1.180 | Train Acc: 72.43%
	 Val. Loss: 1.534 |  Val. Acc: 35.37% 

123
	Train Loss: 1.180 | Train Acc: 72.74%
	 Val. Loss: 1.530 |  Val. Acc: 35.82% 

124
	Train Loss: 1.178 | Train Acc: 72.99%
	 Val. Loss: 1.529 |  Val. Acc: 36.21% 

125
	Train Loss: 1.170 | Train Acc: 73.92%
	 Val. Loss: 1.530 |  Val. Acc: 35.78% 

126
	Train Loss: 1.180 | Train Acc: 72.69%
	 Val. Loss: 1.529 |  Val. Acc: 36.38% 

127
	Train Loss: 1.175 | Train Acc: 72.75%
	 Val. Loss: 1.533 |  Val. Acc: 35.72% 

128
	Train Loss: 1.174 | Train Acc: 72.90%
	 Val. Loss: 1.537 |  Val. Acc: 35.31% 

129
	Train Loss: 1.174 | Train Acc: 73.15%
	 Val. Loss: 1.531 |  Val. Acc: 36.55% 

130
	Train Loss: 1.173 | Train Acc: 73.18%
	 Val. Loss: 1.532 |  Val. Acc: 36.07% 

131
	Train Loss: 1.172 | Train Acc: 73.33%
	 Val. Loss: 1.530 |  Val. Acc: 36.24% 

132
	Train Loss: 1.167 | Train Acc: 73.74%
	 Val. Loss: 1.531 |  Val. Acc: 36.08% 

133
	Train Loss: 1.163 | Train Acc: 74.04%
	 Val. Loss: 1.532 |  Val. Acc: 36.11% 

134
	Train Loss: 1.166 | Train Acc: 73.42%
	 Val. Loss: 1.530 |  Val. Acc: 36.31% 

135
	Train Loss: 1.162 | Train Acc: 74.23%
	 Val. Loss: 1.530 |  Val. Acc: 35.88% 

136
	Train Loss: 1.163 | Train Acc: 73.95%
	 Val. Loss: 1.529 |  Val. Acc: 36.22% 

137
	Train Loss: 1.164 | Train Acc: 74.11%
	 Val. Loss: 1.527 |  Val. Acc: 36.38% 

138
	Train Loss: 1.161 | Train Acc: 74.35%
	 Val. Loss: 1.530 |  Val. Acc: 36.18% 

139
	Train Loss: 1.162 | Train Acc: 74.10%
	 Val. Loss: 1.527 |  Val. Acc: 36.62% 

140
	Train Loss: 1.158 | Train Acc: 74.46%
	 Val. Loss: 1.528 |  Val. Acc: 36.62% 

label distribution: actual vs pred: {2: {2: 159, 3: 42, 0: 158, 1: 152, 4: 45}, 0: {2: 173, 4: 100, 0: 330, 1: 136, 3: 37}, 1: {2: 117, 1: 348, 3: 155, 4: 24, 0: 108}, 4: {4: 80, 2: 68, 3: 23, 0: 179, 1: 51}, 3: {3: 169, 1: 212, 2: 43, 4: 21, 0: 34}}
	 pred vs actuals: {2: {2: 159, 3: 42, 0: 158, 1: 152, 4: 45}, 0: {2: 173, 4: 100, 0: 330, 1: 136, 3: 37}, 1: {2: 117, 1: 348, 3: 155, 4: 24, 0: 108}, 4: {4: 80, 2: 68, 3: 23, 0: 179, 1: 51}, 3: {3: 169, 1: 212, 2: 43, 4: 21, 0: 34}}
141
	Train Loss: 1.160 | Train Acc: 74.15%
	 Val. Loss: 1.530 |  Val. Acc: 36.43% 

142
	Train Loss: 1.158 | Train Acc: 74.66%
	 Val. Loss: 1.531 |  Val. Acc: 36.37% 

143
	Train Loss: 1.155 | Train Acc: 74.64%
	 Val. Loss: 1.536 |  Val. Acc: 35.57% 

144
	Train Loss: 1.156 | Train Acc: 74.80%
	 Val. Loss: 1.533 |  Val. Acc: 36.20% 

145
	Train Loss: 1.159 | Train Acc: 74.52%
	 Val. Loss: 1.530 |  Val. Acc: 36.14% 

146
	Train Loss: 1.154 | Train Acc: 74.84%
	 Val. Loss: 1.531 |  Val. Acc: 35.87% 

147
	Train Loss: 1.150 | Train Acc: 75.32%
	 Val. Loss: 1.535 |  Val. Acc: 35.57% 

148
	Train Loss: 1.154 | Train Acc: 75.27%
	 Val. Loss: 1.536 |  Val. Acc: 35.76% 

149
	Train Loss: 1.150 | Train Acc: 75.87%
	 Val. Loss: 1.539 |  Val. Acc: 35.70% 

150
	Train Loss: 1.155 | Train Acc: 74.70%
	 Val. Loss: 1.538 |  Val. Acc: 35.33% 

151
	Train Loss: 1.153 | Train Acc: 75.01%
	 Val. Loss: 1.537 |  Val. Acc: 35.61% 

152
	Train Loss: 1.152 | Train Acc: 74.98%
	 Val. Loss: 1.535 |  Val. Acc: 35.84% 

153
	Train Loss: 1.146 | Train Acc: 75.54%
	 Val. Loss: 1.537 |  Val. Acc: 35.98% 

154
	Train Loss: 1.148 | Train Acc: 75.57%
	 Val. Loss: 1.538 |  Val. Acc: 35.27% 

155
	Train Loss: 1.147 | Train Acc: 75.85%
	 Val. Loss: 1.531 |  Val. Acc: 36.14% 

156
	Train Loss: 1.148 | Train Acc: 75.46%
	 Val. Loss: 1.529 |  Val. Acc: 36.48% 

157
	Train Loss: 1.142 | Train Acc: 76.33%
	 Val. Loss: 1.532 |  Val. Acc: 36.04% 

158
	Train Loss: 1.141 | Train Acc: 76.08%
	 Val. Loss: 1.533 |  Val. Acc: 36.06% 

159
	Train Loss: 1.146 | Train Acc: 75.51%
	 Val. Loss: 1.537 |  Val. Acc: 35.44% 

160
	Train Loss: 1.142 | Train Acc: 76.35%
	 Val. Loss: 1.531 |  Val. Acc: 36.24% 

label distribution: actual vs pred: {2: {2: 154, 3: 45, 0: 160, 1: 156, 4: 41}, 0: {2: 182, 4: 81, 0: 331, 1: 139, 3: 43}, 1: {1: 345, 2: 127, 3: 158, 4: 24, 0: 98}, 4: {4: 71, 2: 77, 3: 22, 0: 181, 1: 50}, 3: {3: 176, 1: 213, 2: 44, 4: 13, 0: 33}}
	 pred vs actuals: {2: {2: 154, 3: 45, 0: 160, 1: 156, 4: 41}, 0: {2: 182, 4: 81, 0: 331, 1: 139, 3: 43}, 1: {1: 345, 2: 127, 3: 158, 4: 24, 0: 98}, 4: {4: 71, 2: 77, 3: 22, 0: 181, 1: 50}, 3: {3: 176, 1: 213, 2: 44, 4: 13, 0: 33}}
161
	Train Loss: 1.144 | Train Acc: 75.97%
	 Val. Loss: 1.534 |  Val. Acc: 35.58% 

162
	Train Loss: 1.140 | Train Acc: 76.09%
	 Val. Loss: 1.534 |  Val. Acc: 35.57% 

163
	Train Loss: 1.142 | Train Acc: 76.22%
	 Val. Loss: 1.534 |  Val. Acc: 36.07% 

164
	Train Loss: 1.139 | Train Acc: 76.57%
	 Val. Loss: 1.535 |  Val. Acc: 35.90% 

165
	Train Loss: 1.141 | Train Acc: 75.92%
	 Val. Loss: 1.537 |  Val. Acc: 35.72% 

166
	Train Loss: 1.137 | Train Acc: 76.10%
	 Val. Loss: 1.540 |  Val. Acc: 35.64% 

167
	Train Loss: 1.139 | Train Acc: 76.11%
	 Val. Loss: 1.537 |  Val. Acc: 35.74% 

168
	Train Loss: 1.134 | Train Acc: 76.69%
	 Val. Loss: 1.541 |  Val. Acc: 35.11% 

169
	Train Loss: 1.141 | Train Acc: 76.82%
	 Val. Loss: 1.537 |  Val. Acc: 35.76% 

170
	Train Loss: 1.137 | Train Acc: 76.41%
	 Val. Loss: 1.531 |  Val. Acc: 36.17% 

171
	Train Loss: 1.135 | Train Acc: 77.12%
	 Val. Loss: 1.536 |  Val. Acc: 35.73% 

172
	Train Loss: 1.132 | Train Acc: 77.03%
	 Val. Loss: 1.536 |  Val. Acc: 35.77% 

173
	Train Loss: 1.137 | Train Acc: 76.48%
	 Val. Loss: 1.532 |  Val. Acc: 36.11% 

174
	Train Loss: 1.130 | Train Acc: 77.38%
	 Val. Loss: 1.539 |  Val. Acc: 35.31% 

175
	Train Loss: 1.135 | Train Acc: 76.86%
	 Val. Loss: 1.529 |  Val. Acc: 36.45% 

176
	Train Loss: 1.132 | Train Acc: 76.49%
	 Val. Loss: 1.535 |  Val. Acc: 35.74% 

177
	Train Loss: 1.130 | Train Acc: 77.16%
	 Val. Loss: 1.533 |  Val. Acc: 35.90% 

178
	Train Loss: 1.129 | Train Acc: 77.65%
	 Val. Loss: 1.534 |  Val. Acc: 35.97% 

179
	Train Loss: 1.129 | Train Acc: 77.11%
	 Val. Loss: 1.538 |  Val. Acc: 35.66% 

180
	Train Loss: 1.128 | Train Acc: 77.62%
	 Val. Loss: 1.531 |  Val. Acc: 35.97% 

label distribution: actual vs pred: {2: {2: 149, 3: 44, 0: 168, 1: 151, 4: 44}, 0: {2: 174, 4: 88, 0: 340, 1: 136, 3: 38}, 1: {1: 336, 3: 156, 2: 136, 4: 20, 0: 104}, 4: {4: 74, 2: 67, 3: 20, 0: 190, 1: 50}, 3: {3: 172, 2: 40, 1: 212, 4: 17, 0: 38}}
	 pred vs actuals: {2: {2: 149, 3: 44, 0: 168, 1: 151, 4: 44}, 0: {2: 174, 4: 88, 0: 340, 1: 136, 3: 38}, 1: {1: 336, 3: 156, 2: 136, 4: 20, 0: 104}, 4: {4: 74, 2: 67, 3: 20, 0: 190, 1: 50}, 3: {3: 172, 2: 40, 1: 212, 4: 17, 0: 38}}
181
	Train Loss: 1.127 | Train Acc: 77.33%
	 Val. Loss: 1.532 |  Val. Acc: 36.04% 

182
	Train Loss: 1.129 | Train Acc: 77.66%
	 Val. Loss: 1.533 |  Val. Acc: 35.96% 

183
	Train Loss: 1.130 | Train Acc: 77.07%
	 Val. Loss: 1.537 |  Val. Acc: 35.57% 

184
	Train Loss: 1.130 | Train Acc: 77.00%
	 Val. Loss: 1.532 |  Val. Acc: 36.30% 

185
	Train Loss: 1.126 | Train Acc: 77.13%
	 Val. Loss: 1.532 |  Val. Acc: 36.13% 

186
	Train Loss: 1.127 | Train Acc: 77.53%
	 Val. Loss: 1.533 |  Val. Acc: 36.00% 

187
	Train Loss: 1.127 | Train Acc: 77.51%
	 Val. Loss: 1.528 |  Val. Acc: 36.54% 

188
	Train Loss: 1.128 | Train Acc: 77.36%
	 Val. Loss: 1.530 |  Val. Acc: 36.24% 

189
	Train Loss: 1.130 | Train Acc: 77.20%
	 Val. Loss: 1.528 |  Val. Acc: 36.70% 

190
	Train Loss: 1.127 | Train Acc: 77.17%
	 Val. Loss: 1.531 |  Val. Acc: 36.06% 

191
	Train Loss: 1.123 | Train Acc: 77.54%
	 Val. Loss: 1.536 |  Val. Acc: 35.64% 

192
	Train Loss: 1.126 | Train Acc: 77.60%
	 Val. Loss: 1.533 |  Val. Acc: 35.77% 

193
	Train Loss: 1.126 | Train Acc: 77.57%
	 Val. Loss: 1.531 |  Val. Acc: 36.17% 

194
	Train Loss: 1.123 | Train Acc: 77.92%
	 Val. Loss: 1.533 |  Val. Acc: 36.03% 

195
	Train Loss: 1.124 | Train Acc: 77.44%
	 Val. Loss: 1.531 |  Val. Acc: 36.07% 

196
	Train Loss: 1.123 | Train Acc: 78.27%
	 Val. Loss: 1.532 |  Val. Acc: 36.16% 

197
	Train Loss: 1.120 | Train Acc: 78.07%
	 Val. Loss: 1.535 |  Val. Acc: 35.73% 

198
	Train Loss: 1.118 | Train Acc: 78.33%
	 Val. Loss: 1.538 |  Val. Acc: 35.47% 

199
	Train Loss: 1.117 | Train Acc: 78.11%
	 Val. Loss: 1.532 |  Val. Acc: 36.16% 

200
	Train Loss: 1.123 | Train Acc: 77.63%
	 Val. Loss: 1.535 |  Val. Acc: 35.60% 

label distribution: actual vs pred: {2: {2: 157, 3: 42, 0: 161, 1: 154, 4: 42}, 0: {2: 181, 4: 94, 0: 336, 1: 124, 3: 41}, 1: {1: 306, 3: 159, 2: 146, 4: 23, 0: 118}, 4: {4: 84, 2: 66, 3: 21, 0: 178, 1: 52}, 3: {3: 179, 1: 198, 2: 46, 4: 17, 0: 39}}
	 pred vs actuals: {2: {2: 157, 3: 42, 0: 161, 1: 154, 4: 42}, 0: {2: 181, 4: 94, 0: 336, 1: 124, 3: 41}, 1: {1: 306, 3: 159, 2: 146, 4: 23, 0: 118}, 4: {4: 84, 2: 66, 3: 21, 0: 178, 1: 52}, 3: {3: 179, 1: 198, 2: 46, 4: 17, 0: 39}}
     