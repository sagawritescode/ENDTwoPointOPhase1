{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sagar: Sentiment Analysis using LSTM RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagawritescode/ENDTwoPointOPhase1/blob/main/Sagar_Sentiment_Analysis_using_LSTM_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwd9mTxm2fBj"
      },
      "source": [
        "import random\n",
        "import torch, torchtext.legacy\n",
        "from torchtext.legacy import data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrdMw3L8oRlC"
      },
      "source": [
        "## Source of data: PyTreebank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-diBAzDzjvDH",
        "outputId": "11071d76-34c4-4337-dbef-2898fc64e11e"
      },
      "source": [
        "!pip install pytreebank"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytreebank\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/12/626ead6f6c0a0a9617396796b965961e9dfa5e78b36c17a81ea4c43554b1/pytreebank-0.2.7.tar.gz\n",
            "Building wheels for collected packages: pytreebank\n",
            "  Building wheel for pytreebank (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytreebank: filename=pytreebank-0.2.7-cp37-none-any.whl size=37070 sha256=af713eaa89228e69968ec04ddc33e1d93b99256b27046973c43ea3f69e81a4c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/b6/91/e9edcdbf464f623628d5c3aa9de28888c726e270b9a29f2368\n",
            "Successfully built pytreebank\n",
            "Installing collected packages: pytreebank\n",
            "Successfully installed pytreebank-0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg3cweb21wqO"
      },
      "source": [
        "import pytreebank\n",
        "dataset = pytreebank.load_sst()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Um8focl2aAD"
      },
      "source": [
        "Review = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ParVL3_2qSE"
      },
      "source": [
        "fields = [('reviews', Review),('labels',Label)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY_oq21v2A8H"
      },
      "source": [
        "# Use list to collect data as it is more efficient for appending\n",
        "totaldata = []\n",
        "\n",
        "# Pytreebank has three different datasets. Combining all of them \n",
        "for databatch in [dataset[\"train\"], dataset[\"dev\"], dataset[\"test\"]]:\n",
        "  for example in databatch:\n",
        "    i= 0\n",
        "    for label, sentence in example.to_labeled_lines():\n",
        "      if i == 0:\n",
        "        totaldata.append([sentence, label])\n",
        "        break"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjmUyirOAEOK"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvtbg8JSJy6h"
      },
      "source": [
        "Random Delete and Random Swap functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67DcUWictLlV"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining\n",
        " \n",
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAklFbNfJ4ye"
      },
      "source": [
        "Backtranslate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ1pQqb_mWt0",
        "outputId": "76511777-c271-4136-d8d7-0ebbdf267c14"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.5.30)\n",
            "Collecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.3MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=cf9762e03bad23eaa47bf862c99afbd3c02004a6713f6473e55dc368ffc67203\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hpack, hyperframe, h2, sniffio, h11, httpcore, rfc3986, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRHkaRMDmCFP"
      },
      "source": [
        "import random\n",
        "import googletrans\n",
        " \n",
        "def backtranslate(sentence):\n",
        "  translator = googletrans.Translator()\n",
        " \n",
        "  available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "  trans_lang = random.choice(available_langs) \n",
        " \n",
        "  translations = translator.translate(sentence, dest=trans_lang).text \n",
        " \n",
        "  translations_en_random = translator.translate(translations, src=trans_lang, dest='en').text\n",
        "  \n",
        "  return translations_en_random"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P48BoFP4J8KD"
      },
      "source": [
        "Checking class distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfDAFB_-pVkm",
        "outputId": "7885348b-28d4-4ae7-9c67-4a9277ee083a"
      },
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# A textual way to print frequency per label \n",
        "def print_frequency(data):\n",
        "  x, y = list(zip(*data))\n",
        "  # frequency of labels\n",
        "  label_frequency = collections.Counter(y)\n",
        "  print(\"frequency distribution: \", label_frequency)\n",
        " \n",
        "print_frequency(totaldata)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequency distribution:  Counter({1: 3140, 3: 3111, 2: 2242, 4: 1852, 0: 1510})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO34E_--iGCd"
      },
      "source": [
        "# Define split percentage here\n",
        "split_percentage = 0.75"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM9PzULSi8zk"
      },
      "source": [
        "def augment_data(sentence):\n",
        "  n = random.randint(1,3)\n",
        "  if n == 1:\n",
        "    sentence = ''.join(random_deletion(list(sentence)))\n",
        "  elif n == 2:\n",
        "    sentence = ''.join(random_swap(list(sentence)))\n",
        "  else:\n",
        "    sentence = backtranslate(sentence)\n",
        "  return sentence"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9gZjT_4Iat3"
      },
      "source": [
        "def get_label_elements(trainlist, label):\n",
        "   return [x for x in trainlist if x[1] == label]\n",
        " \n",
        "def get_int_index(split, list_to_sample):\n",
        "  return int(split*len(list_to_sample))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiLz4mOtKYHp"
      },
      "source": [
        "Finally split train-test and augment train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT_HFdjUam15",
        "outputId": "d539c03e-b88d-4e7b-8c3f-214671c7bfb8"
      },
      "source": [
        "import random\n",
        "random.shuffle(totaldata)\n",
        " \n",
        "split_index = get_int_index(split_percentage,totaldata)\n",
        " \n",
        "trainlist = totaldata[:split_index]\n",
        "testlist = totaldata[split_index:]\n",
        " \n",
        "changedsent = 0\n",
        "length = len(trainlist)\n",
        "iterator = 0\n",
        " \n",
        "print_frequency(trainlist)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequency distribution:  Counter({1: 2351, 3: 2326, 2: 1686, 4: 1386, 0: 1142})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l18M_8eon9Pz",
        "outputId": "7b20997b-3200-44c9-ba31-0dc8fb99b1f8"
      },
      "source": [
        " \n",
        "train_very_negative = get_label_elements(trainlist, 0)\n",
        "train_negative = get_label_elements(trainlist, 1)\n",
        "train_neutral = get_label_elements(trainlist, 2)\n",
        "train_positive = get_label_elements(trainlist, 3)\n",
        "train_very_positive = get_label_elements(trainlist, 4)\n",
        "\n",
        "# Discrepancy: Using sample here and using shuffle and split above. We realised that, but not changing as realised that late during submission\n",
        "train_list_aug = [x for label_list in [random.sample(train_very_negative, get_int_index(1, train_very_negative)), \n",
        "                              random.sample(train_negative, get_int_index(0.2, train_negative)), random.sample(train_neutral, get_int_index(0.5, train_neutral)), \n",
        "                              random.sample(train_positive, get_int_index(0.2, train_positive)), random.sample(train_very_positive, get_int_index(0.8, train_very_positive))] for x in label_list ]\n",
        " \n",
        "for i in train_list_aug:\n",
        "    if iterator%100 == 0:\n",
        "        print(\"sentences changed: \", iterator)\n",
        "    iterator +=1\n",
        "    x,y = i\n",
        "    x = augment_data(x)\n",
        "    trainlist.append((x, y))\n",
        " \n",
        "print_frequency(trainlist)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentences changed:  0\n",
            "sentences changed:  100\n",
            "sentences changed:  200\n",
            "sentences changed:  300\n",
            "sentences changed:  400\n",
            "sentences changed:  500\n",
            "sentences changed:  600\n",
            "sentences changed:  700\n",
            "sentences changed:  800\n",
            "sentences changed:  900\n",
            "sentences changed:  1000\n",
            "sentences changed:  1100\n",
            "sentences changed:  1200\n",
            "sentences changed:  1300\n",
            "sentences changed:  1400\n",
            "sentences changed:  1500\n",
            "sentences changed:  1600\n",
            "sentences changed:  1700\n",
            "sentences changed:  1800\n",
            "sentences changed:  1900\n",
            "sentences changed:  2000\n",
            "sentences changed:  2100\n",
            "sentences changed:  2200\n",
            "sentences changed:  2300\n",
            "sentences changed:  2400\n",
            "sentences changed:  2500\n",
            "sentences changed:  2600\n",
            "sentences changed:  2700\n",
            "sentences changed:  2800\n",
            "sentences changed:  2900\n",
            "sentences changed:  3000\n",
            "sentences changed:  3100\n",
            "sentences changed:  3200\n",
            "sentences changed:  3300\n",
            "sentences changed:  3400\n",
            "sentences changed:  3500\n",
            "sentences changed:  3600\n",
            "sentences changed:  3700\n",
            "sentences changed:  3800\n",
            "sentences changed:  3900\n",
            "sentences changed:  4000\n",
            "frequency distribution:  Counter({1: 2821, 3: 2791, 2: 2529, 4: 2494, 0: 2284})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNQDbXqtEm1F"
      },
      "source": [
        "example1 = [data.Example.fromlist([trainlist[i][0], trainlist[i][1]], fields) for i in range(len(trainlist))]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6lvWT4RNCRB"
      },
      "source": [
        "example2 = [data.Example.fromlist([testlist[i][0], testlist[i][1]], fields) for i in range(len(testlist))]"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq-J1txtFncT",
        "outputId": "3498d034-55f1-4782-c30e-dfd5ec1f4edc"
      },
      "source": [
        "SEED = 230\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f39fbc0a7d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKj-ztcvFxNr"
      },
      "source": [
        "train = data.Dataset(example1, fields)\n",
        "valid = data.Dataset(example2, fields)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeXWILQzJZQR",
        "outputId": "e5e5315f-2859-4f01-8207-0e1ee925b738"
      },
      "source": [
        "(len(train), len(valid))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12919, 2964)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAzDVixJjth4"
      },
      "source": [
        "Review.build_vocab(train)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8ER7VMIIRrE",
        "outputId": "d6a4e48a-14a9-43a9-8d36-bf28f59b0eaf"
      },
      "source": [
        "print('Size of input vocab : ', len(Review.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Review.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)\n",
        "\n",
        "print('label freq:', list(Label.vocab.freqs.most_common(6)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  31562\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 11381), (',', 10297), ('the', 7890), ('a', 6064), ('of', 5807), ('and', 5781), ('to', 4022), ('-', 3943), ('is', 3420), (\"'s\", 3368)]\n",
            "Labels :  defaultdict(None, {1: 0, 3: 1, 2: 2, 4: 3, 0: 4})\n",
            "label freq: [(1, 2821), (3, 2791), (2, 2529), (4, 2494), (0, 2284)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sli3x8PBIgq8"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72zB41yiIjRg"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = 64, \n",
        "                                                            sort_key = lambda x: len(x.reviews),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLbRChlDAM-J"
      },
      "source": [
        "## Save the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw-8icYQXuAV",
        "outputId": "31e8a002-5ae1-4ac9-abd4-eedb8ccb2fa8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEo7zJ96XwD8"
      },
      "source": [
        "!cd /content/drive/MyDrive/assign5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORRSFhHnJk_q"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Review.vocab.stoi, tokens)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJPc6liGxZUT"
      },
      "source": [
        "with open('train.pkl', 'wb') as tokens: \n",
        "    pickle.dump(train, tokens)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxWe4aFJxnWP"
      },
      "source": [
        "with open('valid.pkl', 'wb') as tokens: \n",
        "    pickle.dump(valid, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW8z1ybQAPgC"
      },
      "source": [
        "# Defining the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISw_Sux5Jrgk"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        " \n",
        "class classifierLSTM(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True,\n",
        "                           bidirectional = False\n",
        "                           )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        embedded = self.dropout(embedded)\n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        " \n",
        "        dense_outputs = self.dropout(dense_outputs)\n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output#dense_outputs[0]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je_qE7DaZ27m"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        " \n",
        "class classifierLSTM_v2(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True,\n",
        "                           bidirectional = False\n",
        "                           )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc1 = nn.Linear(hidden_dim, output_dim*5)\n",
        "\n",
        "        self.fc2 = nn.Linear(output_dim*5, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        embedded = self.dropout(embedded)\n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs1 = self.fc1(hidden)   \n",
        "\n",
        "        dense_outputs1 = self.dropout(dense_outputs1)\n",
        "\n",
        "        dense_outputs = self.fc2(dense_outputs1)\n",
        "        dense_outputs = self.dropout(dense_outputs)\n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output#dense_outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX11D_xwKnBM"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuYNXwUnJ0QS"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Review.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = 5\n",
        "num_layers = 2\n",
        "dropout = 0.3\n",
        "lr = 1e-4\n",
        "# Instantiate the model\n",
        "model = classifierLSTM(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPhOqXivKb6Z",
        "outputId": "063ed439-2e7f-41ca-ede6-13eca0edd995"
      },
      "source": [
        "print(model)\n",
        " \n",
        "#No. of trainable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierLSTM(\n",
            "  (embedding): Embedding(31562, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
            ")\n",
            "The model has 9,710,705 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBmoKGbXKi9y"
      },
      "source": [
        "import torch.optim as optim\n",
        " \n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHE561Q4iij_"
      },
      "source": [
        "# Our own way of printing actual vs predicted (quantity) which we later realised is called confusion matrix.\n",
        "def get_pred_vs_act_per_label(pred_act):\n",
        "  preds, act = list(zip(*pred_act))\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  for i in preds:\n",
        "    _, pred = torch.max(i, 1)\n",
        "    for j in pred:\n",
        "      predictions.append(j.item())\n",
        "  for i in act:\n",
        "    for j in i:\n",
        "      actuals.append(j.item())\n",
        " \n",
        "  assert len(predictions) == len(actuals)\n",
        " \n",
        "  label_dict = {}\n",
        "  \n",
        "  for i in range(0, len(actuals)):\n",
        "    predic = predictions[i]\n",
        "    actual = actuals[i]\n",
        "    if label_dict.get(actual, None) != None:\n",
        "      in_dict = label_dict[actual]\n",
        "      if in_dict.get(predic, None) != None:\n",
        "        in_dict[predic] += 1\n",
        "      else:\n",
        "        in_dict[predic] = 1\n",
        "    else:\n",
        "      in_dict = {predic: 1}\n",
        "    label_dict[actual] = in_dict\n",
        "  print(\"label distribution: actual vs pred:\", label_dict)\n",
        "  return label_dict"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz5LzuJOKtSb"
      },
      "source": [
        "import torch.nn.functional as Fun\n",
        " \n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        review, review_lengths = batch.reviews   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        beforesqueeze = model(review, review_lengths)  \n",
        "        predictions = beforesqueeze.squeeze()\n",
        "        # compute the loss       \n",
        "        \n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ll9CV0Ku7O"
      },
      "source": [
        "def evaluate(model, iterator, criterion, preds_actual_tup):\n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        " \n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            review, review_lengths = batch.reviews\n",
        "            #print(review)\n",
        "            # convert to 1d tensor\n",
        "            predictions = model(review, review_lengths).squeeze()\n",
        "            #print(predictions)\n",
        "            # compute loss and accuracy\n",
        "            preds_actual_tup.append((predictions, batch.labels))\n",
        "            loss = criterion(predictions, batch.labels)\n",
        " \n",
        "            #print(batch.labels)\n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKfRzd9YKz1D",
        "outputId": "65a339e4-d233-4d28-d03d-efdc9790816c"
      },
      "source": [
        "N_EPOCHS = 300\n",
        "best_valid_loss = float('inf')\n",
        " \n",
        " \n",
        "epochi = 0\n",
        "train_acc_list = []\n",
        "valid_acc_list = []\n",
        " \n",
        "for epoch in range(N_EPOCHS):\n",
        " \n",
        "    epochi += 1 \n",
        "    \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    train_acc_list.append(train_acc)\n",
        " \n",
        "    preds_actual_tup = []\n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, preds_actual_tup)\n",
        "    valid_acc_list.append(valid_acc)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    print(epochi)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')\n",
        "    if epochi%20 == 0:\n",
        "      print(f'\\t pred vs actuals: {get_pred_vs_act_per_label(preds_actual_tup)}')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "\tTrain Loss: 1.607 | Train Acc: 21.84%\n",
            "\t Val. Loss: 1.595 |  Val. Acc: 27.93% \n",
            "\n",
            "2\n",
            "\tTrain Loss: 1.604 | Train Acc: 23.13%\n",
            "\t Val. Loss: 1.590 |  Val. Acc: 28.03% \n",
            "\n",
            "3\n",
            "\tTrain Loss: 1.603 | Train Acc: 23.71%\n",
            "\t Val. Loss: 1.589 |  Val. Acc: 28.20% \n",
            "\n",
            "4\n",
            "\tTrain Loss: 1.601 | Train Acc: 24.10%\n",
            "\t Val. Loss: 1.588 |  Val. Acc: 29.13% \n",
            "\n",
            "5\n",
            "\tTrain Loss: 1.598 | Train Acc: 25.16%\n",
            "\t Val. Loss: 1.585 |  Val. Acc: 27.66% \n",
            "\n",
            "6\n",
            "\tTrain Loss: 1.595 | Train Acc: 26.07%\n",
            "\t Val. Loss: 1.584 |  Val. Acc: 30.05% \n",
            "\n",
            "7\n",
            "\tTrain Loss: 1.591 | Train Acc: 26.88%\n",
            "\t Val. Loss: 1.581 |  Val. Acc: 30.18% \n",
            "\n",
            "8\n",
            "\tTrain Loss: 1.586 | Train Acc: 27.61%\n",
            "\t Val. Loss: 1.579 |  Val. Acc: 29.98% \n",
            "\n",
            "9\n",
            "\tTrain Loss: 1.577 | Train Acc: 29.77%\n",
            "\t Val. Loss: 1.576 |  Val. Acc: 29.81% \n",
            "\n",
            "10\n",
            "\tTrain Loss: 1.568 | Train Acc: 30.89%\n",
            "\t Val. Loss: 1.569 |  Val. Acc: 29.43% \n",
            "\n",
            "11\n",
            "\tTrain Loss: 1.558 | Train Acc: 30.88%\n",
            "\t Val. Loss: 1.562 |  Val. Acc: 30.71% \n",
            "\n",
            "12\n",
            "\tTrain Loss: 1.548 | Train Acc: 33.64%\n",
            "\t Val. Loss: 1.560 |  Val. Acc: 31.13% \n",
            "\n",
            "13\n",
            "\tTrain Loss: 1.536 | Train Acc: 34.50%\n",
            "\t Val. Loss: 1.570 |  Val. Acc: 29.79% \n",
            "\n",
            "14\n",
            "\tTrain Loss: 1.527 | Train Acc: 36.04%\n",
            "\t Val. Loss: 1.560 |  Val. Acc: 31.29% \n",
            "\n",
            "15\n",
            "\tTrain Loss: 1.521 | Train Acc: 36.17%\n",
            "\t Val. Loss: 1.564 |  Val. Acc: 30.85% \n",
            "\n",
            "16\n",
            "\tTrain Loss: 1.515 | Train Acc: 36.88%\n",
            "\t Val. Loss: 1.553 |  Val. Acc: 33.00% \n",
            "\n",
            "17\n",
            "\tTrain Loss: 1.501 | Train Acc: 38.73%\n",
            "\t Val. Loss: 1.552 |  Val. Acc: 32.71% \n",
            "\n",
            "18\n",
            "\tTrain Loss: 1.496 | Train Acc: 39.33%\n",
            "\t Val. Loss: 1.555 |  Val. Acc: 31.98% \n",
            "\n",
            "19\n",
            "\tTrain Loss: 1.492 | Train Acc: 39.36%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 33.54% \n",
            "\n",
            "20\n",
            "\tTrain Loss: 1.487 | Train Acc: 40.08%\n",
            "\t Val. Loss: 1.554 |  Val. Acc: 32.31% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 71, 3: 168, 0: 386, 1: 135, 4: 29}, 4: {1: 40, 2: 33, 3: 96, 4: 24, 0: 175}, 3: {2: 23, 3: 263, 0: 57, 1: 110, 4: 13}, 1: {3: 319, 1: 218, 0: 176, 4: 19, 2: 53}, 2: {0: 213, 2: 70, 3: 132, 1: 119, 4: 22}}\n",
            "\t pred vs actuals: {0: {2: 71, 3: 168, 0: 386, 1: 135, 4: 29}, 4: {1: 40, 2: 33, 3: 96, 4: 24, 0: 175}, 3: {2: 23, 3: 263, 0: 57, 1: 110, 4: 13}, 1: {3: 319, 1: 218, 0: 176, 4: 19, 2: 53}, 2: {0: 213, 2: 70, 3: 132, 1: 119, 4: 22}}\n",
            "21\n",
            "\tTrain Loss: 1.479 | Train Acc: 40.84%\n",
            "\t Val. Loss: 1.559 |  Val. Acc: 32.05% \n",
            "\n",
            "22\n",
            "\tTrain Loss: 1.475 | Train Acc: 41.57%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 33.68% \n",
            "\n",
            "23\n",
            "\tTrain Loss: 1.466 | Train Acc: 42.26%\n",
            "\t Val. Loss: 1.556 |  Val. Acc: 32.55% \n",
            "\n",
            "24\n",
            "\tTrain Loss: 1.461 | Train Acc: 42.89%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 33.82% \n",
            "\n",
            "25\n",
            "\tTrain Loss: 1.453 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.550 |  Val. Acc: 33.02% \n",
            "\n",
            "26\n",
            "\tTrain Loss: 1.445 | Train Acc: 44.91%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 33.68% \n",
            "\n",
            "27\n",
            "\tTrain Loss: 1.444 | Train Acc: 44.77%\n",
            "\t Val. Loss: 1.550 |  Val. Acc: 33.36% \n",
            "\n",
            "28\n",
            "\tTrain Loss: 1.441 | Train Acc: 44.83%\n",
            "\t Val. Loss: 1.554 |  Val. Acc: 32.37% \n",
            "\n",
            "29\n",
            "\tTrain Loss: 1.434 | Train Acc: 46.19%\n",
            "\t Val. Loss: 1.551 |  Val. Acc: 32.91% \n",
            "\n",
            "30\n",
            "\tTrain Loss: 1.425 | Train Acc: 47.12%\n",
            "\t Val. Loss: 1.553 |  Val. Acc: 32.25% \n",
            "\n",
            "31\n",
            "\tTrain Loss: 1.419 | Train Acc: 47.63%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 33.15% \n",
            "\n",
            "32\n",
            "\tTrain Loss: 1.415 | Train Acc: 48.13%\n",
            "\t Val. Loss: 1.551 |  Val. Acc: 32.81% \n",
            "\n",
            "33\n",
            "\tTrain Loss: 1.411 | Train Acc: 48.29%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 34.12% \n",
            "\n",
            "34\n",
            "\tTrain Loss: 1.405 | Train Acc: 49.26%\n",
            "\t Val. Loss: 1.550 |  Val. Acc: 33.01% \n",
            "\n",
            "35\n",
            "\tTrain Loss: 1.401 | Train Acc: 49.32%\n",
            "\t Val. Loss: 1.551 |  Val. Acc: 32.61% \n",
            "\n",
            "36\n",
            "\tTrain Loss: 1.396 | Train Acc: 50.48%\n",
            "\t Val. Loss: 1.549 |  Val. Acc: 33.40% \n",
            "\n",
            "37\n",
            "\tTrain Loss: 1.388 | Train Acc: 50.84%\n",
            "\t Val. Loss: 1.544 |  Val. Acc: 33.72% \n",
            "\n",
            "38\n",
            "\tTrain Loss: 1.384 | Train Acc: 51.57%\n",
            "\t Val. Loss: 1.549 |  Val. Acc: 33.14% \n",
            "\n",
            "39\n",
            "\tTrain Loss: 1.381 | Train Acc: 51.64%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 33.68% \n",
            "\n",
            "40\n",
            "\tTrain Loss: 1.380 | Train Acc: 51.80%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 34.42% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 87, 3: 137, 0: 363, 4: 49, 1: 153}, 4: {2: 20, 3: 86, 4: 40, 0: 176, 1: 46}, 3: {2: 20, 3: 260, 0: 46, 1: 131, 4: 9}, 1: {3: 267, 1: 288, 0: 133, 4: 25, 2: 72}, 2: {0: 199, 3: 110, 1: 157, 2: 69, 4: 21}}\n",
            "\t pred vs actuals: {0: {2: 87, 3: 137, 0: 363, 4: 49, 1: 153}, 4: {2: 20, 3: 86, 4: 40, 0: 176, 1: 46}, 3: {2: 20, 3: 260, 0: 46, 1: 131, 4: 9}, 1: {3: 267, 1: 288, 0: 133, 4: 25, 2: 72}, 2: {0: 199, 3: 110, 1: 157, 2: 69, 4: 21}}\n",
            "41\n",
            "\tTrain Loss: 1.369 | Train Acc: 53.21%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 34.53% \n",
            "\n",
            "42\n",
            "\tTrain Loss: 1.366 | Train Acc: 53.29%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 34.39% \n",
            "\n",
            "43\n",
            "\tTrain Loss: 1.362 | Train Acc: 53.87%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 33.88% \n",
            "\n",
            "44\n",
            "\tTrain Loss: 1.360 | Train Acc: 54.15%\n",
            "\t Val. Loss: 1.552 |  Val. Acc: 32.95% \n",
            "\n",
            "45\n",
            "\tTrain Loss: 1.353 | Train Acc: 54.90%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 33.46% \n",
            "\n",
            "46\n",
            "\tTrain Loss: 1.350 | Train Acc: 54.97%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 33.68% \n",
            "\n",
            "47\n",
            "\tTrain Loss: 1.343 | Train Acc: 56.62%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 34.39% \n",
            "\n",
            "48\n",
            "\tTrain Loss: 1.338 | Train Acc: 56.27%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 34.13% \n",
            "\n",
            "49\n",
            "\tTrain Loss: 1.335 | Train Acc: 56.66%\n",
            "\t Val. Loss: 1.544 |  Val. Acc: 33.72% \n",
            "\n",
            "50\n",
            "\tTrain Loss: 1.327 | Train Acc: 57.37%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 33.53% \n",
            "\n",
            "51\n",
            "\tTrain Loss: 1.331 | Train Acc: 56.86%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 34.61% \n",
            "\n",
            "52\n",
            "\tTrain Loss: 1.327 | Train Acc: 57.64%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 33.54% \n",
            "\n",
            "53\n",
            "\tTrain Loss: 1.317 | Train Acc: 58.57%\n",
            "\t Val. Loss: 1.553 |  Val. Acc: 32.78% \n",
            "\n",
            "54\n",
            "\tTrain Loss: 1.318 | Train Acc: 58.23%\n",
            "\t Val. Loss: 1.552 |  Val. Acc: 32.93% \n",
            "\n",
            "55\n",
            "\tTrain Loss: 1.311 | Train Acc: 59.27%\n",
            "\t Val. Loss: 1.546 |  Val. Acc: 34.03% \n",
            "\n",
            "56\n",
            "\tTrain Loss: 1.310 | Train Acc: 59.88%\n",
            "\t Val. Loss: 1.551 |  Val. Acc: 33.38% \n",
            "\n",
            "57\n",
            "\tTrain Loss: 1.305 | Train Acc: 60.41%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 33.56% \n",
            "\n",
            "58\n",
            "\tTrain Loss: 1.303 | Train Acc: 60.08%\n",
            "\t Val. Loss: 1.552 |  Val. Acc: 33.48% \n",
            "\n",
            "59\n",
            "\tTrain Loss: 1.300 | Train Acc: 60.52%\n",
            "\t Val. Loss: 1.554 |  Val. Acc: 32.89% \n",
            "\n",
            "60\n",
            "\tTrain Loss: 1.296 | Train Acc: 61.13%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 34.10% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 108, 4: 89, 3: 123, 0: 319, 1: 150}, 4: {4: 65, 2: 28, 3: 78, 0: 144, 1: 53}, 3: {2: 20, 3: 241, 0: 30, 4: 19, 1: 156}, 1: {3: 264, 1: 307, 2: 72, 4: 42, 0: 100}, 2: {4: 45, 3: 119, 1: 150, 2: 76, 0: 166}}\n",
            "\t pred vs actuals: {0: {2: 108, 4: 89, 3: 123, 0: 319, 1: 150}, 4: {4: 65, 2: 28, 3: 78, 0: 144, 1: 53}, 3: {2: 20, 3: 241, 0: 30, 4: 19, 1: 156}, 1: {3: 264, 1: 307, 2: 72, 4: 42, 0: 100}, 2: {4: 45, 3: 119, 1: 150, 2: 76, 0: 166}}\n",
            "61\n",
            "\tTrain Loss: 1.290 | Train Acc: 61.53%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 33.76% \n",
            "\n",
            "62\n",
            "\tTrain Loss: 1.286 | Train Acc: 61.85%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 33.69% \n",
            "\n",
            "63\n",
            "\tTrain Loss: 1.285 | Train Acc: 61.74%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 33.83% \n",
            "\n",
            "64\n",
            "\tTrain Loss: 1.281 | Train Acc: 62.39%\n",
            "\t Val. Loss: 1.546 |  Val. Acc: 33.73% \n",
            "\n",
            "65\n",
            "\tTrain Loss: 1.281 | Train Acc: 62.48%\n",
            "\t Val. Loss: 1.549 |  Val. Acc: 33.53% \n",
            "\n",
            "66\n",
            "\tTrain Loss: 1.275 | Train Acc: 62.77%\n",
            "\t Val. Loss: 1.544 |  Val. Acc: 34.77% \n",
            "\n",
            "67\n",
            "\tTrain Loss: 1.277 | Train Acc: 62.99%\n",
            "\t Val. Loss: 1.551 |  Val. Acc: 33.52% \n",
            "\n",
            "68\n",
            "\tTrain Loss: 1.275 | Train Acc: 63.12%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 34.04% \n",
            "\n",
            "69\n",
            "\tTrain Loss: 1.265 | Train Acc: 63.89%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 34.03% \n",
            "\n",
            "70\n",
            "\tTrain Loss: 1.265 | Train Acc: 64.18%\n",
            "\t Val. Loss: 1.544 |  Val. Acc: 34.57% \n",
            "\n",
            "71\n",
            "\tTrain Loss: 1.257 | Train Acc: 64.88%\n",
            "\t Val. Loss: 1.546 |  Val. Acc: 34.20% \n",
            "\n",
            "72\n",
            "\tTrain Loss: 1.259 | Train Acc: 64.28%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 34.64% \n",
            "\n",
            "73\n",
            "\tTrain Loss: 1.256 | Train Acc: 64.71%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 34.97% \n",
            "\n",
            "74\n",
            "\tTrain Loss: 1.257 | Train Acc: 64.36%\n",
            "\t Val. Loss: 1.544 |  Val. Acc: 34.28% \n",
            "\n",
            "75\n",
            "\tTrain Loss: 1.251 | Train Acc: 65.45%\n",
            "\t Val. Loss: 1.550 |  Val. Acc: 33.77% \n",
            "\n",
            "76\n",
            "\tTrain Loss: 1.249 | Train Acc: 65.39%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 33.96% \n",
            "\n",
            "77\n",
            "\tTrain Loss: 1.244 | Train Acc: 66.06%\n",
            "\t Val. Loss: 1.544 |  Val. Acc: 34.31% \n",
            "\n",
            "78\n",
            "\tTrain Loss: 1.244 | Train Acc: 66.44%\n",
            "\t Val. Loss: 1.550 |  Val. Acc: 33.90% \n",
            "\n",
            "79\n",
            "\tTrain Loss: 1.243 | Train Acc: 66.17%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 34.67% \n",
            "\n",
            "80\n",
            "\tTrain Loss: 1.237 | Train Acc: 66.86%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 34.16% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 124, 4: 71, 2: 105, 0: 326, 1: 163}, 4: {4: 57, 2: 26, 3: 76, 0: 152, 1: 57}, 3: {2: 22, 3: 255, 4: 13, 0: 30, 1: 146}, 1: {3: 279, 1: 295, 4: 33, 2: 70, 0: 108}, 2: {4: 31, 3: 119, 1: 161, 2: 77, 0: 168}}\n",
            "\t pred vs actuals: {0: {3: 124, 4: 71, 2: 105, 0: 326, 1: 163}, 4: {4: 57, 2: 26, 3: 76, 0: 152, 1: 57}, 3: {2: 22, 3: 255, 4: 13, 0: 30, 1: 146}, 1: {3: 279, 1: 295, 4: 33, 2: 70, 0: 108}, 2: {4: 31, 3: 119, 1: 161, 2: 77, 0: 168}}\n",
            "81\n",
            "\tTrain Loss: 1.235 | Train Acc: 66.80%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.59% \n",
            "\n",
            "82\n",
            "\tTrain Loss: 1.235 | Train Acc: 67.20%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 34.83% \n",
            "\n",
            "83\n",
            "\tTrain Loss: 1.233 | Train Acc: 67.31%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 34.97% \n",
            "\n",
            "84\n",
            "\tTrain Loss: 1.229 | Train Acc: 67.46%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.43% \n",
            "\n",
            "85\n",
            "\tTrain Loss: 1.224 | Train Acc: 68.01%\n",
            "\t Val. Loss: 1.550 |  Val. Acc: 33.86% \n",
            "\n",
            "86\n",
            "\tTrain Loss: 1.224 | Train Acc: 68.44%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.96% \n",
            "\n",
            "87\n",
            "\tTrain Loss: 1.223 | Train Acc: 68.29%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 34.84% \n",
            "\n",
            "88\n",
            "\tTrain Loss: 1.220 | Train Acc: 68.45%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.43% \n",
            "\n",
            "89\n",
            "\tTrain Loss: 1.218 | Train Acc: 68.96%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 34.04% \n",
            "\n",
            "90\n",
            "\tTrain Loss: 1.217 | Train Acc: 68.73%\n",
            "\t Val. Loss: 1.549 |  Val. Acc: 33.97% \n",
            "\n",
            "91\n",
            "\tTrain Loss: 1.214 | Train Acc: 68.71%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.40% \n",
            "\n",
            "92\n",
            "\tTrain Loss: 1.210 | Train Acc: 69.56%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 33.73% \n",
            "\n",
            "93\n",
            "\tTrain Loss: 1.210 | Train Acc: 69.39%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 34.67% \n",
            "\n",
            "94\n",
            "\tTrain Loss: 1.204 | Train Acc: 70.26%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.17% \n",
            "\n",
            "95\n",
            "\tTrain Loss: 1.205 | Train Acc: 69.72%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.48% \n",
            "\n",
            "96\n",
            "\tTrain Loss: 1.203 | Train Acc: 69.75%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 34.81% \n",
            "\n",
            "97\n",
            "\tTrain Loss: 1.203 | Train Acc: 69.79%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 35.71% \n",
            "\n",
            "98\n",
            "\tTrain Loss: 1.197 | Train Acc: 70.95%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.73% \n",
            "\n",
            "99\n",
            "\tTrain Loss: 1.197 | Train Acc: 70.29%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 35.06% \n",
            "\n",
            "100\n",
            "\tTrain Loss: 1.193 | Train Acc: 71.15%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 34.89% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 117, 4: 78, 0: 329, 2: 118, 1: 147}, 4: {4: 61, 2: 39, 3: 62, 0: 151, 1: 55}, 3: {2: 25, 3: 247, 0: 29, 1: 155, 4: 10}, 1: {3: 262, 1: 310, 2: 74, 4: 33, 0: 106}, 2: {0: 162, 3: 118, 1: 155, 2: 85, 4: 36}}\n",
            "\t pred vs actuals: {0: {3: 117, 4: 78, 0: 329, 2: 118, 1: 147}, 4: {4: 61, 2: 39, 3: 62, 0: 151, 1: 55}, 3: {2: 25, 3: 247, 0: 29, 1: 155, 4: 10}, 1: {3: 262, 1: 310, 2: 74, 4: 33, 0: 106}, 2: {0: 162, 3: 118, 1: 155, 2: 85, 4: 36}}\n",
            "101\n",
            "\tTrain Loss: 1.194 | Train Acc: 71.01%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.07% \n",
            "\n",
            "102\n",
            "\tTrain Loss: 1.194 | Train Acc: 71.02%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 35.07% \n",
            "\n",
            "103\n",
            "\tTrain Loss: 1.192 | Train Acc: 71.26%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 34.83% \n",
            "\n",
            "104\n",
            "\tTrain Loss: 1.189 | Train Acc: 71.32%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.27% \n",
            "\n",
            "105\n",
            "\tTrain Loss: 1.185 | Train Acc: 71.59%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 35.13% \n",
            "\n",
            "106\n",
            "\tTrain Loss: 1.184 | Train Acc: 72.22%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.27% \n",
            "\n",
            "107\n",
            "\tTrain Loss: 1.180 | Train Acc: 72.64%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 34.36% \n",
            "\n",
            "108\n",
            "\tTrain Loss: 1.185 | Train Acc: 71.57%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.50% \n",
            "\n",
            "109\n",
            "\tTrain Loss: 1.182 | Train Acc: 72.22%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.03% \n",
            "\n",
            "110\n",
            "\tTrain Loss: 1.179 | Train Acc: 72.47%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.20% \n",
            "\n",
            "111\n",
            "\tTrain Loss: 1.175 | Train Acc: 72.80%\n",
            "\t Val. Loss: 1.544 |  Val. Acc: 34.93% \n",
            "\n",
            "112\n",
            "\tTrain Loss: 1.178 | Train Acc: 72.40%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.27% \n",
            "\n",
            "113\n",
            "\tTrain Loss: 1.176 | Train Acc: 72.81%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 35.03% \n",
            "\n",
            "114\n",
            "\tTrain Loss: 1.171 | Train Acc: 72.79%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.19% \n",
            "\n",
            "115\n",
            "\tTrain Loss: 1.172 | Train Acc: 72.92%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 34.63% \n",
            "\n",
            "116\n",
            "\tTrain Loss: 1.171 | Train Acc: 73.30%\n",
            "\t Val. Loss: 1.550 |  Val. Acc: 34.00% \n",
            "\n",
            "117\n",
            "\tTrain Loss: 1.167 | Train Acc: 73.58%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 35.84% \n",
            "\n",
            "118\n",
            "\tTrain Loss: 1.169 | Train Acc: 73.67%\n",
            "\t Val. Loss: 1.553 |  Val. Acc: 33.79% \n",
            "\n",
            "119\n",
            "\tTrain Loss: 1.168 | Train Acc: 73.41%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.93% \n",
            "\n",
            "120\n",
            "\tTrain Loss: 1.167 | Train Acc: 73.35%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 34.93% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 102, 4: 92, 0: 302, 2: 114, 1: 179}, 4: {4: 74, 2: 34, 3: 66, 0: 132, 1: 62}, 3: {1: 162, 3: 243, 2: 22, 0: 25, 4: 14}, 1: {3: 254, 1: 328, 2: 72, 4: 39, 0: 92}, 2: {0: 149, 3: 109, 1: 172, 2: 84, 4: 42}}\n",
            "\t pred vs actuals: {0: {3: 102, 4: 92, 0: 302, 2: 114, 1: 179}, 4: {4: 74, 2: 34, 3: 66, 0: 132, 1: 62}, 3: {1: 162, 3: 243, 2: 22, 0: 25, 4: 14}, 1: {3: 254, 1: 328, 2: 72, 4: 39, 0: 92}, 2: {0: 149, 3: 109, 1: 172, 2: 84, 4: 42}}\n",
            "121\n",
            "\tTrain Loss: 1.161 | Train Acc: 74.11%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 34.53% \n",
            "\n",
            "122\n",
            "\tTrain Loss: 1.166 | Train Acc: 73.34%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 35.20% \n",
            "\n",
            "123\n",
            "\tTrain Loss: 1.163 | Train Acc: 74.06%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.43% \n",
            "\n",
            "124\n",
            "\tTrain Loss: 1.163 | Train Acc: 73.62%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.57% \n",
            "\n",
            "125\n",
            "\tTrain Loss: 1.164 | Train Acc: 73.65%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.91% \n",
            "\n",
            "126\n",
            "\tTrain Loss: 1.157 | Train Acc: 74.45%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 34.50% \n",
            "\n",
            "127\n",
            "\tTrain Loss: 1.154 | Train Acc: 74.61%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.93% \n",
            "\n",
            "128\n",
            "\tTrain Loss: 1.152 | Train Acc: 75.12%\n",
            "\t Val. Loss: 1.544 |  Val. Acc: 35.00% \n",
            "\n",
            "129\n",
            "\tTrain Loss: 1.160 | Train Acc: 74.17%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.45% \n",
            "\n",
            "130\n",
            "\tTrain Loss: 1.148 | Train Acc: 75.38%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.24% \n",
            "\n",
            "131\n",
            "\tTrain Loss: 1.155 | Train Acc: 74.75%\n",
            "\t Val. Loss: 1.546 |  Val. Acc: 34.53% \n",
            "\n",
            "132\n",
            "\tTrain Loss: 1.151 | Train Acc: 74.93%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.87% \n",
            "\n",
            "133\n",
            "\tTrain Loss: 1.147 | Train Acc: 75.13%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.26% \n",
            "\n",
            "134\n",
            "\tTrain Loss: 1.146 | Train Acc: 75.78%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.00% \n",
            "\n",
            "135\n",
            "\tTrain Loss: 1.145 | Train Acc: 75.29%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 34.99% \n",
            "\n",
            "136\n",
            "\tTrain Loss: 1.149 | Train Acc: 75.46%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 35.64% \n",
            "\n",
            "137\n",
            "\tTrain Loss: 1.150 | Train Acc: 75.19%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 36.03% \n",
            "\n",
            "138\n",
            "\tTrain Loss: 1.142 | Train Acc: 76.13%\n",
            "\t Val. Loss: 1.524 |  Val. Acc: 37.00% \n",
            "\n",
            "139\n",
            "\tTrain Loss: 1.143 | Train Acc: 75.66%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.24% \n",
            "\n",
            "140\n",
            "\tTrain Loss: 1.139 | Train Acc: 76.31%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.37% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 86, 4: 121, 2: 105, 0: 316, 1: 161}, 4: {4: 83, 3: 51, 0: 145, 1: 57, 2: 32}, 3: {1: 165, 3: 230, 2: 23, 0: 30, 4: 18}, 1: {4: 46, 3: 235, 1: 324, 2: 79, 0: 101}, 2: {0: 164, 3: 96, 1: 160, 2: 89, 4: 47}}\n",
            "\t pred vs actuals: {0: {3: 86, 4: 121, 2: 105, 0: 316, 1: 161}, 4: {4: 83, 3: 51, 0: 145, 1: 57, 2: 32}, 3: {1: 165, 3: 230, 2: 23, 0: 30, 4: 18}, 1: {4: 46, 3: 235, 1: 324, 2: 79, 0: 101}, 2: {0: 164, 3: 96, 1: 160, 2: 89, 4: 47}}\n",
            "141\n",
            "\tTrain Loss: 1.141 | Train Acc: 76.41%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.37% \n",
            "\n",
            "142\n",
            "\tTrain Loss: 1.141 | Train Acc: 76.01%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 36.03% \n",
            "\n",
            "143\n",
            "\tTrain Loss: 1.141 | Train Acc: 75.72%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 36.54% \n",
            "\n",
            "144\n",
            "\tTrain Loss: 1.138 | Train Acc: 76.12%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 35.96% \n",
            "\n",
            "145\n",
            "\tTrain Loss: 1.142 | Train Acc: 75.95%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.27% \n",
            "\n",
            "146\n",
            "\tTrain Loss: 1.134 | Train Acc: 76.73%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.37% \n",
            "\n",
            "147\n",
            "\tTrain Loss: 1.135 | Train Acc: 76.84%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.26% \n",
            "\n",
            "148\n",
            "\tTrain Loss: 1.134 | Train Acc: 76.71%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.42% \n",
            "\n",
            "149\n",
            "\tTrain Loss: 1.136 | Train Acc: 76.82%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.38% \n",
            "\n",
            "150\n",
            "\tTrain Loss: 1.132 | Train Acc: 76.92%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 35.92% \n",
            "\n",
            "151\n",
            "\tTrain Loss: 1.129 | Train Acc: 77.50%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.09% \n",
            "\n",
            "152\n",
            "\tTrain Loss: 1.127 | Train Acc: 77.39%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.36% \n",
            "\n",
            "153\n",
            "\tTrain Loss: 1.129 | Train Acc: 77.31%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.62% \n",
            "\n",
            "154\n",
            "\tTrain Loss: 1.133 | Train Acc: 76.70%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.22% \n",
            "\n",
            "155\n",
            "\tTrain Loss: 1.128 | Train Acc: 77.41%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.09% \n",
            "\n",
            "156\n",
            "\tTrain Loss: 1.126 | Train Acc: 77.28%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.63% \n",
            "\n",
            "157\n",
            "\tTrain Loss: 1.128 | Train Acc: 77.16%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.16% \n",
            "\n",
            "158\n",
            "\tTrain Loss: 1.124 | Train Acc: 77.46%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 35.26% \n",
            "\n",
            "159\n",
            "\tTrain Loss: 1.125 | Train Acc: 77.52%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.59% \n",
            "\n",
            "160\n",
            "\tTrain Loss: 1.121 | Train Acc: 77.93%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.89% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 84, 4: 107, 0: 333, 2: 112, 1: 153}, 4: {4: 82, 3: 57, 0: 148, 1: 43, 2: 38}, 3: {1: 149, 3: 238, 2: 28, 0: 36, 4: 15}, 1: {3: 257, 1: 309, 2: 77, 4: 37, 0: 105}, 2: {0: 165, 3: 99, 1: 144, 2: 100, 4: 48}}\n",
            "\t pred vs actuals: {0: {3: 84, 4: 107, 0: 333, 2: 112, 1: 153}, 4: {4: 82, 3: 57, 0: 148, 1: 43, 2: 38}, 3: {1: 149, 3: 238, 2: 28, 0: 36, 4: 15}, 1: {3: 257, 1: 309, 2: 77, 4: 37, 0: 105}, 2: {0: 165, 3: 99, 1: 144, 2: 100, 4: 48}}\n",
            "161\n",
            "\tTrain Loss: 1.126 | Train Acc: 77.53%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.13% \n",
            "\n",
            "162\n",
            "\tTrain Loss: 1.124 | Train Acc: 77.98%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 34.96% \n",
            "\n",
            "163\n",
            "\tTrain Loss: 1.123 | Train Acc: 77.94%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 35.72% \n",
            "\n",
            "164\n",
            "\tTrain Loss: 1.120 | Train Acc: 78.13%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.53% \n",
            "\n",
            "165\n",
            "\tTrain Loss: 1.120 | Train Acc: 78.32%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.12% \n",
            "\n",
            "166\n",
            "\tTrain Loss: 1.122 | Train Acc: 77.67%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.76% \n",
            "\n",
            "167\n",
            "\tTrain Loss: 1.115 | Train Acc: 78.52%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 35.96% \n",
            "\n",
            "168\n",
            "\tTrain Loss: 1.113 | Train Acc: 78.51%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.76% \n",
            "\n",
            "169\n",
            "\tTrain Loss: 1.118 | Train Acc: 78.34%\n",
            "\t Val. Loss: 1.528 |  Val. Acc: 36.44% \n",
            "\n",
            "170\n",
            "\tTrain Loss: 1.117 | Train Acc: 78.19%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.19% \n",
            "\n",
            "171\n",
            "\tTrain Loss: 1.115 | Train Acc: 78.34%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 35.99% \n",
            "\n",
            "172\n",
            "\tTrain Loss: 1.116 | Train Acc: 78.62%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.63% \n",
            "\n",
            "173\n",
            "\tTrain Loss: 1.116 | Train Acc: 78.32%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 36.00% \n",
            "\n",
            "174\n",
            "\tTrain Loss: 1.111 | Train Acc: 78.69%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 35.30% \n",
            "\n",
            "175\n",
            "\tTrain Loss: 1.115 | Train Acc: 78.25%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.92% \n",
            "\n",
            "176\n",
            "\tTrain Loss: 1.111 | Train Acc: 78.92%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.36% \n",
            "\n",
            "177\n",
            "\tTrain Loss: 1.110 | Train Acc: 78.96%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.19% \n",
            "\n",
            "178\n",
            "\tTrain Loss: 1.112 | Train Acc: 79.03%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.57% \n",
            "\n",
            "179\n",
            "\tTrain Loss: 1.109 | Train Acc: 79.12%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 35.93% \n",
            "\n",
            "180\n",
            "\tTrain Loss: 1.111 | Train Acc: 78.74%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.00% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 89, 4: 105, 2: 104, 0: 327, 1: 164}, 4: {4: 85, 3: 54, 0: 146, 1: 57, 2: 26}, 3: {1: 165, 4: 14, 3: 232, 2: 26, 0: 29}, 1: {4: 45, 3: 235, 1: 337, 2: 66, 0: 102}, 2: {0: 172, 3: 95, 1: 163, 2: 82, 4: 44}}\n",
            "\t pred vs actuals: {0: {3: 89, 4: 105, 2: 104, 0: 327, 1: 164}, 4: {4: 85, 3: 54, 0: 146, 1: 57, 2: 26}, 3: {1: 165, 4: 14, 3: 232, 2: 26, 0: 29}, 1: {4: 45, 3: 235, 1: 337, 2: 66, 0: 102}, 2: {0: 172, 3: 95, 1: 163, 2: 82, 4: 44}}\n",
            "181\n",
            "\tTrain Loss: 1.109 | Train Acc: 78.73%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 35.66% \n",
            "\n",
            "182\n",
            "\tTrain Loss: 1.106 | Train Acc: 79.23%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.83% \n",
            "\n",
            "183\n",
            "\tTrain Loss: 1.109 | Train Acc: 79.03%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.06% \n",
            "\n",
            "184\n",
            "\tTrain Loss: 1.100 | Train Acc: 79.73%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.76% \n",
            "\n",
            "185\n",
            "\tTrain Loss: 1.109 | Train Acc: 78.90%\n",
            "\t Val. Loss: 1.528 |  Val. Acc: 36.74% \n",
            "\n",
            "186\n",
            "\tTrain Loss: 1.109 | Train Acc: 78.91%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 35.36% \n",
            "\n",
            "187\n",
            "\tTrain Loss: 1.107 | Train Acc: 79.32%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.12% \n",
            "\n",
            "188\n",
            "\tTrain Loss: 1.104 | Train Acc: 79.34%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 35.19% \n",
            "\n",
            "189\n",
            "\tTrain Loss: 1.103 | Train Acc: 79.67%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 36.86% \n",
            "\n",
            "190\n",
            "\tTrain Loss: 1.103 | Train Acc: 79.57%\n",
            "\t Val. Loss: 1.527 |  Val. Acc: 36.80% \n",
            "\n",
            "191\n",
            "\tTrain Loss: 1.104 | Train Acc: 79.59%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.20% \n",
            "\n",
            "192\n",
            "\tTrain Loss: 1.105 | Train Acc: 79.57%\n",
            "\t Val. Loss: 1.526 |  Val. Acc: 37.03% \n",
            "\n",
            "193\n",
            "\tTrain Loss: 1.102 | Train Acc: 79.57%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.44% \n",
            "\n",
            "194\n",
            "\tTrain Loss: 1.097 | Train Acc: 80.14%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.12% \n",
            "\n",
            "195\n",
            "\tTrain Loss: 1.101 | Train Acc: 80.09%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.60% \n",
            "\n",
            "196\n",
            "\tTrain Loss: 1.098 | Train Acc: 80.13%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.26% \n",
            "\n",
            "197\n",
            "\tTrain Loss: 1.103 | Train Acc: 79.09%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.16% \n",
            "\n",
            "198\n",
            "\tTrain Loss: 1.097 | Train Acc: 80.08%\n",
            "\t Val. Loss: 1.526 |  Val. Acc: 36.93% \n",
            "\n",
            "199\n",
            "\tTrain Loss: 1.103 | Train Acc: 79.58%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.13% \n",
            "\n",
            "200\n",
            "\tTrain Loss: 1.104 | Train Acc: 79.36%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.06% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 70, 4: 119, 2: 116, 0: 326, 1: 158}, 4: {4: 87, 3: 50, 0: 146, 1: 52, 2: 33}, 3: {1: 179, 4: 19, 3: 210, 2: 32, 0: 26}, 1: {4: 41, 3: 213, 1: 344, 2: 83, 0: 104}, 2: {0: 161, 3: 72, 1: 171, 2: 98, 4: 54}}\n",
            "\t pred vs actuals: {0: {3: 70, 4: 119, 2: 116, 0: 326, 1: 158}, 4: {4: 87, 3: 50, 0: 146, 1: 52, 2: 33}, 3: {1: 179, 4: 19, 3: 210, 2: 32, 0: 26}, 1: {4: 41, 3: 213, 1: 344, 2: 83, 0: 104}, 2: {0: 161, 3: 72, 1: 171, 2: 98, 4: 54}}\n",
            "201\n",
            "\tTrain Loss: 1.100 | Train Acc: 80.10%\n",
            "\t Val. Loss: 1.523 |  Val. Acc: 37.13% \n",
            "\n",
            "202\n",
            "\tTrain Loss: 1.097 | Train Acc: 79.85%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.36% \n",
            "\n",
            "203\n",
            "\tTrain Loss: 1.096 | Train Acc: 80.22%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.06% \n",
            "\n",
            "204\n",
            "\tTrain Loss: 1.092 | Train Acc: 80.70%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.03% \n",
            "\n",
            "205\n",
            "\tTrain Loss: 1.098 | Train Acc: 79.83%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.33% \n",
            "\n",
            "206\n",
            "\tTrain Loss: 1.097 | Train Acc: 80.22%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 35.86% \n",
            "\n",
            "207\n",
            "\tTrain Loss: 1.095 | Train Acc: 80.23%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.19% \n",
            "\n",
            "208\n",
            "\tTrain Loss: 1.090 | Train Acc: 80.89%\n",
            "\t Val. Loss: 1.527 |  Val. Acc: 36.83% \n",
            "\n",
            "209\n",
            "\tTrain Loss: 1.087 | Train Acc: 81.39%\n",
            "\t Val. Loss: 1.526 |  Val. Acc: 36.89% \n",
            "\n",
            "210\n",
            "\tTrain Loss: 1.091 | Train Acc: 80.45%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.36% \n",
            "\n",
            "211\n",
            "\tTrain Loss: 1.090 | Train Acc: 80.68%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 36.46% \n",
            "\n",
            "212\n",
            "\tTrain Loss: 1.094 | Train Acc: 80.89%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.26% \n",
            "\n",
            "213\n",
            "\tTrain Loss: 1.092 | Train Acc: 80.88%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 35.93% \n",
            "\n",
            "214\n",
            "\tTrain Loss: 1.092 | Train Acc: 80.81%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.96% \n",
            "\n",
            "215\n",
            "\tTrain Loss: 1.093 | Train Acc: 80.60%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.96% \n",
            "\n",
            "216\n",
            "\tTrain Loss: 1.086 | Train Acc: 81.10%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.20% \n",
            "\n",
            "217\n",
            "\tTrain Loss: 1.090 | Train Acc: 80.75%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 35.50% \n",
            "\n",
            "218\n",
            "\tTrain Loss: 1.089 | Train Acc: 80.64%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 35.17% \n",
            "\n",
            "219\n",
            "\tTrain Loss: 1.092 | Train Acc: 80.34%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.22% \n",
            "\n",
            "220\n",
            "\tTrain Loss: 1.089 | Train Acc: 80.75%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.66% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 97, 4: 113, 2: 124, 0: 308, 1: 147}, 4: {4: 86, 2: 40, 3: 61, 0: 128, 1: 53}, 3: {1: 158, 4: 17, 3: 237, 2: 31, 0: 23}, 1: {4: 39, 3: 255, 1: 317, 2: 81, 0: 93}, 2: {0: 144, 3: 98, 1: 159, 2: 105, 4: 50}}\n",
            "\t pred vs actuals: {0: {3: 97, 4: 113, 2: 124, 0: 308, 1: 147}, 4: {4: 86, 2: 40, 3: 61, 0: 128, 1: 53}, 3: {1: 158, 4: 17, 3: 237, 2: 31, 0: 23}, 1: {4: 39, 3: 255, 1: 317, 2: 81, 0: 93}, 2: {0: 144, 3: 98, 1: 159, 2: 105, 4: 50}}\n",
            "221\n",
            "\tTrain Loss: 1.091 | Train Acc: 81.00%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 35.86% \n",
            "\n",
            "222\n",
            "\tTrain Loss: 1.090 | Train Acc: 80.51%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.16% \n",
            "\n",
            "223\n",
            "\tTrain Loss: 1.083 | Train Acc: 81.42%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.10% \n",
            "\n",
            "224\n",
            "\tTrain Loss: 1.086 | Train Acc: 81.18%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 35.86% \n",
            "\n",
            "225\n",
            "\tTrain Loss: 1.082 | Train Acc: 81.43%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.70% \n",
            "\n",
            "226\n",
            "\tTrain Loss: 1.084 | Train Acc: 81.50%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 36.22% \n",
            "\n",
            "227\n",
            "\tTrain Loss: 1.089 | Train Acc: 80.97%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.46% \n",
            "\n",
            "228\n",
            "\tTrain Loss: 1.089 | Train Acc: 81.01%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.13% \n",
            "\n",
            "229\n",
            "\tTrain Loss: 1.088 | Train Acc: 81.34%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 36.53% \n",
            "\n",
            "230\n",
            "\tTrain Loss: 1.084 | Train Acc: 81.47%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.62% \n",
            "\n",
            "231\n",
            "\tTrain Loss: 1.083 | Train Acc: 81.79%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.93% \n",
            "\n",
            "232\n",
            "\tTrain Loss: 1.084 | Train Acc: 81.68%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.80% \n",
            "\n",
            "233\n",
            "\tTrain Loss: 1.083 | Train Acc: 81.24%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.33% \n",
            "\n",
            "234\n",
            "\tTrain Loss: 1.082 | Train Acc: 81.51%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.83% \n",
            "\n",
            "235\n",
            "\tTrain Loss: 1.084 | Train Acc: 81.71%\n",
            "\t Val. Loss: 1.537 |  Val. Acc: 35.70% \n",
            "\n",
            "236\n",
            "\tTrain Loss: 1.084 | Train Acc: 81.62%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.36% \n",
            "\n",
            "237\n",
            "\tTrain Loss: 1.080 | Train Acc: 82.16%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.36% \n",
            "\n",
            "238\n",
            "\tTrain Loss: 1.084 | Train Acc: 81.47%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.36% \n",
            "\n",
            "239\n",
            "\tTrain Loss: 1.085 | Train Acc: 81.38%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.40% \n",
            "\n",
            "240\n",
            "\tTrain Loss: 1.081 | Train Acc: 81.74%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 36.79% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 84, 4: 112, 2: 109, 0: 354, 1: 130}, 4: {4: 92, 3: 49, 0: 148, 2: 40, 1: 39}, 3: {1: 160, 4: 19, 3: 231, 2: 29, 0: 27}, 1: {4: 44, 3: 240, 1: 311, 2: 78, 0: 112}, 2: {0: 172, 3: 85, 1: 147, 2: 101, 4: 51}}\n",
            "\t pred vs actuals: {0: {3: 84, 4: 112, 2: 109, 0: 354, 1: 130}, 4: {4: 92, 3: 49, 0: 148, 2: 40, 1: 39}, 3: {1: 160, 4: 19, 3: 231, 2: 29, 0: 27}, 1: {4: 44, 3: 240, 1: 311, 2: 78, 0: 112}, 2: {0: 172, 3: 85, 1: 147, 2: 101, 4: 51}}\n",
            "241\n",
            "\tTrain Loss: 1.078 | Train Acc: 82.02%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.56% \n",
            "\n",
            "242\n",
            "\tTrain Loss: 1.080 | Train Acc: 81.66%\n",
            "\t Val. Loss: 1.526 |  Val. Acc: 37.02% \n",
            "\n",
            "243\n",
            "\tTrain Loss: 1.080 | Train Acc: 81.72%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.29% \n",
            "\n",
            "244\n",
            "\tTrain Loss: 1.082 | Train Acc: 81.49%\n",
            "\t Val. Loss: 1.526 |  Val. Acc: 36.99% \n",
            "\n",
            "245\n",
            "\tTrain Loss: 1.081 | Train Acc: 81.61%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.42% \n",
            "\n",
            "246\n",
            "\tTrain Loss: 1.074 | Train Acc: 82.25%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.19% \n",
            "\n",
            "247\n",
            "\tTrain Loss: 1.078 | Train Acc: 81.97%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.50% \n",
            "\n",
            "248\n",
            "\tTrain Loss: 1.079 | Train Acc: 82.01%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 35.66% \n",
            "\n",
            "249\n",
            "\tTrain Loss: 1.081 | Train Acc: 81.20%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.63% \n",
            "\n",
            "250\n",
            "\tTrain Loss: 1.080 | Train Acc: 82.02%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 37.16% \n",
            "\n",
            "251\n",
            "\tTrain Loss: 1.076 | Train Acc: 82.25%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.77% \n",
            "\n",
            "252\n",
            "\tTrain Loss: 1.078 | Train Acc: 81.77%\n",
            "\t Val. Loss: 1.527 |  Val. Acc: 37.19% \n",
            "\n",
            "253\n",
            "\tTrain Loss: 1.082 | Train Acc: 81.27%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.46% \n",
            "\n",
            "254\n",
            "\tTrain Loss: 1.077 | Train Acc: 81.94%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.46% \n",
            "\n",
            "255\n",
            "\tTrain Loss: 1.076 | Train Acc: 82.06%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.66% \n",
            "\n",
            "256\n",
            "\tTrain Loss: 1.073 | Train Acc: 82.68%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.06% \n",
            "\n",
            "257\n",
            "\tTrain Loss: 1.073 | Train Acc: 82.32%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.70% \n",
            "\n",
            "258\n",
            "\tTrain Loss: 1.074 | Train Acc: 82.47%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.39% \n",
            "\n",
            "259\n",
            "\tTrain Loss: 1.074 | Train Acc: 82.41%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 36.03% \n",
            "\n",
            "260\n",
            "\tTrain Loss: 1.076 | Train Acc: 82.16%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 36.20% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 113, 4: 109, 2: 106, 0: 321, 1: 140}, 4: {4: 87, 3: 62, 0: 137, 1: 47, 2: 35}, 3: {1: 149, 4: 15, 3: 248, 0: 26, 2: 28}, 1: {4: 31, 3: 263, 1: 319, 2: 72, 0: 100}, 2: {4: 53, 3: 108, 1: 154, 2: 92, 0: 149}}\n",
            "\t pred vs actuals: {0: {3: 113, 4: 109, 2: 106, 0: 321, 1: 140}, 4: {4: 87, 3: 62, 0: 137, 1: 47, 2: 35}, 3: {1: 149, 4: 15, 3: 248, 0: 26, 2: 28}, 1: {4: 31, 3: 263, 1: 319, 2: 72, 0: 100}, 2: {4: 53, 3: 108, 1: 154, 2: 92, 0: 149}}\n",
            "261\n",
            "\tTrain Loss: 1.073 | Train Acc: 82.42%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.79% \n",
            "\n",
            "262\n",
            "\tTrain Loss: 1.077 | Train Acc: 81.93%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.79% \n",
            "\n",
            "263\n",
            "\tTrain Loss: 1.074 | Train Acc: 82.23%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.39% \n",
            "\n",
            "264\n",
            "\tTrain Loss: 1.073 | Train Acc: 82.57%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.59% \n",
            "\n",
            "265\n",
            "\tTrain Loss: 1.074 | Train Acc: 82.54%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.49% \n",
            "\n",
            "266\n",
            "\tTrain Loss: 1.072 | Train Acc: 82.45%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 36.06% \n",
            "\n",
            "267\n",
            "\tTrain Loss: 1.077 | Train Acc: 81.74%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.79% \n",
            "\n",
            "268\n",
            "\tTrain Loss: 1.074 | Train Acc: 82.38%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.66% \n",
            "\n",
            "269\n",
            "\tTrain Loss: 1.072 | Train Acc: 82.27%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 37.20% \n",
            "\n",
            "270\n",
            "\tTrain Loss: 1.074 | Train Acc: 82.82%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 36.30% \n",
            "\n",
            "271\n",
            "\tTrain Loss: 1.070 | Train Acc: 82.75%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.53% \n",
            "\n",
            "272\n",
            "\tTrain Loss: 1.072 | Train Acc: 82.72%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.56% \n",
            "\n",
            "273\n",
            "\tTrain Loss: 1.072 | Train Acc: 82.74%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.89% \n",
            "\n",
            "274\n",
            "\tTrain Loss: 1.073 | Train Acc: 82.34%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.60% \n",
            "\n",
            "275\n",
            "\tTrain Loss: 1.069 | Train Acc: 82.43%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.50% \n",
            "\n",
            "276\n",
            "\tTrain Loss: 1.067 | Train Acc: 82.99%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.56% \n",
            "\n",
            "277\n",
            "\tTrain Loss: 1.069 | Train Acc: 82.93%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.23% \n",
            "\n",
            "278\n",
            "\tTrain Loss: 1.071 | Train Acc: 82.57%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.66% \n",
            "\n",
            "279\n",
            "\tTrain Loss: 1.069 | Train Acc: 82.73%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 37.02% \n",
            "\n",
            "280\n",
            "\tTrain Loss: 1.073 | Train Acc: 82.24%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.80% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 101, 4: 111, 2: 99, 0: 336, 1: 142}, 4: {4: 91, 3: 58, 0: 143, 1: 42, 2: 34}, 3: {1: 156, 4: 16, 3: 243, 0: 26, 2: 25}, 1: {4: 35, 3: 252, 1: 328, 2: 60, 0: 110}, 2: {0: 163, 3: 93, 1: 159, 2: 89, 4: 52}}\n",
            "\t pred vs actuals: {0: {3: 101, 4: 111, 2: 99, 0: 336, 1: 142}, 4: {4: 91, 3: 58, 0: 143, 1: 42, 2: 34}, 3: {1: 156, 4: 16, 3: 243, 0: 26, 2: 25}, 1: {4: 35, 3: 252, 1: 328, 2: 60, 0: 110}, 2: {0: 163, 3: 93, 1: 159, 2: 89, 4: 52}}\n",
            "281\n",
            "\tTrain Loss: 1.071 | Train Acc: 82.68%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.33% \n",
            "\n",
            "282\n",
            "\tTrain Loss: 1.071 | Train Acc: 82.33%\n",
            "\t Val. Loss: 1.527 |  Val. Acc: 37.27% \n",
            "\n",
            "283\n",
            "\tTrain Loss: 1.067 | Train Acc: 83.03%\n",
            "\t Val. Loss: 1.526 |  Val. Acc: 37.30% \n",
            "\n",
            "284\n",
            "\tTrain Loss: 1.069 | Train Acc: 82.99%\n",
            "\t Val. Loss: 1.527 |  Val. Acc: 36.89% \n",
            "\n",
            "285\n",
            "\tTrain Loss: 1.067 | Train Acc: 82.57%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 36.93% \n",
            "\n",
            "286\n",
            "\tTrain Loss: 1.069 | Train Acc: 82.52%\n",
            "\t Val. Loss: 1.528 |  Val. Acc: 37.10% \n",
            "\n",
            "287\n",
            "\tTrain Loss: 1.064 | Train Acc: 83.59%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 36.10% \n",
            "\n",
            "288\n",
            "\tTrain Loss: 1.072 | Train Acc: 82.61%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 36.06% \n",
            "\n",
            "289\n",
            "\tTrain Loss: 1.064 | Train Acc: 83.11%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 36.64% \n",
            "\n",
            "290\n",
            "\tTrain Loss: 1.069 | Train Acc: 82.68%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.54% \n",
            "\n",
            "291\n",
            "\tTrain Loss: 1.067 | Train Acc: 83.12%\n",
            "\t Val. Loss: 1.528 |  Val. Acc: 36.74% \n",
            "\n",
            "292\n",
            "\tTrain Loss: 1.069 | Train Acc: 82.79%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.41% \n",
            "\n",
            "293\n",
            "\tTrain Loss: 1.066 | Train Acc: 82.99%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.20% \n",
            "\n",
            "294\n",
            "\tTrain Loss: 1.067 | Train Acc: 82.69%\n",
            "\t Val. Loss: 1.534 |  Val. Acc: 36.56% \n",
            "\n",
            "295\n",
            "\tTrain Loss: 1.065 | Train Acc: 83.36%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 36.96% \n",
            "\n",
            "296\n",
            "\tTrain Loss: 1.064 | Train Acc: 83.47%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.36% \n",
            "\n",
            "297\n",
            "\tTrain Loss: 1.065 | Train Acc: 82.94%\n",
            "\t Val. Loss: 1.528 |  Val. Acc: 36.80% \n",
            "\n",
            "298\n",
            "\tTrain Loss: 1.066 | Train Acc: 82.98%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.54% \n",
            "\n",
            "299\n",
            "\tTrain Loss: 1.062 | Train Acc: 83.15%\n",
            "\t Val. Loss: 1.526 |  Val. Acc: 37.23% \n",
            "\n",
            "300\n",
            "\tTrain Loss: 1.065 | Train Acc: 83.24%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 36.77% \n",
            "\n",
            "label distribution: actual vs pred: {0: {3: 95, 4: 133, 2: 100, 0: 338, 1: 123}, 4: {4: 105, 3: 54, 0: 136, 1: 36, 2: 37}, 3: {1: 159, 4: 18, 3: 234, 0: 26, 2: 29}, 1: {4: 41, 3: 249, 1: 316, 2: 74, 0: 105}, 2: {0: 154, 3: 92, 1: 156, 2: 91, 4: 63}}\n",
            "\t pred vs actuals: {0: {3: 95, 4: 133, 2: 100, 0: 338, 1: 123}, 4: {4: 105, 3: 54, 0: 136, 1: 36, 2: 37}, 3: {1: 159, 4: 18, 3: 234, 0: 26, 2: 29}, 1: {4: 41, 3: 249, 1: 316, 2: 74, 0: 105}, 2: {0: 154, 3: 92, 1: 156, 2: 91, 4: 63}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVFVWAa-j3OD",
        "outputId": "d74093df-cca5-4da2-a8e8-3efb2b9812a9"
      },
      "source": [
        "print(train_acc_list)\n",
        "print(valid_acc_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.21647315346487037, 0.22305585600469754, 0.22028208364090418, 0.22451784431117855, 0.2238757210235073, 0.22926304778551945, 0.22856710123144872, 0.2364052611671082, 0.23527372223601495, 0.24240842543236196, 0.24392423700524246, 0.244272210214236, 0.2545812304161455, 0.2541093386743711, 0.2607971842158331, 0.2582286910651481, 0.26447218221072194, 0.2726232676201215, 0.27459344716921247, 0.2738449290462825, 0.2876587159013095, 0.28631313587432583, 0.28707917776281977, 0.29130742811176874, 0.2943040034542345, 0.3018843126079263, 0.30072523633094683, 0.31263142878606437, 0.31345004210733385, 0.32096150762414277, 0.3155917047637783, 0.3168759513391207, 0.31602980053588137, 0.322950462636338, 0.32902247053847467, 0.3285330549630945, 0.3326699311874773, 0.3337413882964278, 0.3294956139777893, 0.336398752312682, 0.33931521874040227, 0.3390022931305785, 0.34568137665317483, 0.3442556877114457, 0.34998973605295297, 0.35023882481605495, 0.3543869662502585, 0.3530576584273822, 0.35958528600326956, 0.3604777497515831, 0.36071932832944337, 0.36193973805806406, 0.3615654791326828, 0.3688879376404906, 0.3678452695749666, 0.37400614834267254, 0.3693961287470169, 0.37364065130007323, 0.37477344192870676, 0.37749339102610063, 0.38541291157404584, 0.3749161359926337, 0.3863491848723529, 0.3872867098682003, 0.38969373349185404, 0.39333243212199104, 0.3938669090129469, 0.3928067171138171, 0.39422489587030457, 0.3924675058556474, 0.39646419134314204, 0.398433119058609, 0.398816140002856, 0.40316956065016796, 0.40192036174203705, 0.411258060910386, 0.40522110067546097, 0.4105095430596234, 0.40781587961057547, 0.4212466453852719, 0.413568703415187, 0.4215420472295317, 0.4203278959886124, 0.4253497257080252, 0.42131799241723533, 0.42506433758017137, 0.4256175899342315, 0.4288181927922654, 0.4289433629545447, 0.43277857891500815, 0.42990592217336504, 0.4384775795893038, 0.4327335175313906, 0.43522941195257175, 0.4280696748054191, 0.4382447628942254, 0.4410272971408008, 0.44405140996523645, 0.43963665586628325, 0.4418208765112646, 0.4398056357146398, 0.4465485560839579, 0.4453707042077905, 0.4438724165365576, 0.45378965397948, 0.44926725348381147, 0.44694033891098683, 0.4528533805450892, 0.4543779541368354, 0.45640320837769877, 0.4572055494948609, 0.46210971923723615, 0.4610219899922201, 0.462511515780671, 0.46318117641422846, 0.46027347186929013, 0.45967515826769617, 0.4584810341874214, 0.46903038133769276, 0.4715187655736322, 0.47396334012349445, 0.4691380277344081, 0.46747952209760063, 0.4766382281094381, 0.47353400623417335, 0.4782529239240847, 0.4705111452433617, 0.48008917129203066, 0.48466414322047474, 0.4778411138003275, 0.4830682228689324, 0.47297324355878784, 0.48061613799774483, 0.4860735600941802, 0.4867519827466033, 0.4810079208247738, 0.4824623988099294, 0.48380923053445335, 0.4856730152482856, 0.49144211125700443, 0.49097772983655535, 0.49285152813070987, 0.4954738445477943, 0.5048478430808951, 0.4939930806421254, 0.4925849154659602, 0.5023306698015292, 0.499709605081985, 0.4977832352189713, 0.4994167067688894, 0.4979622286476501, 0.5030366299359221, 0.5026623708744572, 0.5002628575721287, 0.5068881179644092, 0.5038039233858727, 0.4992014139754587, 0.4978908816156866, 0.5017799207064659, 0.5108397420682863, 0.5108848034519039, 0.5078519286086026, 0.5058104021636318, 0.5133907114534073, 0.5175376011900705, 0.5125157713345742, 0.5138888889796114, 0.5176978190195615, 0.5079770989069655, 0.5116420831854485, 0.5137824941443526, 0.5086104603118549, 0.5206230473300638, 0.5114017564412121, 0.5149240466557681, 0.5198557537440296, 0.5209797824898811, 0.5137199089951712, 0.5147463050607133, 0.5220324642037693, 0.5209447349587532, 0.5243243311093823, 0.5264735039510683, 0.5169668249343629, 0.5291408816156866, 0.528525044112445, 0.5323239605720729, 0.523557037523348, 0.5246898281519816, 0.5275174237821745, 0.5346784126268674, 0.5292385144320797, 0.5331801249556345, 0.5329135124269686, 0.5254471081576936, 0.5435166827075557, 0.5309408296188808, 0.5363469318439972, 0.5304514139754587, 0.5373633141931333, 0.53157419102377, 0.5322338380770052, 0.5406452776634529, 0.5400745014077453, 0.5412961628339062, 0.5371229873128134, 0.5400382019069097, 0.5380317231291505, 0.5386563226512578, 0.5456746174320238, 0.5417054695625828, 0.5441049827288275, 0.5493496155629963, 0.5450600317624061, 0.5380855463955501, 0.5492156833818514, 0.5394223645397517, 0.5431599475477384, 0.5530058379586973, 0.551427441372719, 0.552987062495593, 0.5553865756618378, 0.5499028679170564, 0.5531648040906479, 0.5493859147916645, 0.5513010193768157, 0.5603007589845352, 0.5522823540587404, 0.5504811544940896, 0.5548971601544994, 0.5524876332718488, 0.5591779820723076, 0.5604171674001163, 0.5539058120283362, 0.5530145998414793, 0.5534514439160421, 0.5545304112782762, 0.5618879174532956, 0.5594445946009736, 0.5580802391108857, 0.5647518125843239, 0.56451023387038, 0.5642961927744896, 0.560881548956649, 0.5628604902524382, 0.5655629155842681, 0.5655716774670501, 0.5686045523103513, 0.5677120884259542, 0.5659546982752134, 0.5629130618212974, 0.5609604060377704, 0.568131408734953, 0.5650459625949599, 0.5650184249769062, 0.5677921973406996, 0.5707349494167658, 0.5708062964487294, 0.5727689658125786, 0.5742935391321574, 0.5744900564624839, 0.5729554695625828, 0.57225076112573, 0.5748918530059187, 0.5726788433175108, 0.5726700814347289, 0.5784229051032567, 0.57259122408144, 0.5772725907090592, 0.575435091643573, 0.5667044680956836, 0.5825234820309295, 0.5792878313696004, 0.5750520708354097, 0.577843366964767, 0.5816235079612906, 0.5740169128870856, 0.5786194224335831, 0.5800463630728525, 0.583175618898923, 0.5767731614853149, 0.5905882000378823, 0.5767719097877746, 0.5739292937870983, 0.5874389170511672, 0.5785205380557334, 0.5856477309307552, 0.5827663121702464, 0.5849142333143922, 0.579065028390928, 0.5830429384153183, 0.5872436514183811, 0.582686203255501, 0.5870020727044372, 0.586048275640566, 0.5886793539404325, 0.5852809820545318, 0.5839166265644439, 0.585933119058609, 0.5900875191165976, 0.5853348053209314, 0.5892326064305763, 0.5900699950788664, 0.592130297123025, 0.5917372627345394, 0.5901951653772293, 0.5937625169754028, 0.5883939658125786, 0.5905606624198286, 0.5824709104620702, 0.5841481914258984, 0.5918724466132247, 0.586948249710205, 0.5899723623985569, 0.593494652613113, 0.5974187393711038, 0.5956713630728525, 0.594138027734408, 0.5914080649206083, 0.5888470819551651, 0.6007995874914405, 0.5997193684316662, 0.5979356926325794, 0.5989721020062765, 0.5886530682920865, 0.5974275015260531, 0.6011463090709356, 0.5979982777817608, 0.5961520168334926, 0.5936736461778754, 0.5998182528095158, 0.5948777838384725, 0.605007810679745, 0.5923168008730292, 0.60062935580946, 0.5999158854898252, 0.6028411138003276, 0.5989620884259542, 0.5995228511013396, 0.6093249318262214, 0.60545466833463, 0.5999697087562248, 0.605329498036267, 0.6006018184635737, 0.604259292556815, 0.597909406712066, 0.6062745334895234, 0.6023679704970966, 0.6033230192585078, 0.6027785289233134, 0.6023153989282373, 0.6079693382189154, 0.6023679704970966, 0.5996480213997026, 0.6096015580712932, 0.600013518442302, 0.6033580670618031, 0.6053645458395622, 0.6086465093098818, 0.608227189000883, 0.6093962788581848, 0.6076751882082796, 0.6130074401424356, 0.6133829507653572, 0.600603070161114, 0.6079242771074652, 0.6116606084179116, 0.6078266441549884, 0.605437144569066, 0.6080231614853149, 0.6125080109186912, 0.6047850077010725, 0.6237195085172784, 0.6141677681169554, 0.6085225907090592, 0.6133028418506117, 0.6126870042112864, 0.6059278119100283, 0.6080582092886102, 0.608486291480391, 0.613534406712066, 0.6158800968840786, 0.6076301270968294, 0.610047164028638, 0.613953727021065, 0.6132502704178362, 0.605007810679745, 0.6123477930892004, 0.6176187115717152, 0.6157912260865512, 0.6126331812170542, 0.616708723921754, 0.6225679424255406, 0.6172356904913846, 0.6196339522322564, 0.6122764460572369, 0.612749589360468, 0.6171993912627164, 0.622400214410808, 0.619018114592931, 0.6165309820545318, 0.6153981914258984, 0.612321507168687, 0.623380297123025, 0.6190631759765486, 0.6187152026995132, 0.6153719057775524, 0.622301329760791, 0.6181807256724736, 0.6258411440130782, 0.6188503865781985, 0.6202848374027096, 0.6198655170937107, 0.623371535240243, 0.6179116098848108, 0.6203937354969652, 0.6189755568765614, 0.6199468777059964, 0.6247972241819721, 0.6204012458182905, 0.627749990110528, 0.6186438556675498, 0.6203661980149953, 0.6220960505476826, 0.6259750761942232, 0.6209282123879211, 0.6186438556675498, 0.626563376215495, 0.6315050968840786, 0.6232013038304298, 0.6211334916010295, 0.627624819812165, 0.63019331296285, 0.624744652613113, 0.629427271074356, 0.6291231074833978, 0.625779810561437, 0.6237733317836779, 0.625814858092565, 0.6280178542006506, 0.6325214790971312, 0.6240411961459678, 0.6228545822509347, 0.6273494452646334, 0.627642343577729, 0.6284183990465452, 0.628846481238326, 0.6320846350225684, 0.6284021269785215, 0.6309255587455889, 0.6278751601367236, 0.6286962769891573, 0.6261002462204188, 0.633412691283988, 0.6332875209856251, 0.6265108046466357, 0.6297389448505558, 0.6317804714316102, 0.6321997917406091, 0.6357320955354874, 0.626554614332713, 0.633074731587275, 0.6304524151701906, 0.6194637208224432, 0.633832011592987, 0.6313624028201517, 0.6322986761184588, 0.6323612612676403, 0.631870593926678, 0.6352151425461791, 0.6320483357939002, 0.6315501582676961, 0.6404760474483716, 0.6363642052428363, 0.6355706260084562, 0.6408866058745885, 0.6296137748243602, 0.6345454819126216, 0.6339747056569138, 0.635366598492888, 0.6406275033950806, 0.6332074120708796, 0.6331273031561342, 0.632601588284044, 0.6327530442307528, 0.6352138908486388, 0.6432510715097053, 0.6381040716280132, 0.6403871766508442, 0.6360700552322004, 0.6337969637896916, 0.6339571818913499, 0.639005297123025, 0.6333851539381018, 0.6303097211062636, 0.6431521868596882, 0.6323787850332042, 0.639477189000883, 0.640806496959843, 0.6382117178886448]\n",
            "[0.26981382991405245, 0.2757978724672439, 0.27679521289277587, 0.27945478736086093, 0.28836436157530926, 0.289494681231519, 0.289494681231519, 0.2953457445540327, 0.2994015958714992, 0.2994015958714992, 0.2984707450613062, 0.3036569147667986, 0.2946808509370114, 0.3009973402987135, 0.3009973402987135, 0.3023271275327561, 0.30964095731999014, 0.3086436168944582, 0.3086436168944582, 0.31396276583062843, 0.3101728723404255, 0.3128989364238495, 0.3109042555727857, 0.3202127662110836, 0.3193484043821375, 0.33158244668169223, 0.33158244668169223, 0.33351063855150914, 0.3348404257855517, 0.33118351089193465, 0.33916223429618997, 0.33783244706214743, 0.34288563817105394, 0.34720744668169223, 0.34089095731999014, 0.3412234041285008, 0.33643617021276595, 0.32533244693532903, 0.3486037235310737, 0.34521276583062843, 0.3519281916161801, 0.3464095744680851, 0.34295212778639284, 0.3488031917429985, 0.3394281914893617, 0.3474734045089559, 0.3443484043187283, 0.3406250001268184, 0.3535239360433944, 0.3539228724672439, 0.35757978736086093, 0.35831117059322115, 0.36117021263913907, 0.3561835105114795, 0.3615691490629886, 0.35445478748767933, 0.3548537232774369, 0.36216755306467097, 0.36449468072424546, 0.35132978729745173, 0.3513962765957447, 0.35113031940257294, 0.3591755317880752, 0.36183510625616033, 0.35731382985064325, 0.3661569147667986, 0.36283244668169223, 0.3708776597012865, 0.3598404254050965, 0.3641622339157348, 0.36316489349020287, 0.3632313831055418, 0.36243351089193465, 0.3615026594476497, 0.365824467958288, 0.36456117033958435, 0.3671542551923305, 0.3799202131464126, 0.3729388301676892, 0.3661569147667986, 0.36648936157530926, 0.36748670200084116, 0.3646941489361702, 0.362101064083424, 0.3724734041285008, 0.3724734041285008, 0.3738031913625433, 0.3754654254050965, 0.36775265982810484, 0.3681515956178625, 0.3678191488093518, 0.3761303190221178, 0.3668218083838199, 0.36981382966041565, 0.3728723405523503, 0.36981382966041565, 0.369481382851905, 0.3724734041285008, 0.37845744668169223, 0.374069149189807, 0.3761303190221178, 0.3754654254050965, 0.3755319150204354, 0.37945478710722413, 0.380518617148095, 0.3771276594476497, 0.3795212767225631, 0.37746010625616033, 0.37679521263913907, 0.3807845743412667, 0.37679521263913907, 0.3668218083838199, 0.3691489360433944, 0.3718750001268184, 0.3781914894885205, 0.3765292554459673, 0.37785904268000986, 0.37353723416937157, 0.37746010625616033, 0.3771276594476497, 0.3748005317880752, 0.3841090424263731, 0.3781914894885205, 0.3848404256587333, 0.3788563831055418, 0.38091755357194457, 0.3815159575736269, 0.3831781916161801, 0.3775265958714992, 0.38916223416937157, 0.3798537235310737, 0.3785239362970312, 0.3821808511906482, 0.37679521263913907, 0.38011968072424546, 0.37686170225447796, 0.3798537235310737, 0.3849069152740722, 0.3798537235310737, 0.3815159575736269, 0.3775265958714992, 0.3755319150204354, 0.3795212767225631, 0.37586436182894606, 0.37586436182894606, 0.37785904268000986, 0.37486702140341416, 0.3728058509370114, 0.3718085105114795, 0.3684840424263731, 0.3738696809778822, 0.3691489360433944, 0.369481382851905, 0.3724734041285008, 0.3753989364238495, 0.37353723416937157, 0.37686170225447796, 0.3785904259123701, 0.3744680849795646, 0.3791223402987135, 0.3708776597012865, 0.3711436168944582, 0.3765292554459673, 0.3678191488093518, 0.37081117008594755, 0.37081117008594755, 0.37513297859658584, 0.3744015959983176, 0.3761303190221178, 0.37646276583062843, 0.37041223429618997, 0.37273936195576446, 0.3734707445540327, 0.37214095731999014, 0.3671542551923305, 0.37107712791321124, 0.37253989374383967, 0.36628989399747647, 0.3724734041285008, 0.36575797897704104, 0.35611702153023256, 0.358444149189807, 0.3634973402987135, 0.3775930854868382, 0.36648936157530926, 0.37413563817105394, 0.3751994682119248, 0.3708776597012865, 0.37486702140341416, 0.3781914894885205, 0.37918882991405245, 0.3771941490629886, 0.369481382851905, 0.3714760637029688, 0.3744680849795646, 0.37140957472172187, 0.36748670200084116, 0.36103723404255317, 0.38683510650979713, 0.3761303190221178, 0.37413563817105394, 0.3795212767225631, 0.3753989364238495, 0.3755984046357743, 0.3815159575736269, 0.37586436182894606, 0.38091755357194457, 0.37373670238129636, 0.36808510663661553, 0.3808510639566056, 0.37785904268000986, 0.37473404280682826, 0.36988031927575454, 0.36648936157530926, 0.383244681231519, 0.37313829774552204, 0.3744680849795646, 0.3751994682119248, 0.3748005317880752, 0.368550532041712, 0.37746010625616033, 0.380518617148095, 0.377726064083424, 0.3793882981259772, 0.37606383004087085, 0.37413563817105394, 0.37513297859658584, 0.3738696809778822, 0.37998670212765956, 0.3718750001268184, 0.3716755319148936, 0.36881648923488375, 0.3671542551923305, 0.3714760637029688, 0.37918882991405245, 0.3775265958714992, 0.38257978761449773, 0.37586436182894606, 0.38503989387065807, 0.37586436182894606, 0.37686170225447796, 0.3761303190221178, 0.3750664896153389, 0.3752659578272637, 0.3785239362970312, 0.37420212778639284, 0.3808510639566056, 0.38158244718896583, 0.3809840425531915, 0.3789228727208807, 0.3793218085106383, 0.3808510639566056, 0.3701462764689263, 0.37313829774552204, 0.369481382851905, 0.3701462764689263, 0.3785239362970312, 0.37121010650979713, 0.3748005317880752, 0.3734042555727857, 0.37679521263913907, 0.3815159575736269, 0.3765292554459673, 0.3773936172749134, 0.3785239362970312, 0.37918882991405245, 0.37313829774552204, 0.3818484043821375, 0.3842420216570509, 0.3714760637029688, 0.38158244718896583, 0.3781914894885205, 0.3718750001268184, 0.38257978761449773, 0.3773271276595745, 0.37726063867832754, 0.3785904259123701, 0.383244681231519, 0.3745345745949035, 0.37420212778639284, 0.37845744668169223, 0.3765292554459673, 0.3851728724672439, 0.3812500003804552, 0.38251329799915884, 0.3751994682119248, 0.38025265995492324, 0.3779255322953488, 0.38231382978723405, 0.37586436182894606, 0.37925531952939134, 0.38091755357194457, 0.38430851063829785, 0.38457446846556154, 0.38257978761449773, 0.37785904268000986, 0.38430851063829785, 0.3816489361702128, 0.37273936195576446, 0.3761968086374567, 0.3718750001268184, 0.3711436168944582, 0.38091755357194457, 0.37679521263913907, 0.37686170225447796, 0.37174202153023256, 0.37107712791321124, 0.37273936195576446, 0.37606383004087085, 0.38138297897704104, 0.37905585131746655, 0.37486702140341416, 0.3775265958714992, 0.3775930854868382, 0.3755984046357743, 0.3785904259123701, 0.3755319150204354, 0.3785239362970312, 0.3808510639566056, 0.3791223402987135, 0.37007978748767933, 0.37413563817105394, 0.3714760637029688, 0.3746010642102424, 0.37513297859658584, 0.3797207449344879, 0.3761303190221178, 0.37107712791321124, 0.3728058509370114, 0.37679521263913907, 0.37845744668169223, 0.38018617033958435, 0.3797207449344879, 0.3812500003804552, 0.3834441488093518, 0.3800531917429985, 0.39035904280682826, 0.38390957484854027, 0.38351063842469074, 0.37998670212765956, 0.37686170225447796, 0.3781914894885205, 0.3714760637029688, 0.380518617148095, 0.3791223402987135, 0.3785239362970312, 0.38623670250811476, 0.3798537235310737, 0.3853058510638298, 0.37945478710722413, 0.38251329799915884, 0.37646276583062843, 0.380518617148095, 0.3761968086374567, 0.3798537235310737, 0.37785904268000986, 0.37925531952939134, 0.3781914894885205, 0.3779255322953488, 0.38131648936170215, 0.37121010650979713, 0.38470744706214743, 0.3910239364238495, 0.3762632982527956, 0.38091755357194457, 0.3882978723404255, 0.38390957484854027, 0.3879654255319149, 0.38523936208258286, 0.3815159575736269, 0.378656914893617, 0.38257978761449773, 0.38663563829787234, 0.3795212767225631, 0.3839760638297872, 0.38896276595744683, 0.3859707446808511, 0.39268617046640275, 0.3812500003804552, 0.3859042556996041, 0.38843085157110335, 0.3873005319148936, 0.38257978761449773, 0.3836436170212766, 0.38337765982810484, 0.3859707446808511, 0.38191489399747647, 0.3761968086374567, 0.3718750001268184, 0.37679521263913907, 0.38404255344512617, 0.38291223442300837, 0.37686170225447796, 0.3826462765957447, 0.3843750002536368, 0.3900265959983176, 0.3880319151472538, 0.380518617148095, 0.37513297859658584, 0.38863031914893614, 0.3916223404255319, 0.3855718088910935, 0.3819813829787234, 0.3863696811047006, 0.3910239364238495, 0.383244681231519, 0.3953457449344879, 0.37586436182894606, 0.3913563832323602, 0.3966755321685304, 0.3902260642102424, 0.38191489399747647, 0.3913563832323602, 0.3880319151472538, 0.3882978723404255, 0.3900265959983176, 0.38191489399747647, 0.38996010638297873, 0.392420213273231, 0.390625, 0.393351064083424, 0.39893617021276595, 0.39268617046640275, 0.39029255319148937, 0.39507978774131614, 0.39261968085106386, 0.385106382851905, 0.38896276595744683, 0.39268617046640275, 0.38956117059322115, 0.3859042556996041, 0.38783244693532903, 0.39128989361702127, 0.3882978723404255, 0.38703457472172187, 0.3858377660842652, 0.38111702114977736, 0.3935505322953488, 0.38238031940257294, 0.3817819147667986, 0.3837765956178625, 0.3855718088910935, 0.38916223416937157, 0.3863696811047006, 0.38996010638297873, 0.3921542554459673, 0.38091755357194457, 0.3902260642102424, 0.386901596125136, 0.3898936174017318, 0.39155585144428495, 0.38863031914893614, 0.38523936208258286, 0.3855718088910935, 0.38683510650979713, 0.39468085131746655, 0.3935505322953488, 0.3888962769761999, 0.39654255357194457, 0.39800531940257294, 0.3929521276595745, 0.3947473409328055, 0.3980718090179119, 0.3906914896153389, 0.3923537236578921, 0.3912234046357743, 0.3932180854868382, 0.3939494680851064, 0.39461436170212766, 0.39288563867832754, 0.3962765957446808, 0.38783244693532903, 0.3912234046357743, 0.39148936182894606, 0.3939494680851064, 0.3925531918698169, 0.3929521276595745, 0.4015957446808511, 0.3966090425531915, 0.3912234046357743, 0.3976063829787234, 0.3932180854868382, 0.386901596125136, 0.3925531918698169, 0.3925531918698169, 0.39288563867832754, 0.38816489374383967, 0.395212766337902, 0.38956117059322115, 0.3912234046357743, 0.39029255319148937, 0.3929521276595745, 0.3916223404255319, 0.3966755321685304, 0.3967420217838693, 0.3935505322953488, 0.39601063855150914, 0.3921542554459673, 0.3966090425531915, 0.3871010637029688]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnYyDyVoehSr",
        "outputId": "a5484ce4-5be1-4131-aa64-68a7662715ec"
      },
      "source": [
        "max(valid_acc_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4015957446808511"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 385
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOXSn_4MM62i"
      },
      "source": [
        "### Model diagnosis and Functions for showing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdP_YkZrCCiu",
        "outputId": "e9e030be-c9d7-41f6-c5ab-4108282e9668"
      },
      "source": [
        "preds, act = list(zip(*preds_actual_tup))\n",
        "zero_preds = 0\n",
        "same_probability = 0\n",
        "for i in preds:\n",
        "  for j in i:\n",
        "    maxv, pred = torch.max(j, dim = 0)\n",
        "    minv, pred = torch.min(j, dim = 0)\n",
        "    if pred == 0:\n",
        "      zero_preds += 1\n",
        "      if maxv - minv < 0.1:\n",
        "        same_probability += 1\n",
        "      \n",
        "zero_preds, same_probability"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZfnWo0abRx"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_review(review):\n",
        "    \n",
        "    categories = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3:\"Positive\", 4:\"Very Positive\"}\n",
        "    \n",
        "    # tokenize the review \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(review)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()], pred.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SnsyRRHL3Zb"
      },
      "source": [
        "def classify_and_print(test_list):\n",
        "  for i in test_list:\n",
        "    x,y = i\n",
        "    cat  = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3:\"Positive\", 4:\"Very Positive\"}\n",
        "    actual_label = cat[y]\n",
        "    predicted_label_str, predicted_label = classify_review(x) \n",
        "    print(\"sentence: \", x)\n",
        "    print(\"actual_label: \", actual_label, \"predicted_label: \", predicted_label_str)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4CKUTC1PdIC",
        "outputId": "0b0fe25d-4b24-4b9a-8620-1ffcc7cfbcaf"
      },
      "source": [
        "testdata_to_test = random.sample(testlist, 25)\n",
        "classify_and_print(testdata_to_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence:  Possibly not since Grumpy Old Men have I heard a film so solidly connect with one demographic while striking out with another .\n",
            "actual_label:  Neutral predicted_label:  Very Negative\n",
            "\n",
            "sentence:  Burns never really harnesses to full effect the energetic cast .\n",
            "actual_label:  Negative predicted_label:  Negative\n",
            "\n",
            "sentence:  ( Screenwriter ) Pimental took the Farrelly Brothers comedy and feminized it , but it is a rather poor imitation .\n",
            "actual_label:  Neutral predicted_label:  Negative\n",
            "\n",
            "sentence:  Unless you 're a fanatic , the best advice is : ` Scooby ' do n't .\n",
            "actual_label:  Negative predicted_label:  Very Positive\n",
            "\n",
            "sentence:  The film takes the materials of human tragedy and dresses them in lovely costumes , Southern California locations and star power .\n",
            "actual_label:  Neutral predicted_label:  Negative\n",
            "\n",
            "sentence:  It is philosophy , illustrated through everyday events .\n",
            "actual_label:  Positive predicted_label:  Very Negative\n",
            "\n",
            "sentence:  Daughter From Danang reveals that efforts toward closure only open new wounds .\n",
            "actual_label:  Negative predicted_label:  Very Negative\n",
            "\n",
            "sentence:  A small movie with a big impact .\n",
            "actual_label:  Very Positive predicted_label:  Negative\n",
            "\n",
            "sentence:  ( Reaches ) wholly believable and heart-wrenching depths of despair .\n",
            "actual_label:  Positive predicted_label:  Negative\n",
            "\n",
            "sentence:  It 's not just the vampires that are damned in Queen of the Damned -- the viewers will feel they suffer the same fate .\n",
            "actual_label:  Very Negative predicted_label:  Negative\n",
            "\n",
            "sentence:  A woozy , roisterous , exhausting mess , and the off-beat casting of its two leads turns out to be as ill-starred as you might expect .\n",
            "actual_label:  Negative predicted_label:  Very Negative\n",
            "\n",
            "sentence:  It 's difficult to imagine the process that produced such a script , but here 's guessing that spray cheese and underarm noises played a crucial role .\n",
            "actual_label:  Very Negative predicted_label:  Neutral\n",
            "\n",
            "sentence:  For most of its footage , the new thriller proves that director M. Night Shyamalan can weave an eerie spell and that Mel Gibson can gasp , shudder and even tremble without losing his machismo .\n",
            "actual_label:  Very Positive predicted_label:  Positive\n",
            "\n",
            "sentence:  A model of what films like this should be like .\n",
            "actual_label:  Very Positive predicted_label:  Very Negative\n",
            "\n",
            "sentence:  Offers laughs and insight into one of the toughest ages a kid can go through .\n",
            "actual_label:  Positive predicted_label:  Very Negative\n",
            "\n",
            "sentence:  Is it something any true film addict will want to check out ?\n",
            "actual_label:  Neutral predicted_label:  Very Negative\n",
            "\n",
            "sentence:  What happens when something goes bump in the night and nobody cares ?\n",
            "actual_label:  Negative predicted_label:  Negative\n",
            "\n",
            "sentence:  It dares to be a little different , and that shading is what makes it worthwhile .\n",
            "actual_label:  Positive predicted_label:  Negative\n",
            "\n",
            "sentence:  If you like quirky , odd movies and/or the ironic , here 's a fun one .\n",
            "actual_label:  Very Positive predicted_label:  Very Positive\n",
            "\n",
            "sentence:  It would be interesting to hear from the other side , but in Talk to Her , the women are down for the count .\n",
            "actual_label:  Neutral predicted_label:  Negative\n",
            "\n",
            "sentence:  Ah yes , and then there 's the music ...\n",
            "actual_label:  Neutral predicted_label:  Negative\n",
            "\n",
            "sentence:  If you 're not the target demographic ... this movie is one long chick-flick slog .\n",
            "actual_label:  Negative predicted_label:  Very Positive\n",
            "\n",
            "sentence:  But watching Huppert , a great actress tearing into a landmark role , is riveting .\n",
            "actual_label:  Very Positive predicted_label:  Neutral\n",
            "\n",
            "sentence:  We hate ( Madonna ) within the film 's first five minutes , and she lacks the skill or presence to regain any ground .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  Filmmaker Stacy Peralta has a flashy editing style that does n't always jell with Sean Penn 's monotone narration , but he respects the material without sentimentalizing it .\n",
            "actual_label:  Positive predicted_label:  Positive\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVHxktkNPjFG",
        "outputId": "33ae0951-d4bc-4a5b-cea1-077e6d7d0bb6"
      },
      "source": [
        "count = 0\n",
        "false_pos_list = []\n",
        "for i in testlist:\n",
        "  x,y = i\n",
        "  _, predicted_label = classify_review(x) \n",
        "  if predicted_label in [3,4] and y in [0,1]:\n",
        "    false_pos_list.append(i)\n",
        "    count += 1\n",
        "    if count == 10:\n",
        "      break\n",
        "\n",
        "print(false_pos_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"The movie is silly beyond comprehension , and even if it were n't silly , it would still be beyond comprehension .\", 0], ['One of those decades-spanning historical epics that strives to be intimate and socially encompassing but fails to do justice to either effort in three hours of screen time .', 1], ['Alas , getting there is not even half the interest .', 0], [\"It 's quite diverting nonsense .\", 1], [\"... Liotta is put in an impossible spot because his character 's deceptions ultimately undo him and the believability of the entire scenario .\", 1], ['After the first 10 minutes , which is worth seeing , the movie sinks into an abyss of clichés , depression and bad alternative music .', 1], ['Witless and utterly pointless .', 1], [\"The filmmakers juggle and juxtapose three story lines but fail to come up with one cogent point , unless it 's that life stinks , especially for sensitive married women who really love other women .\", 1], ['I hated every minute of it .', 0], ['Why he was given free reign over this project -- he wrote , directed , starred and produced -- is beyond me .', 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZvrK7k0RsGY",
        "outputId": "46d84f76-cf48-44aa-fa10-1702e05f09f7"
      },
      "source": [
        "classify_and_print(false_pos_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence:  The movie is silly beyond comprehension , and even if it were n't silly , it would still be beyond comprehension .\n",
            "actual_label:  Very Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  One of those decades-spanning historical epics that strives to be intimate and socially encompassing but fails to do justice to either effort in three hours of screen time .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  Alas , getting there is not even half the interest .\n",
            "actual_label:  Very Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  It 's quite diverting nonsense .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  ... Liotta is put in an impossible spot because his character 's deceptions ultimately undo him and the believability of the entire scenario .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  After the first 10 minutes , which is worth seeing , the movie sinks into an abyss of clichés , depression and bad alternative music .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  Witless and utterly pointless .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  The filmmakers juggle and juxtapose three story lines but fail to come up with one cogent point , unless it 's that life stinks , especially for sensitive married women who really love other women .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  I hated every minute of it .\n",
            "actual_label:  Very Negative predicted_label:  Very Positive\n",
            "\n",
            "sentence:  Why he was given free reign over this project -- he wrote , directed , starred and produced -- is beyond me .\n",
            "actual_label:  Very Negative predicted_label:  Very Positive\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMZLqxEPO21E"
      },
      "source": [
        "\n",
        "for i in testdata_to_test:\n",
        "  x,y = i\n",
        "  cat  = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3:\"Positive\", 4:\"Very Positive\"}\n",
        "  actual_label = cat[y]\n",
        "  predicted_label = classify_review(x) \n",
        "  print(\"sentence: \", x)\n",
        "  print(\"actual_label: \", actual_label, \"predicted_label: \", predicted_label)\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qUbSUk58RuD"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq6OyEd68RHG"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK8qm_OjCoHx"
      },
      "source": [
        "preds2 = []\n",
        "for i in preds:\n",
        "  _, predictions = torch.max(i, 1)\n",
        "  #print(predictions)\n",
        "  #break\n",
        "  for j in predictions:\n",
        "    preds2.append(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ShyiCwd-y_p",
        "outputId": "30399d4b-6304-4b9e-82bf-2ba1162fad18"
      },
      "source": [
        "dic = get_pred_vs_act_per_label(preds_actual_tup)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label distribution: actual vs pred: {3: {1: 194, 4: 32, 3: 105, 0: 194, 2: 33}, 2: {2: 202, 1: 203, 3: 23, 0: 32, 4: 6}, 1: {3: 84, 1: 426, 0: 127, 4: 26, 2: 143}, 0: {0: 354, 3: 121, 4: 80, 1: 159, 2: 27}, 4: {1: 72, 3: 41, 0: 201, 4: 59, 2: 20}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkpqgf3SO8KH",
        "outputId": "9d09ebe4-ecd2-477a-9eec-3e5b54fd7c89"
      },
      "source": [
        "final_su = 0\n",
        "final_co = 0\n",
        "for keys in dic:\n",
        "  key = dic[keys]\n",
        "  su = 0\n",
        "  predicted = 0\n",
        "  for keys2 in key:\n",
        "     su += key[keys2]\n",
        "     if keys2 == keys:\n",
        "       predicted = key[keys2]\n",
        "  print(keys, su, predicted)\n",
        "  final_su += su\n",
        "  final_co += predicted\n",
        "print(final_su, final_co)\n",
        "print(final_co/final_su)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 558 105\n",
            "2 466 202\n",
            "1 806 426\n",
            "0 741 354\n",
            "4 393 59\n",
            "2964 1146\n",
            "0.3866396761133603\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
