size_of_vocab = len(Review.vocab)
embedding_dim = 300
num_hidden_nodes = 100
num_output_nodes = 5
num_layers = 2
dropout = 0.3
lr = 1e-4

1
	Train Loss: 1.605 | Train Acc: 23.47%
	 Val. Loss: 1.595 |  Val. Acc: 25.66% 

2
	Train Loss: 1.602 | Train Acc: 23.83%
	 Val. Loss: 1.591 |  Val. Acc: 26.34% 

3
	Train Loss: 1.599 | Train Acc: 24.86%
	 Val. Loss: 1.588 |  Val. Acc: 26.47% 

4
	Train Loss: 1.600 | Train Acc: 23.97%
	 Val. Loss: 1.588 |  Val. Acc: 26.88% 

5
	Train Loss: 1.598 | Train Acc: 25.33%
	 Val. Loss: 1.587 |  Val. Acc: 27.65% 

6
	Train Loss: 1.598 | Train Acc: 25.55%
	 Val. Loss: 1.587 |  Val. Acc: 27.88% 

7
	Train Loss: 1.595 | Train Acc: 25.65%
	 Val. Loss: 1.585 |  Val. Acc: 28.42% 

8
	Train Loss: 1.591 | Train Acc: 26.69%
	 Val. Loss: 1.582 |  Val. Acc: 28.72% 

9
	Train Loss: 1.590 | Train Acc: 26.41%
	 Val. Loss: 1.580 |  Val. Acc: 29.32% 

10
	Train Loss: 1.584 | Train Acc: 27.79%
	 Val. Loss: 1.577 |  Val. Acc: 29.92% 

11
	Train Loss: 1.575 | Train Acc: 29.42%
	 Val. Loss: 1.570 |  Val. Acc: 30.93% 

12
	Train Loss: 1.568 | Train Acc: 30.53%
	 Val. Loss: 1.564 |  Val. Acc: 31.56% 

13
	Train Loss: 1.558 | Train Acc: 32.16%
	 Val. Loss: 1.562 |  Val. Acc: 31.42% 

14
	Train Loss: 1.547 | Train Acc: 33.04%
	 Val. Loss: 1.559 |  Val. Acc: 30.92% 

15
	Train Loss: 1.538 | Train Acc: 34.92%
	 Val. Loss: 1.556 |  Val. Acc: 30.86% 

16
	Train Loss: 1.535 | Train Acc: 34.85%
	 Val. Loss: 1.551 |  Val. Acc: 31.92% 

17
	Train Loss: 1.528 | Train Acc: 35.84%
	 Val. Loss: 1.548 |  Val. Acc: 32.69% 

18
	Train Loss: 1.518 | Train Acc: 36.95%
	 Val. Loss: 1.549 |  Val. Acc: 32.32% 

19
	Train Loss: 1.513 | Train Acc: 37.95%
	 Val. Loss: 1.547 |  Val. Acc: 32.92% 

20
	Train Loss: 1.506 | Train Acc: 38.19%
	 Val. Loss: 1.542 |  Val. Acc: 33.32% 

label distribution: actual vs pred: {2: {0: 246, 2: 76, 1: 172, 3: 62}, 0: {2: 81, 0: 421, 1: 201, 3: 73}, 1: {1: 308, 0: 189, 2: 60, 3: 194, 4: 1}, 4: {1: 82, 0: 236, 3: 51, 2: 31, 4: 1}, 3: {0: 80, 1: 190, 2: 25, 3: 183, 4: 1}}
	 pred vs actuals: {2: {0: 246, 2: 76, 1: 172, 3: 62}, 0: {2: 81, 0: 421, 1: 201, 3: 73}, 1: {1: 308, 0: 189, 2: 60, 3: 194, 4: 1}, 4: {1: 82, 0: 236, 3: 51, 2: 31, 4: 1}, 3: {0: 80, 1: 190, 2: 25, 3: 183, 4: 1}}
21
	Train Loss: 1.504 | Train Acc: 38.39%
	 Val. Loss: 1.540 |  Val. Acc: 33.56% 

22
	Train Loss: 1.499 | Train Acc: 39.27%
	 Val. Loss: 1.538 |  Val. Acc: 33.83% 

23
	Train Loss: 1.489 | Train Acc: 40.25%
	 Val. Loss: 1.534 |  Val. Acc: 34.79% 

24
	Train Loss: 1.480 | Train Acc: 41.44%
	 Val. Loss: 1.536 |  Val. Acc: 34.30% 

25
	Train Loss: 1.476 | Train Acc: 41.96%
	 Val. Loss: 1.537 |  Val. Acc: 34.56% 

26
	Train Loss: 1.474 | Train Acc: 41.62%
	 Val. Loss: 1.540 |  Val. Acc: 34.13% 

27
	Train Loss: 1.469 | Train Acc: 42.62%
	 Val. Loss: 1.530 |  Val. Acc: 34.99% 

28
	Train Loss: 1.462 | Train Acc: 43.36%
	 Val. Loss: 1.533 |  Val. Acc: 34.56% 

29
	Train Loss: 1.460 | Train Acc: 43.25%
	 Val. Loss: 1.537 |  Val. Acc: 34.29% 

30
	Train Loss: 1.455 | Train Acc: 43.95%
	 Val. Loss: 1.528 |  Val. Acc: 35.65% 

31
	Train Loss: 1.449 | Train Acc: 44.84%
	 Val. Loss: 1.528 |  Val. Acc: 35.36% 

32
	Train Loss: 1.451 | Train Acc: 44.72%
	 Val. Loss: 1.533 |  Val. Acc: 34.85% 

33
	Train Loss: 1.437 | Train Acc: 45.76%
	 Val. Loss: 1.532 |  Val. Acc: 34.86% 

34
	Train Loss: 1.435 | Train Acc: 46.10%
	 Val. Loss: 1.531 |  Val. Acc: 35.19% 

35
	Train Loss: 1.434 | Train Acc: 46.60%
	 Val. Loss: 1.535 |  Val. Acc: 34.33% 

36
	Train Loss: 1.428 | Train Acc: 47.03%
	 Val. Loss: 1.532 |  Val. Acc: 34.96% 

37
	Train Loss: 1.424 | Train Acc: 47.38%
	 Val. Loss: 1.531 |  Val. Acc: 34.86% 

38
	Train Loss: 1.418 | Train Acc: 48.33%
	 Val. Loss: 1.528 |  Val. Acc: 35.29% 

39
	Train Loss: 1.414 | Train Acc: 48.71%
	 Val. Loss: 1.535 |  Val. Acc: 34.65% 

40
	Train Loss: 1.414 | Train Acc: 48.49%
	 Val. Loss: 1.531 |  Val. Acc: 35.18% 

label distribution: actual vs pred: {2: {0: 220, 2: 79, 3: 51, 1: 201, 4: 5}, 0: {2: 94, 4: 15, 0: 403, 1: 200, 3: 64}, 1: {2: 56, 0: 132, 1: 385, 3: 171, 4: 8}, 4: {0: 222, 3: 32, 2: 36, 4: 7, 1: 104}, 3: {4: 9, 2: 18, 1: 228, 3: 171, 0: 53}}
	 pred vs actuals: {2: {0: 220, 2: 79, 3: 51, 1: 201, 4: 5}, 0: {2: 94, 4: 15, 0: 403, 1: 200, 3: 64}, 1: {2: 56, 0: 132, 1: 385, 3: 171, 4: 8}, 4: {0: 222, 3: 32, 2: 36, 4: 7, 1: 104}, 3: {4: 9, 2: 18, 1: 228, 3: 171, 0: 53}}
41
	Train Loss: 1.406 | Train Acc: 49.60%
	 Val. Loss: 1.535 |  Val. Acc: 34.46% 

42
	Train Loss: 1.405 | Train Acc: 49.80%
	 Val. Loss: 1.529 |  Val. Acc: 35.55% 

43
	Train Loss: 1.396 | Train Acc: 50.94%
	 Val. Loss: 1.535 |  Val. Acc: 35.08% 

44
	Train Loss: 1.392 | Train Acc: 51.04%
	 Val. Loss: 1.532 |  Val. Acc: 35.31% 

45
	Train Loss: 1.394 | Train Acc: 50.77%
	 Val. Loss: 1.528 |  Val. Acc: 35.82% 

46
	Train Loss: 1.391 | Train Acc: 51.01%
	 Val. Loss: 1.530 |  Val. Acc: 35.13% 

47
	Train Loss: 1.387 | Train Acc: 51.38%
	 Val. Loss: 1.532 |  Val. Acc: 35.29% 

48
	Train Loss: 1.377 | Train Acc: 52.98%
	 Val. Loss: 1.532 |  Val. Acc: 35.02% 

49
	Train Loss: 1.375 | Train Acc: 52.82%
	 Val. Loss: 1.532 |  Val. Acc: 35.03% 

50
	Train Loss: 1.378 | Train Acc: 52.41%
	 Val. Loss: 1.531 |  Val. Acc: 35.36% 

51
	Train Loss: 1.374 | Train Acc: 53.42%
	 Val. Loss: 1.533 |  Val. Acc: 35.25% 

52
	Train Loss: 1.375 | Train Acc: 52.74%
	 Val. Loss: 1.532 |  Val. Acc: 35.39% 

53
	Train Loss: 1.362 | Train Acc: 54.12%
	 Val. Loss: 1.537 |  Val. Acc: 34.93% 

54
	Train Loss: 1.363 | Train Acc: 54.11%
	 Val. Loss: 1.536 |  Val. Acc: 35.02% 

55
	Train Loss: 1.358 | Train Acc: 55.07%
	 Val. Loss: 1.531 |  Val. Acc: 35.08% 

56
	Train Loss: 1.356 | Train Acc: 54.58%
	 Val. Loss: 1.531 |  Val. Acc: 36.12% 

57
	Train Loss: 1.352 | Train Acc: 55.39%
	 Val. Loss: 1.531 |  Val. Acc: 35.39% 

58
	Train Loss: 1.354 | Train Acc: 55.19%
	 Val. Loss: 1.533 |  Val. Acc: 35.45% 

59
	Train Loss: 1.350 | Train Acc: 55.63%
	 Val. Loss: 1.536 |  Val. Acc: 35.15% 

60
	Train Loss: 1.349 | Train Acc: 55.63%
	 Val. Loss: 1.537 |  Val. Acc: 34.89% 

label distribution: actual vs pred: {2: {0: 189, 2: 84, 3: 53, 1: 218, 4: 12}, 0: {2: 121, 4: 25, 0: 369, 1: 207, 3: 54}, 1: {2: 75, 1: 387, 3: 165, 4: 13, 0: 112}, 4: {1: 102, 2: 45, 0: 212, 3: 24, 4: 18}, 3: {3: 176, 2: 22, 1: 234, 4: 5, 0: 42}}
	 pred vs actuals: {2: {0: 189, 2: 84, 3: 53, 1: 218, 4: 12}, 0: {2: 121, 4: 25, 0: 369, 1: 207, 3: 54}, 1: {2: 75, 1: 387, 3: 165, 4: 13, 0: 112}, 4: {1: 102, 2: 45, 0: 212, 3: 24, 4: 18}, 3: {3: 176, 2: 22, 1: 234, 4: 5, 0: 42}}
61
	Train Loss: 1.343 | Train Acc: 56.03%
	 Val. Loss: 1.536 |  Val. Acc: 35.22% 

62
	Train Loss: 1.340 | Train Acc: 56.68%
	 Val. Loss: 1.542 |  Val. Acc: 34.38% 

63
	Train Loss: 1.341 | Train Acc: 56.39%
	 Val. Loss: 1.534 |  Val. Acc: 35.29% 

64
	Train Loss: 1.334 | Train Acc: 57.15%
	 Val. Loss: 1.536 |  Val. Acc: 35.09% 

65
	Train Loss: 1.330 | Train Acc: 57.73%
	 Val. Loss: 1.535 |  Val. Acc: 35.11% 

66
	Train Loss: 1.331 | Train Acc: 57.29%
	 Val. Loss: 1.533 |  Val. Acc: 35.62% 

67
	Train Loss: 1.325 | Train Acc: 58.14%
	 Val. Loss: 1.534 |  Val. Acc: 35.35% 

68
	Train Loss: 1.324 | Train Acc: 57.84%
	 Val. Loss: 1.544 |  Val. Acc: 34.59% 

69
	Train Loss: 1.321 | Train Acc: 58.77%
	 Val. Loss: 1.537 |  Val. Acc: 35.01% 

70
	Train Loss: 1.319 | Train Acc: 58.70%
	 Val. Loss: 1.533 |  Val. Acc: 35.56% 

71
	Train Loss: 1.319 | Train Acc: 58.56%
	 Val. Loss: 1.536 |  Val. Acc: 35.22% 

72
	Train Loss: 1.315 | Train Acc: 59.19%
	 Val. Loss: 1.539 |  Val. Acc: 34.85% 

73
	Train Loss: 1.314 | Train Acc: 59.27%
	 Val. Loss: 1.536 |  Val. Acc: 35.02% 

74
	Train Loss: 1.312 | Train Acc: 59.17%
	 Val. Loss: 1.536 |  Val. Acc: 35.45% 

75
	Train Loss: 1.311 | Train Acc: 59.81%
	 Val. Loss: 1.537 |  Val. Acc: 35.41% 

76
	Train Loss: 1.304 | Train Acc: 60.15%
	 Val. Loss: 1.539 |  Val. Acc: 34.94% 

77
	Train Loss: 1.304 | Train Acc: 60.10%
	 Val. Loss: 1.530 |  Val. Acc: 35.92% 

78
	Train Loss: 1.296 | Train Acc: 61.08%
	 Val. Loss: 1.532 |  Val. Acc: 35.96% 

79
	Train Loss: 1.292 | Train Acc: 61.31%
	 Val. Loss: 1.538 |  Val. Acc: 35.05% 

80
	Train Loss: 1.297 | Train Acc: 60.93%
	 Val. Loss: 1.537 |  Val. Acc: 35.16% 

label distribution: actual vs pred: {2: {2: 99, 3: 56, 0: 167, 1: 214, 4: 20}, 0: {2: 143, 3: 55, 0: 331, 1: 219, 4: 28}, 1: {2: 82, 1: 396, 3: 162, 0: 99, 4: 13}, 4: {1: 107, 2: 49, 3: 27, 0: 183, 4: 35}, 3: {3: 179, 2: 24, 1: 236, 4: 5, 0: 35}}
	 pred vs actuals: {2: {2: 99, 3: 56, 0: 167, 1: 214, 4: 20}, 0: {2: 143, 3: 55, 0: 331, 1: 219, 4: 28}, 1: {2: 82, 1: 396, 3: 162, 0: 99, 4: 13}, 4: {1: 107, 2: 49, 3: 27, 0: 183, 4: 35}, 3: {3: 179, 2: 24, 1: 236, 4: 5, 0: 35}}
81
	Train Loss: 1.297 | Train Acc: 60.97%
	 Val. Loss: 1.534 |  Val. Acc: 35.46% 

82
	Train Loss: 1.292 | Train Acc: 61.57%
	 Val. Loss: 1.530 |  Val. Acc: 35.96% 

83
	Train Loss: 1.295 | Train Acc: 60.93%
	 Val. Loss: 1.535 |  Val. Acc: 35.56% 

84
	Train Loss: 1.282 | Train Acc: 62.48%
	 Val. Loss: 1.532 |  Val. Acc: 35.65% 

85
	Train Loss: 1.288 | Train Acc: 61.79%
	 Val. Loss: 1.532 |  Val. Acc: 35.59% 

86
	Train Loss: 1.280 | Train Acc: 62.67%
	 Val. Loss: 1.534 |  Val. Acc: 35.55% 

87
	Train Loss: 1.280 | Train Acc: 61.97%
	 Val. Loss: 1.536 |  Val. Acc: 35.65% 

88
	Train Loss: 1.279 | Train Acc: 62.84%
	 Val. Loss: 1.536 |  Val. Acc: 35.53% 

89
	Train Loss: 1.279 | Train Acc: 62.86%
	 Val. Loss: 1.526 |  Val. Acc: 36.50% 

90
	Train Loss: 1.277 | Train Acc: 62.88%
	 Val. Loss: 1.536 |  Val. Acc: 35.32% 

91
	Train Loss: 1.274 | Train Acc: 62.71%
	 Val. Loss: 1.531 |  Val. Acc: 36.13% 

92
	Train Loss: 1.270 | Train Acc: 63.59%
	 Val. Loss: 1.533 |  Val. Acc: 35.86% 

93
	Train Loss: 1.271 | Train Acc: 63.24%
	 Val. Loss: 1.527 |  Val. Acc: 36.59% 

94
	Train Loss: 1.261 | Train Acc: 64.39%
	 Val. Loss: 1.525 |  Val. Acc: 36.74% 

95
	Train Loss: 1.268 | Train Acc: 63.80%
	 Val. Loss: 1.539 |  Val. Acc: 35.18% 

96
	Train Loss: 1.259 | Train Acc: 64.56%
	 Val. Loss: 1.543 |  Val. Acc: 34.95% 

97
	Train Loss: 1.265 | Train Acc: 64.09%
	 Val. Loss: 1.541 |  Val. Acc: 35.05% 

98
	Train Loss: 1.255 | Train Acc: 64.98%
	 Val. Loss: 1.538 |  Val. Acc: 35.33% 

99
	Train Loss: 1.256 | Train Acc: 64.83%
	 Val. Loss: 1.536 |  Val. Acc: 35.64% 

100
	Train Loss: 1.265 | Train Acc: 64.22%
	 Val. Loss: 1.535 |  Val. Acc: 35.68% 

label distribution: actual vs pred: {2: {2: 86, 4: 25, 0: 189, 1: 195, 3: 61}, 0: {2: 134, 4: 39, 0: 368, 3: 54, 1: 181}, 1: {2: 73, 1: 357, 3: 185, 0: 123, 4: 14}, 4: {0: 206, 2: 41, 1: 81, 3: 26, 4: 47}, 3: {1: 203, 2: 25, 4: 10, 3: 200, 0: 41}}
	 pred vs actuals: {2: {2: 86, 4: 25, 0: 189, 1: 195, 3: 61}, 0: {2: 134, 4: 39, 0: 368, 3: 54, 1: 181}, 1: {2: 73, 1: 357, 3: 185, 0: 123, 4: 14}, 4: {0: 206, 2: 41, 1: 81, 3: 26, 4: 47}, 3: {1: 203, 2: 25, 4: 10, 3: 200, 0: 41}}
101
	Train Loss: 1.253 | Train Acc: 65.36%
	 Val. Loss: 1.535 |  Val. Acc: 35.88% 

102
	Train Loss: 1.256 | Train Acc: 64.98%
	 Val. Loss: 1.540 |  Val. Acc: 35.01% 

103
	Train Loss: 1.252 | Train Acc: 65.42%
	 Val. Loss: 1.535 |  Val. Acc: 35.59% 

104
	Train Loss: 1.246 | Train Acc: 65.80%
	 Val. Loss: 1.540 |  Val. Acc: 35.33% 

105
	Train Loss: 1.244 | Train Acc: 66.00%
	 Val. Loss: 1.535 |  Val. Acc: 35.81% 

106
	Train Loss: 1.245 | Train Acc: 65.91%
	 Val. Loss: 1.528 |  Val. Acc: 36.82% 

107
	Train Loss: 1.246 | Train Acc: 65.98%
	 Val. Loss: 1.537 |  Val. Acc: 35.68% 

108
	Train Loss: 1.246 | Train Acc: 65.99%
	 Val. Loss: 1.534 |  Val. Acc: 36.13% 

109
	Train Loss: 1.244 | Train Acc: 65.94%
	 Val. Loss: 1.539 |  Val. Acc: 35.84% 

110
	Train Loss: 1.246 | Train Acc: 65.72%
	 Val. Loss: 1.538 |  Val. Acc: 35.57% 

111
	Train Loss: 1.242 | Train Acc: 66.29%
	 Val. Loss: 1.535 |  Val. Acc: 35.94% 

112
	Train Loss: 1.239 | Train Acc: 66.31%
	 Val. Loss: 1.532 |  Val. Acc: 36.34% 

113
	Train Loss: 1.239 | Train Acc: 66.33%
	 Val. Loss: 1.531 |  Val. Acc: 36.58% 

114
	Train Loss: 1.234 | Train Acc: 66.93%
	 Val. Loss: 1.529 |  Val. Acc: 36.61% 

115
	Train Loss: 1.233 | Train Acc: 67.29%
	 Val. Loss: 1.532 |  Val. Acc: 36.22% 

116
	Train Loss: 1.230 | Train Acc: 67.39%
	 Val. Loss: 1.536 |  Val. Acc: 35.55% 

117
	Train Loss: 1.228 | Train Acc: 68.02%
	 Val. Loss: 1.534 |  Val. Acc: 35.84% 

118
	Train Loss: 1.228 | Train Acc: 67.51%
	 Val. Loss: 1.534 |  Val. Acc: 36.10% 

119
	Train Loss: 1.226 | Train Acc: 67.71%
	 Val. Loss: 1.531 |  Val. Acc: 36.32% 

120
	Train Loss: 1.226 | Train Acc: 67.58%
	 Val. Loss: 1.536 |  Val. Acc: 35.92% 

label distribution: actual vs pred: {2: {2: 94, 4: 21, 0: 174, 1: 218, 3: 49}, 0: {2: 139, 4: 32, 0: 338, 1: 218, 3: 49}, 1: {2: 80, 1: 407, 3: 152, 4: 12, 0: 101}, 4: {4: 42, 2: 46, 1: 101, 3: 27, 0: 185}, 3: {1: 233, 2: 23, 3: 182, 4: 4, 0: 37}}
	 pred vs actuals: {2: {2: 94, 4: 21, 0: 174, 1: 218, 3: 49}, 0: {2: 139, 4: 32, 0: 338, 1: 218, 3: 49}, 1: {2: 80, 1: 407, 3: 152, 4: 12, 0: 101}, 4: {4: 42, 2: 46, 1: 101, 3: 27, 0: 185}, 3: {1: 233, 2: 23, 3: 182, 4: 4, 0: 37}}
121
	Train Loss: 1.225 | Train Acc: 68.21%
	 Val. Loss: 1.534 |  Val. Acc: 36.16% 

122
	Train Loss: 1.224 | Train Acc: 68.35%
	 Val. Loss: 1.533 |  Val. Acc: 36.22% 

123
	Train Loss: 1.222 | Train Acc: 67.89%
	 Val. Loss: 1.537 |  Val. Acc: 35.75% 

124
	Train Loss: 1.216 | Train Acc: 68.66%
	 Val. Loss: 1.534 |  Val. Acc: 36.45% 

125
	Train Loss: 1.221 | Train Acc: 68.39%
	 Val. Loss: 1.536 |  Val. Acc: 36.02% 

126
	Train Loss: 1.216 | Train Acc: 68.72%
	 Val. Loss: 1.533 |  Val. Acc: 36.15% 

127
	Train Loss: 1.216 | Train Acc: 68.88%
	 Val. Loss: 1.529 |  Val. Acc: 36.76% 

128
	Train Loss: 1.212 | Train Acc: 69.48%
	 Val. Loss: 1.537 |  Val. Acc: 35.82% 

129
	Train Loss: 1.211 | Train Acc: 69.66%
	 Val. Loss: 1.532 |  Val. Acc: 36.38% 

130
	Train Loss: 1.213 | Train Acc: 69.12%
	 Val. Loss: 1.532 |  Val. Acc: 36.42% 

131
	Train Loss: 1.211 | Train Acc: 69.40%
	 Val. Loss: 1.533 |  Val. Acc: 35.95% 

132
	Train Loss: 1.210 | Train Acc: 69.34%
	 Val. Loss: 1.537 |  Val. Acc: 35.41% 

133
	Train Loss: 1.203 | Train Acc: 70.23%
	 Val. Loss: 1.533 |  Val. Acc: 36.18% 

134
	Train Loss: 1.209 | Train Acc: 69.52%
	 Val. Loss: 1.530 |  Val. Acc: 36.18% 

135
	Train Loss: 1.205 | Train Acc: 70.30%
	 Val. Loss: 1.534 |  Val. Acc: 36.01% 

136
	Train Loss: 1.206 | Train Acc: 69.84%
	 Val. Loss: 1.531 |  Val. Acc: 36.61% 

137
	Train Loss: 1.207 | Train Acc: 69.65%
	 Val. Loss: 1.528 |  Val. Acc: 36.83% 

138
	Train Loss: 1.204 | Train Acc: 69.64%
	 Val. Loss: 1.532 |  Val. Acc: 36.42% 

139
	Train Loss: 1.207 | Train Acc: 69.65%
	 Val. Loss: 1.530 |  Val. Acc: 36.48% 

140
	Train Loss: 1.203 | Train Acc: 69.85%
	 Val. Loss: 1.529 |  Val. Acc: 36.50% 

label distribution: actual vs pred: {2: {2: 111, 4: 23, 0: 177, 1: 199, 3: 46}, 0: {4: 34, 1: 196, 0: 348, 2: 159, 3: 39}, 1: {2: 96, 1: 406, 3: 137, 4: 8, 0: 105}, 4: {4: 41, 2: 55, 1: 94, 3: 20, 0: 191}, 3: {1: 235, 2: 30, 3: 172, 4: 6, 0: 36}}
	 pred vs actuals: {2: {2: 111, 4: 23, 0: 177, 1: 199, 3: 46}, 0: {4: 34, 1: 196, 0: 348, 2: 159, 3: 39}, 1: {2: 96, 1: 406, 3: 137, 4: 8, 0: 105}, 4: {4: 41, 2: 55, 1: 94, 3: 20, 0: 191}, 3: {1: 235, 2: 30, 3: 172, 4: 6, 0: 36}}
141
	Train Loss: 1.193 | Train Acc: 71.06%
	 Val. Loss: 1.530 |  Val. Acc: 36.59% 

142
	Train Loss: 1.197 | Train Acc: 70.46%
	 Val. Loss: 1.534 |  Val. Acc: 35.95% 

143
	Train Loss: 1.198 | Train Acc: 70.38%
	 Val. Loss: 1.543 |  Val. Acc: 35.13% 

144
	Train Loss: 1.194 | Train Acc: 70.72%
	 Val. Loss: 1.538 |  Val. Acc: 35.53% 

145
	Train Loss: 1.194 | Train Acc: 71.45%
	 Val. Loss: 1.526 |  Val. Acc: 36.72% 

146
	Train Loss: 1.196 | Train Acc: 70.83%
	 Val. Loss: 1.533 |  Val. Acc: 35.98% 

147
	Train Loss: 1.194 | Train Acc: 71.04%
	 Val. Loss: 1.530 |  Val. Acc: 36.22% 

148
	Train Loss: 1.189 | Train Acc: 71.70%
	 Val. Loss: 1.537 |  Val. Acc: 35.46% 

149
	Train Loss: 1.193 | Train Acc: 70.93%
	 Val. Loss: 1.523 |  Val. Acc: 37.37% 

150
	Train Loss: 1.189 | Train Acc: 71.58%
	 Val. Loss: 1.529 |  Val. Acc: 36.45% 

151
	Train Loss: 1.185 | Train Acc: 72.05%
	 Val. Loss: 1.529 |  Val. Acc: 36.40% 

152
	Train Loss: 1.181 | Train Acc: 72.47%
	 Val. Loss: 1.530 |  Val. Acc: 36.40% 

153
	Train Loss: 1.188 | Train Acc: 71.47%
	 Val. Loss: 1.532 |  Val. Acc: 36.27% 

154
	Train Loss: 1.182 | Train Acc: 72.59%
	 Val. Loss: 1.532 |  Val. Acc: 36.51% 

155
	Train Loss: 1.177 | Train Acc: 72.60%
	 Val. Loss: 1.532 |  Val. Acc: 36.34% 

156
	Train Loss: 1.178 | Train Acc: 72.69%
	 Val. Loss: 1.530 |  Val. Acc: 36.16% 

157
	Train Loss: 1.176 | Train Acc: 72.81%
	 Val. Loss: 1.529 |  Val. Acc: 36.36% 

158
	Train Loss: 1.177 | Train Acc: 72.87%
	 Val. Loss: 1.532 |  Val. Acc: 36.10% 

159
	Train Loss: 1.176 | Train Acc: 72.36%
	 Val. Loss: 1.529 |  Val. Acc: 36.89% 

160
	Train Loss: 1.177 | Train Acc: 72.60%
	 Val. Loss: 1.536 |  Val. Acc: 35.62% 

label distribution: actual vs pred: {2: {2: 117, 4: 40, 0: 157, 1: 168, 3: 74}, 0: {2: 176, 1: 151, 0: 302, 3: 75, 4: 72}, 1: {2: 95, 1: 339, 3: 211, 4: 15, 0: 92}, 4: {4: 72, 2: 58, 1: 78, 3: 36, 0: 157}, 3: {1: 181, 2: 33, 3: 226, 4: 9, 0: 30}}
	 pred vs actuals: {2: {2: 117, 4: 40, 0: 157, 1: 168, 3: 74}, 0: {2: 176, 1: 151, 0: 302, 3: 75, 4: 72}, 1: {2: 95, 1: 339, 3: 211, 4: 15, 0: 92}, 4: {4: 72, 2: 58, 1: 78, 3: 36, 0: 157}, 3: {1: 181, 2: 33, 3: 226, 4: 9, 0: 30}}
161
	Train Loss: 1.173 | Train Acc: 72.99%
	 Val. Loss: 1.530 |  Val. Acc: 36.77% 

162
	Train Loss: 1.179 | Train Acc: 72.46%
	 Val. Loss: 1.530 |  Val. Acc: 36.36% 

163
	Train Loss: 1.172 | Train Acc: 73.07%
	 Val. Loss: 1.529 |  Val. Acc: 36.66% 

164
	Train Loss: 1.176 | Train Acc: 73.10%
	 Val. Loss: 1.535 |  Val. Acc: 35.82% 

165
	Train Loss: 1.169 | Train Acc: 73.23%
	 Val. Loss: 1.524 |  Val. Acc: 37.27% 

166
	Train Loss: 1.172 | Train Acc: 72.94%
	 Val. Loss: 1.528 |  Val. Acc: 36.66% 

167
	Train Loss: 1.172 | Train Acc: 73.43%
	 Val. Loss: 1.528 |  Val. Acc: 36.70% 

168
	Train Loss: 1.173 | Train Acc: 72.67%
	 Val. Loss: 1.521 |  Val. Acc: 37.83% 

169
	Train Loss: 1.169 | Train Acc: 73.44%
	 Val. Loss: 1.530 |  Val. Acc: 36.37% 

170
	Train Loss: 1.164 | Train Acc: 74.07%
	 Val. Loss: 1.522 |  Val. Acc: 37.37% 

171
	Train Loss: 1.171 | Train Acc: 73.09%
	 Val. Loss: 1.525 |  Val. Acc: 36.94% 

172
	Train Loss: 1.165 | Train Acc: 73.66%
	 Val. Loss: 1.526 |  Val. Acc: 36.86% 

173
	Train Loss: 1.165 | Train Acc: 73.56%
	 Val. Loss: 1.527 |  Val. Acc: 36.70% 

174
	Train Loss: 1.160 | Train Acc: 74.49%
	 Val. Loss: 1.524 |  Val. Acc: 37.36% 

175
	Train Loss: 1.162 | Train Acc: 73.89%
	 Val. Loss: 1.523 |  Val. Acc: 37.37% 

176
	Train Loss: 1.163 | Train Acc: 73.81%
	 Val. Loss: 1.523 |  Val. Acc: 37.63% 

177
	Train Loss: 1.163 | Train Acc: 73.90%
	 Val. Loss: 1.519 |  Val. Acc: 37.67% 

178
	Train Loss: 1.159 | Train Acc: 74.38%
	 Val. Loss: 1.525 |  Val. Acc: 37.20% 

179
	Train Loss: 1.157 | Train Acc: 74.39%
	 Val. Loss: 1.525 |  Val. Acc: 37.07% 

180
	Train Loss: 1.160 | Train Acc: 74.40%
	 Val. Loss: 1.523 |  Val. Acc: 37.65% 

label distribution: actual vs pred: {2: {2: 128, 4: 26, 0: 191, 1: 164, 3: 47}, 0: {0: 378, 1: 157, 2: 170, 4: 39, 3: 32}, 1: {2: 110, 1: 378, 3: 135, 4: 8, 0: 121}, 4: {4: 45, 2: 58, 1: 73, 3: 15, 0: 210}, 3: {1: 215, 2: 39, 3: 177, 4: 11, 0: 37}}
	 pred vs actuals: {2: {2: 128, 4: 26, 0: 191, 1: 164, 3: 47}, 0: {0: 378, 1: 157, 2: 170, 4: 39, 3: 32}, 1: {2: 110, 1: 378, 3: 135, 4: 8, 0: 121}, 4: {4: 45, 2: 58, 1: 73, 3: 15, 0: 210}, 3: {1: 215, 2: 39, 3: 177, 4: 11, 0: 37}}
181
	Train Loss: 1.156 | Train Acc: 74.76%
	 Val. Loss: 1.522 |  Val. Acc: 37.51% 

182
	Train Loss: 1.161 | Train Acc: 74.06%
	 Val. Loss: 1.525 |  Val. Acc: 36.97% 

183
	Train Loss: 1.158 | Train Acc: 74.21%
	 Val. Loss: 1.530 |  Val. Acc: 36.60% 

184
	Train Loss: 1.158 | Train Acc: 74.53%
	 Val. Loss: 1.526 |  Val. Acc: 37.06% 

185
	Train Loss: 1.159 | Train Acc: 74.69%
	 Val. Loss: 1.525 |  Val. Acc: 37.15% 

186
	Train Loss: 1.154 | Train Acc: 74.79%
	 Val. Loss: 1.526 |  Val. Acc: 36.88% 

187
	Train Loss: 1.154 | Train Acc: 75.16%
	 Val. Loss: 1.525 |  Val. Acc: 37.10% 

188
	Train Loss: 1.153 | Train Acc: 74.94%
	 Val. Loss: 1.522 |  Val. Acc: 37.31% 

189
	Train Loss: 1.153 | Train Acc: 74.98%
	 Val. Loss: 1.529 |  Val. Acc: 36.54% 

190
	Train Loss: 1.150 | Train Acc: 75.29%
	 Val. Loss: 1.523 |  Val. Acc: 37.16% 

191
	Train Loss: 1.149 | Train Acc: 75.41%
	 Val. Loss: 1.532 |  Val. Acc: 36.44% 

192
	Train Loss: 1.149 | Train Acc: 75.45%
	 Val. Loss: 1.532 |  Val. Acc: 36.31% 

193
	Train Loss: 1.151 | Train Acc: 75.43%
	 Val. Loss: 1.529 |  Val. Acc: 36.53% 

194
	Train Loss: 1.148 | Train Acc: 75.48%
	 Val. Loss: 1.531 |  Val. Acc: 36.44% 

195
	Train Loss: 1.149 | Train Acc: 75.23%
	 Val. Loss: 1.521 |  Val. Acc: 37.57% 

196
	Train Loss: 1.151 | Train Acc: 75.13%
	 Val. Loss: 1.522 |  Val. Acc: 37.46% 

197
	Train Loss: 1.146 | Train Acc: 75.84%
	 Val. Loss: 1.522 |  Val. Acc: 37.43% 

198
	Train Loss: 1.149 | Train Acc: 75.45%
	 Val. Loss: 1.522 |  Val. Acc: 37.43% 

199
	Train Loss: 1.145 | Train Acc: 75.48%
	 Val. Loss: 1.524 |  Val. Acc: 37.12% 

200
	Train Loss: 1.144 | Train Acc: 75.84%
	 Val. Loss: 1.524 |  Val. Acc: 37.33% 

label distribution: actual vs pred: {2: {2: 99, 4: 40, 1: 198, 0: 167, 3: 52}, 0: {4: 63, 1: 186, 0: 348, 2: 130, 3: 49}, 1: {2: 84, 1: 402, 3: 157, 0: 98, 4: 11}, 4: {4: 69, 1: 100, 3: 21, 2: 38, 0: 173}, 3: {1: 228, 2: 28, 3: 183, 4: 9, 0: 31}}
	 pred vs actuals: {2: {2: 99, 4: 40, 1: 198, 0: 167, 3: 52}, 0: {4: 63, 1: 186, 0: 348, 2: 130, 3: 49}, 1: {2: 84, 1: 402, 3: 157, 0: 98, 4: 11}, 4: {4: 69, 1: 100, 3: 21, 2: 38, 0: 173}, 3: {1: 228, 2: 28, 3: 183, 4: 9, 0: 31}}









size_of_vocab = len(Review.vocab)
embedding_dim = 350
num_hidden_nodes = 150
num_output_nodes = 5
num_layers = 2
dropout = 0.4
lr = 1e-4







     1
	Train Loss: 1.607 | Train Acc: 22.41%
	 Val. Loss: 1.599 |  Val. Acc: 26.15% 

2
	Train Loss: 1.604 | Train Acc: 23.65%
	 Val. Loss: 1.595 |  Val. Acc: 26.08% 

3
	Train Loss: 1.603 | Train Acc: 23.50%
	 Val. Loss: 1.593 |  Val. Acc: 25.95% 

4
	Train Loss: 1.602 | Train Acc: 23.61%
	 Val. Loss: 1.592 |  Val. Acc: 27.45% 

5
	Train Loss: 1.602 | Train Acc: 23.70%
	 Val. Loss: 1.591 |  Val. Acc: 26.99% 

6
	Train Loss: 1.602 | Train Acc: 23.48%
	 Val. Loss: 1.590 |  Val. Acc: 27.62% 

7
	Train Loss: 1.603 | Train Acc: 23.72%
	 Val. Loss: 1.590 |  Val. Acc: 27.78% 

8
	Train Loss: 1.602 | Train Acc: 23.79%
	 Val. Loss: 1.590 |  Val. Acc: 28.05% 

9
	Train Loss: 1.600 | Train Acc: 24.45%
	 Val. Loss: 1.589 |  Val. Acc: 26.85% 

10
	Train Loss: 1.600 | Train Acc: 24.19%
	 Val. Loss: 1.588 |  Val. Acc: 27.83% 

11
	Train Loss: 1.600 | Train Acc: 24.14%
	 Val. Loss: 1.588 |  Val. Acc: 28.21% 

12
	Train Loss: 1.600 | Train Acc: 24.40%
	 Val. Loss: 1.588 |  Val. Acc: 27.93% 

13
	Train Loss: 1.600 | Train Acc: 24.40%
	 Val. Loss: 1.588 |  Val. Acc: 27.97% 

14
	Train Loss: 1.599 | Train Acc: 24.66%
	 Val. Loss: 1.587 |  Val. Acc: 28.62% 

15
	Train Loss: 1.597 | Train Acc: 25.05%
	 Val. Loss: 1.586 |  Val. Acc: 28.23% 

16
	Train Loss: 1.597 | Train Acc: 25.17%
	 Val. Loss: 1.586 |  Val. Acc: 28.13% 

17
	Train Loss: 1.596 | Train Acc: 25.05%
	 Val. Loss: 1.586 |  Val. Acc: 28.62% 

18
	Train Loss: 1.596 | Train Acc: 25.21%
	 Val. Loss: 1.585 |  Val. Acc: 29.12% 

19
	Train Loss: 1.594 | Train Acc: 25.25%
	 Val. Loss: 1.585 |  Val. Acc: 28.56% 

20
	Train Loss: 1.594 | Train Acc: 25.72%
	 Val. Loss: 1.585 |  Val. Acc: 28.59% 

label distribution: actual vs pred: {2: {0: 162, 1: 354, 2: 40}, 0: {2: 48, 0: 304, 1: 424}, 1: {1: 505, 2: 33, 0: 214}, 4: {0: 144, 1: 241, 2: 16}, 3: {1: 336, 0: 126, 2: 17}}
	 pred vs actuals: {2: {0: 162, 1: 354, 2: 40}, 0: {2: 48, 0: 304, 1: 424}, 1: {1: 505, 2: 33, 0: 214}, 4: {0: 144, 1: 241, 2: 16}, 3: {1: 336, 0: 126, 2: 17}}
21
	Train Loss: 1.592 | Train Acc: 26.50%
	 Val. Loss: 1.583 |  Val. Acc: 29.19% 

22
	Train Loss: 1.591 | Train Acc: 26.30%
	 Val. Loss: 1.582 |  Val. Acc: 29.16% 

23
	Train Loss: 1.587 | Train Acc: 26.66%
	 Val. Loss: 1.581 |  Val. Acc: 29.36% 

24
	Train Loss: 1.587 | Train Acc: 27.09%
	 Val. Loss: 1.581 |  Val. Acc: 29.39% 

25
	Train Loss: 1.587 | Train Acc: 27.01%
	 Val. Loss: 1.579 |  Val. Acc: 29.99% 

26
	Train Loss: 1.585 | Train Acc: 27.20%
	 Val. Loss: 1.579 |  Val. Acc: 29.89% 

27
	Train Loss: 1.583 | Train Acc: 28.26%
	 Val. Loss: 1.579 |  Val. Acc: 29.46% 

28
	Train Loss: 1.580 | Train Acc: 28.86%
	 Val. Loss: 1.578 |  Val. Acc: 29.59% 

29
	Train Loss: 1.581 | Train Acc: 28.40%
	 Val. Loss: 1.576 |  Val. Acc: 29.93% 

30
	Train Loss: 1.575 | Train Acc: 29.61%
	 Val. Loss: 1.574 |  Val. Acc: 30.06% 

31
	Train Loss: 1.576 | Train Acc: 28.92%
	 Val. Loss: 1.573 |  Val. Acc: 30.23% 

32
	Train Loss: 1.574 | Train Acc: 29.28%
	 Val. Loss: 1.573 |  Val. Acc: 30.40% 

33
	Train Loss: 1.570 | Train Acc: 30.02%
	 Val. Loss: 1.570 |  Val. Acc: 30.90% 

34
	Train Loss: 1.566 | Train Acc: 30.80%
	 Val. Loss: 1.569 |  Val. Acc: 31.07% 

35
	Train Loss: 1.567 | Train Acc: 30.48%
	 Val. Loss: 1.569 |  Val. Acc: 30.70% 

36
	Train Loss: 1.563 | Train Acc: 31.16%
	 Val. Loss: 1.568 |  Val. Acc: 30.87% 

37
	Train Loss: 1.560 | Train Acc: 31.30%
	 Val. Loss: 1.567 |  Val. Acc: 30.74% 

38
	Train Loss: 1.558 | Train Acc: 31.80%
	 Val. Loss: 1.567 |  Val. Acc: 30.70% 

39
	Train Loss: 1.559 | Train Acc: 31.32%
	 Val. Loss: 1.566 |  Val. Acc: 30.87% 

40
	Train Loss: 1.556 | Train Acc: 32.17%
	 Val. Loss: 1.566 |  Val. Acc: 31.01% 

label distribution: actual vs pred: {2: {0: 182, 2: 46, 1: 286, 3: 42}, 0: {2: 48, 0: 313, 1: 353, 3: 62}, 1: {0: 151, 1: 479, 2: 28, 3: 93, 4: 1}, 4: {0: 145, 1: 200, 3: 37, 2: 19}, 3: {1: 313, 0: 75, 3: 75, 2: 16}}
	 pred vs actuals: {2: {0: 182, 2: 46, 1: 286, 3: 42}, 0: {2: 48, 0: 313, 1: 353, 3: 62}, 1: {0: 151, 1: 479, 2: 28, 3: 93, 4: 1}, 4: {0: 145, 1: 200, 3: 37, 2: 19}, 3: {1: 313, 0: 75, 3: 75, 2: 16}}
41
	Train Loss: 1.550 | Train Acc: 32.98%
	 Val. Loss: 1.566 |  Val. Acc: 30.23% 

42
	Train Loss: 1.550 | Train Acc: 32.72%
	 Val. Loss: 1.566 |  Val. Acc: 30.57% 

43
	Train Loss: 1.551 | Train Acc: 32.15%
	 Val. Loss: 1.566 |  Val. Acc: 30.16% 

44
	Train Loss: 1.548 | Train Acc: 33.28%
	 Val. Loss: 1.563 |  Val. Acc: 31.03% 

45
	Train Loss: 1.547 | Train Acc: 33.00%
	 Val. Loss: 1.563 |  Val. Acc: 31.20% 

46
	Train Loss: 1.542 | Train Acc: 33.43%
	 Val. Loss: 1.564 |  Val. Acc: 30.59% 

47
	Train Loss: 1.539 | Train Acc: 34.42%
	 Val. Loss: 1.562 |  Val. Acc: 31.56% 

48
	Train Loss: 1.537 | Train Acc: 34.45%
	 Val. Loss: 1.561 |  Val. Acc: 31.16% 

49
	Train Loss: 1.532 | Train Acc: 34.87%
	 Val. Loss: 1.561 |  Val. Acc: 30.89% 

50
	Train Loss: 1.532 | Train Acc: 34.99%
	 Val. Loss: 1.559 |  Val. Acc: 31.05% 

51
	Train Loss: 1.530 | Train Acc: 35.11%
	 Val. Loss: 1.557 |  Val. Acc: 31.52% 

52
	Train Loss: 1.527 | Train Acc: 35.60%
	 Val. Loss: 1.557 |  Val. Acc: 31.59% 

53
	Train Loss: 1.528 | Train Acc: 35.36%
	 Val. Loss: 1.557 |  Val. Acc: 31.62% 

54
	Train Loss: 1.526 | Train Acc: 35.72%
	 Val. Loss: 1.555 |  Val. Acc: 31.82% 

55
	Train Loss: 1.527 | Train Acc: 35.87%
	 Val. Loss: 1.559 |  Val. Acc: 31.12% 

56
	Train Loss: 1.523 | Train Acc: 36.14%
	 Val. Loss: 1.557 |  Val. Acc: 31.58% 

57
	Train Loss: 1.523 | Train Acc: 36.38%
	 Val. Loss: 1.557 |  Val. Acc: 31.58% 

58
	Train Loss: 1.518 | Train Acc: 36.95%
	 Val. Loss: 1.553 |  Val. Acc: 32.15% 

59
	Train Loss: 1.517 | Train Acc: 36.87%
	 Val. Loss: 1.556 |  Val. Acc: 31.45% 

60
	Train Loss: 1.516 | Train Acc: 36.94%
	 Val. Loss: 1.552 |  Val. Acc: 31.92% 

label distribution: actual vs pred: {2: {2: 49, 0: 204, 3: 48, 1: 251, 4: 4}, 0: {0: 344, 3: 75, 2: 56, 1: 299, 4: 2}, 1: {2: 45, 1: 444, 0: 146, 3: 115, 4: 2}, 4: {1: 158, 3: 36, 2: 28, 0: 177, 4: 2}, 3: {1: 288, 0: 66, 3: 108, 2: 15, 4: 2}}
	 pred vs actuals: {2: {2: 49, 0: 204, 3: 48, 1: 251, 4: 4}, 0: {0: 344, 3: 75, 2: 56, 1: 299, 4: 2}, 1: {2: 45, 1: 444, 0: 146, 3: 115, 4: 2}, 4: {1: 158, 3: 36, 2: 28, 0: 177, 4: 2}, 3: {1: 288, 0: 66, 3: 108, 2: 15, 4: 2}}
61
	Train Loss: 1.513 | Train Acc: 37.38%
	 Val. Loss: 1.553 |  Val. Acc: 31.75% 

62
	Train Loss: 1.514 | Train Acc: 37.94%
	 Val. Loss: 1.553 |  Val. Acc: 31.59% 

63
	Train Loss: 1.510 | Train Acc: 38.25%
	 Val. Loss: 1.552 |  Val. Acc: 32.09% 

64
	Train Loss: 1.512 | Train Acc: 37.21%
	 Val. Loss: 1.554 |  Val. Acc: 31.83% 

65
	Train Loss: 1.510 | Train Acc: 37.93%
	 Val. Loss: 1.553 |  Val. Acc: 31.83% 

66
	Train Loss: 1.504 | Train Acc: 38.26%
	 Val. Loss: 1.552 |  Val. Acc: 32.23% 

67
	Train Loss: 1.501 | Train Acc: 38.69%
	 Val. Loss: 1.550 |  Val. Acc: 32.26% 

68
	Train Loss: 1.506 | Train Acc: 38.34%
	 Val. Loss: 1.553 |  Val. Acc: 31.93% 

69
	Train Loss: 1.502 | Train Acc: 39.13%
	 Val. Loss: 1.549 |  Val. Acc: 32.63% 

70
	Train Loss: 1.496 | Train Acc: 39.39%
	 Val. Loss: 1.550 |  Val. Acc: 32.59% 

71
	Train Loss: 1.499 | Train Acc: 39.31%
	 Val. Loss: 1.551 |  Val. Acc: 31.99% 

72
	Train Loss: 1.495 | Train Acc: 39.41%
	 Val. Loss: 1.548 |  Val. Acc: 32.63% 

73
	Train Loss: 1.500 | Train Acc: 39.68%
	 Val. Loss: 1.545 |  Val. Acc: 33.36% 

74
	Train Loss: 1.494 | Train Acc: 39.51%
	 Val. Loss: 1.546 |  Val. Acc: 33.39% 

75
	Train Loss: 1.488 | Train Acc: 40.33%
	 Val. Loss: 1.545 |  Val. Acc: 33.29% 

76
	Train Loss: 1.492 | Train Acc: 40.01%
	 Val. Loss: 1.544 |  Val. Acc: 33.46% 

77
	Train Loss: 1.492 | Train Acc: 39.93%
	 Val. Loss: 1.541 |  Val. Acc: 34.57% 

78
	Train Loss: 1.489 | Train Acc: 39.95%
	 Val. Loss: 1.540 |  Val. Acc: 34.87% 

79
	Train Loss: 1.487 | Train Acc: 40.50%
	 Val. Loss: 1.540 |  Val. Acc: 34.37% 

80
	Train Loss: 1.488 | Train Acc: 39.94%
	 Val. Loss: 1.541 |  Val. Acc: 34.33% 

label distribution: actual vs pred: {2: {2: 45, 0: 217, 3: 56, 1: 231, 4: 7}, 0: {0: 377, 3: 80, 2: 51, 1: 263, 4: 5}, 1: {2: 47, 1: 436, 0: 129, 3: 133, 4: 7}, 4: {1: 121, 0: 207, 3: 42, 2: 24, 4: 7}, 3: {1: 248, 3: 150, 0: 68, 2: 10, 4: 3}}
	 pred vs actuals: {2: {2: 45, 0: 217, 3: 56, 1: 231, 4: 7}, 0: {0: 377, 3: 80, 2: 51, 1: 263, 4: 5}, 1: {2: 47, 1: 436, 0: 129, 3: 133, 4: 7}, 4: {1: 121, 0: 207, 3: 42, 2: 24, 4: 7}, 3: {1: 248, 3: 150, 0: 68, 2: 10, 4: 3}}
81
	Train Loss: 1.490 | Train Acc: 39.84%
	 Val. Loss: 1.538 |  Val. Acc: 34.63% 

82
	Train Loss: 1.488 | Train Acc: 40.24%
	 Val. Loss: 1.540 |  Val. Acc: 34.39% 

83
	Train Loss: 1.483 | Train Acc: 40.61%
	 Val. Loss: 1.539 |  Val. Acc: 34.66% 

84
	Train Loss: 1.488 | Train Acc: 40.47%
	 Val. Loss: 1.539 |  Val. Acc: 34.53% 

85
	Train Loss: 1.479 | Train Acc: 42.23%
	 Val. Loss: 1.537 |  Val. Acc: 34.49% 

86
	Train Loss: 1.480 | Train Acc: 41.91%
	 Val. Loss: 1.539 |  Val. Acc: 34.43% 

87
	Train Loss: 1.481 | Train Acc: 41.26%
	 Val. Loss: 1.540 |  Val. Acc: 34.23% 

88
	Train Loss: 1.477 | Train Acc: 41.87%
	 Val. Loss: 1.540 |  Val. Acc: 33.96% 

89
	Train Loss: 1.472 | Train Acc: 42.48%
	 Val. Loss: 1.536 |  Val. Acc: 34.76% 

90
	Train Loss: 1.469 | Train Acc: 42.60%
	 Val. Loss: 1.537 |  Val. Acc: 34.36% 

91
	Train Loss: 1.469 | Train Acc: 43.05%
	 Val. Loss: 1.537 |  Val. Acc: 34.49% 

92
	Train Loss: 1.470 | Train Acc: 42.78%
	 Val. Loss: 1.536 |  Val. Acc: 34.42% 

93
	Train Loss: 1.473 | Train Acc: 42.40%
	 Val. Loss: 1.540 |  Val. Acc: 34.25% 

94
	Train Loss: 1.472 | Train Acc: 42.12%
	 Val. Loss: 1.535 |  Val. Acc: 34.67% 

95
	Train Loss: 1.463 | Train Acc: 43.03%
	 Val. Loss: 1.537 |  Val. Acc: 34.45% 

96
	Train Loss: 1.464 | Train Acc: 43.16%
	 Val. Loss: 1.536 |  Val. Acc: 34.79% 

97
	Train Loss: 1.462 | Train Acc: 43.30%
	 Val. Loss: 1.535 |  Val. Acc: 34.92% 

98
	Train Loss: 1.463 | Train Acc: 43.67%
	 Val. Loss: 1.537 |  Val. Acc: 34.75% 

99
	Train Loss: 1.461 | Train Acc: 43.54%
	 Val. Loss: 1.538 |  Val. Acc: 34.62% 

100
	Train Loss: 1.458 | Train Acc: 43.69%
	 Val. Loss: 1.539 |  Val. Acc: 34.32% 

label distribution: actual vs pred: {2: {2: 47, 0: 214, 3: 58, 1: 230, 4: 7}, 0: {0: 382, 3: 82, 2: 56, 4: 11, 1: 245}, 1: {2: 40, 1: 416, 0: 129, 3: 158, 4: 9}, 4: {1: 116, 3: 45, 2: 28, 0: 205, 4: 7}, 3: {1: 243, 3: 165, 0: 56, 2: 12, 4: 3}}
	 pred vs actuals: {2: {2: 47, 0: 214, 3: 58, 1: 230, 4: 7}, 0: {0: 382, 3: 82, 2: 56, 4: 11, 1: 245}, 1: {2: 40, 1: 416, 0: 129, 3: 158, 4: 9}, 4: {1: 116, 3: 45, 2: 28, 0: 205, 4: 7}, 3: {1: 243, 3: 165, 0: 56, 2: 12, 4: 3}}
101
	Train Loss: 1.456 | Train Acc: 44.00%
	 Val. Loss: 1.537 |  Val. Acc: 34.43% 

102
	Train Loss: 1.458 | Train Acc: 43.64%
	 Val. Loss: 1.536 |  Val. Acc: 34.74% 

103
	Train Loss: 1.459 | Train Acc: 43.80%
	 Val. Loss: 1.538 |  Val. Acc: 34.55% 

104
	Train Loss: 1.450 | Train Acc: 44.89%
	 Val. Loss: 1.537 |  Val. Acc: 34.40% 

105
	Train Loss: 1.455 | Train Acc: 44.37%
	 Val. Loss: 1.539 |  Val. Acc: 34.31% 

106
	Train Loss: 1.456 | Train Acc: 44.36%
	 Val. Loss: 1.541 |  Val. Acc: 34.10% 

107
	Train Loss: 1.452 | Train Acc: 44.08%
	 Val. Loss: 1.538 |  Val. Acc: 34.71% 

108
	Train Loss: 1.448 | Train Acc: 44.71%
	 Val. Loss: 1.537 |  Val. Acc: 34.43% 

109
	Train Loss: 1.447 | Train Acc: 44.88%
	 Val. Loss: 1.538 |  Val. Acc: 34.33% 

110
	Train Loss: 1.447 | Train Acc: 45.13%
	 Val. Loss: 1.539 |  Val. Acc: 34.36% 

111
	Train Loss: 1.445 | Train Acc: 45.08%
	 Val. Loss: 1.540 |  Val. Acc: 33.94% 

112
	Train Loss: 1.446 | Train Acc: 45.12%
	 Val. Loss: 1.538 |  Val. Acc: 34.24% 

113
	Train Loss: 1.448 | Train Acc: 44.92%
	 Val. Loss: 1.536 |  Val. Acc: 34.53% 

114
	Train Loss: 1.440 | Train Acc: 45.73%
	 Val. Loss: 1.536 |  Val. Acc: 34.60% 

115
	Train Loss: 1.446 | Train Acc: 45.56%
	 Val. Loss: 1.534 |  Val. Acc: 34.87% 

116
	Train Loss: 1.441 | Train Acc: 45.80%
	 Val. Loss: 1.535 |  Val. Acc: 34.77% 

117
	Train Loss: 1.447 | Train Acc: 45.14%
	 Val. Loss: 1.535 |  Val. Acc: 34.42% 

118
	Train Loss: 1.446 | Train Acc: 45.13%
	 Val. Loss: 1.534 |  Val. Acc: 35.04% 

119
	Train Loss: 1.434 | Train Acc: 46.73%
	 Val. Loss: 1.536 |  Val. Acc: 34.43% 

120
	Train Loss: 1.435 | Train Acc: 46.24%
	 Val. Loss: 1.535 |  Val. Acc: 34.73% 

label distribution: actual vs pred: {2: {2: 48, 0: 222, 3: 64, 1: 209, 4: 13}, 0: {0: 403, 3: 79, 2: 77, 4: 12, 1: 205}, 1: {2: 54, 1: 391, 0: 132, 3: 163, 4: 12}, 4: {1: 101, 0: 208, 3: 45, 2: 35, 4: 12}, 3: {1: 229, 3: 173, 0: 56, 2: 17, 4: 4}}
	 pred vs actuals: {2: {2: 48, 0: 222, 3: 64, 1: 209, 4: 13}, 0: {0: 403, 3: 79, 2: 77, 4: 12, 1: 205}, 1: {2: 54, 1: 391, 0: 132, 3: 163, 4: 12}, 4: {1: 101, 0: 208, 3: 45, 2: 35, 4: 12}, 3: {1: 229, 3: 173, 0: 56, 2: 17, 4: 4}}
121
	Train Loss: 1.435 | Train Acc: 46.59%
	 Val. Loss: 1.536 |  Val. Acc: 34.89% 

122
	Train Loss: 1.433 | Train Acc: 46.82%
	 Val. Loss: 1.536 |  Val. Acc: 34.49% 

123
	Train Loss: 1.429 | Train Acc: 47.22%
	 Val. Loss: 1.535 |  Val. Acc: 35.09% 

124
	Train Loss: 1.435 | Train Acc: 46.71%
	 Val. Loss: 1.533 |  Val. Acc: 34.96% 

125
	Train Loss: 1.430 | Train Acc: 47.17%
	 Val. Loss: 1.533 |  Val. Acc: 35.23% 

126
	Train Loss: 1.438 | Train Acc: 45.55%
	 Val. Loss: 1.534 |  Val. Acc: 34.83% 

127
	Train Loss: 1.429 | Train Acc: 47.36%
	 Val. Loss: 1.535 |  Val. Acc: 34.73% 

128
	Train Loss: 1.430 | Train Acc: 46.65%
	 Val. Loss: 1.533 |  Val. Acc: 35.17% 

129
	Train Loss: 1.426 | Train Acc: 47.56%
	 Val. Loss: 1.536 |  Val. Acc: 35.00% 

130
	Train Loss: 1.428 | Train Acc: 47.53%
	 Val. Loss: 1.536 |  Val. Acc: 34.91% 

131
	Train Loss: 1.424 | Train Acc: 47.67%
	 Val. Loss: 1.532 |  Val. Acc: 35.14% 

132
	Train Loss: 1.429 | Train Acc: 46.84%
	 Val. Loss: 1.533 |  Val. Acc: 34.89% 

133
	Train Loss: 1.426 | Train Acc: 47.36%
	 Val. Loss: 1.534 |  Val. Acc: 34.83% 

134
	Train Loss: 1.420 | Train Acc: 48.19%
	 Val. Loss: 1.533 |  Val. Acc: 34.89% 

135
	Train Loss: 1.426 | Train Acc: 47.24%
	 Val. Loss: 1.532 |  Val. Acc: 34.96% 

136
	Train Loss: 1.424 | Train Acc: 47.50%
	 Val. Loss: 1.532 |  Val. Acc: 35.13% 

137
	Train Loss: 1.423 | Train Acc: 47.59%
	 Val. Loss: 1.534 |  Val. Acc: 34.36% 

138
	Train Loss: 1.416 | Train Acc: 48.32%
	 Val. Loss: 1.533 |  Val. Acc: 34.49% 

139
	Train Loss: 1.424 | Train Acc: 47.43%
	 Val. Loss: 1.536 |  Val. Acc: 34.59% 

140
	Train Loss: 1.412 | Train Acc: 49.05%
	 Val. Loss: 1.534 |  Val. Acc: 34.76% 

label distribution: actual vs pred: {2: {2: 50, 0: 225, 3: 57, 1: 211, 4: 13}, 0: {0: 389, 3: 73, 2: 79, 4: 13, 1: 222}, 1: {2: 52, 1: 413, 0: 124, 3: 153, 4: 10}, 4: {1: 110, 3: 42, 2: 35, 0: 206, 4: 8}, 3: {1: 239, 3: 168, 0: 54, 2: 15, 4: 3}}
	 pred vs actuals: {2: {2: 50, 0: 225, 3: 57, 1: 211, 4: 13}, 0: {0: 389, 3: 73, 2: 79, 4: 13, 1: 222}, 1: {2: 52, 1: 413, 0: 124, 3: 153, 4: 10}, 4: {1: 110, 3: 42, 2: 35, 0: 206, 4: 8}, 3: {1: 239, 3: 168, 0: 54, 2: 15, 4: 3}}
141
	Train Loss: 1.418 | Train Acc: 48.04%
	 Val. Loss: 1.535 |  Val. Acc: 34.69% 

142
	Train Loss: 1.417 | Train Acc: 48.17%
	 Val. Loss: 1.534 |  Val. Acc: 34.53% 

143
	Train Loss: 1.414 | Train Acc: 48.59%
	 Val. Loss: 1.536 |  Val. Acc: 34.55% 

144
	Train Loss: 1.413 | Train Acc: 49.13%
	 Val. Loss: 1.535 |  Val. Acc: 34.49% 

145
	Train Loss: 1.414 | Train Acc: 48.33%
	 Val. Loss: 1.533 |  Val. Acc: 34.93% 

146
	Train Loss: 1.416 | Train Acc: 48.61%
	 Val. Loss: 1.531 |  Val. Acc: 35.23% 

147
	Train Loss: 1.412 | Train Acc: 49.03%
	 Val. Loss: 1.531 |  Val. Acc: 35.39% 

148
	Train Loss: 1.412 | Train Acc: 48.82%
	 Val. Loss: 1.533 |  Val. Acc: 34.76% 

149
	Train Loss: 1.403 | Train Acc: 49.70%
	 Val. Loss: 1.534 |  Val. Acc: 34.87% 

150
	Train Loss: 1.408 | Train Acc: 48.89%
	 Val. Loss: 1.532 |  Val. Acc: 35.16% 

151
	Train Loss: 1.407 | Train Acc: 49.08%
	 Val. Loss: 1.535 |  Val. Acc: 34.83% 

152
	Train Loss: 1.403 | Train Acc: 49.83%
	 Val. Loss: 1.533 |  Val. Acc: 35.13% 

153
	Train Loss: 1.406 | Train Acc: 49.35%
	 Val. Loss: 1.532 |  Val. Acc: 35.50% 

154
	Train Loss: 1.401 | Train Acc: 49.86%
	 Val. Loss: 1.533 |  Val. Acc: 35.30% 

155
	Train Loss: 1.405 | Train Acc: 49.09%
	 Val. Loss: 1.533 |  Val. Acc: 35.43% 

156
	Train Loss: 1.395 | Train Acc: 50.30%
	 Val. Loss: 1.534 |  Val. Acc: 35.30% 

157
	Train Loss: 1.403 | Train Acc: 49.65%
	 Val. Loss: 1.535 |  Val. Acc: 35.23% 

158
	Train Loss: 1.398 | Train Acc: 50.16%
	 Val. Loss: 1.533 |  Val. Acc: 35.40% 

159
	Train Loss: 1.398 | Train Acc: 50.09%
	 Val. Loss: 1.534 |  Val. Acc: 35.11% 

160
	Train Loss: 1.399 | Train Acc: 50.36%
	 Val. Loss: 1.533 |  Val. Acc: 35.33% 

label distribution: actual vs pred: {2: {2: 57, 0: 225, 3: 70, 1: 188, 4: 16}, 0: {0: 402, 3: 80, 2: 81, 4: 17, 1: 196}, 1: {2: 61, 1: 381, 0: 117, 3: 176, 4: 17}, 4: {1: 93, 3: 45, 4: 15, 0: 213, 2: 35}, 3: {1: 215, 3: 190, 0: 53, 2: 15, 4: 6}}
	 pred vs actuals: {2: {2: 57, 0: 225, 3: 70, 1: 188, 4: 16}, 0: {0: 402, 3: 80, 2: 81, 4: 17, 1: 196}, 1: {2: 61, 1: 381, 0: 117, 3: 176, 4: 17}, 4: {1: 93, 3: 45, 4: 15, 0: 213, 2: 35}, 3: {1: 215, 3: 190, 0: 53, 2: 15, 4: 6}}
161
	Train Loss: 1.394 | Train Acc: 50.80%
	 Val. Loss: 1.537 |  Val. Acc: 34.70% 

162
	Train Loss: 1.395 | Train Acc: 50.88%
	 Val. Loss: 1.533 |  Val. Acc: 35.21% 

163
	Train Loss: 1.397 | Train Acc: 50.53%
	 Val. Loss: 1.534 |  Val. Acc: 35.41% 

164
	Train Loss: 1.398 | Train Acc: 50.13%
	 Val. Loss: 1.536 |  Val. Acc: 35.03% 

165
	Train Loss: 1.396 | Train Acc: 50.59%
	 Val. Loss: 1.534 |  Val. Acc: 35.16% 

166
	Train Loss: 1.390 | Train Acc: 51.56%
	 Val. Loss: 1.535 |  Val. Acc: 35.09% 

167
	Train Loss: 1.394 | Train Acc: 50.97%
	 Val. Loss: 1.534 |  Val. Acc: 35.16% 

168
	Train Loss: 1.390 | Train Acc: 51.04%
	 Val. Loss: 1.537 |  Val. Acc: 34.85% 

169
	Train Loss: 1.391 | Train Acc: 51.21%
	 Val. Loss: 1.535 |  Val. Acc: 35.05% 

170
	Train Loss: 1.385 | Train Acc: 51.75%
	 Val. Loss: 1.537 |  Val. Acc: 34.89% 

171
	Train Loss: 1.391 | Train Acc: 51.00%
	 Val. Loss: 1.534 |  Val. Acc: 35.49% 

172
	Train Loss: 1.390 | Train Acc: 50.66%
	 Val. Loss: 1.534 |  Val. Acc: 35.49% 

173
	Train Loss: 1.387 | Train Acc: 51.34%
	 Val. Loss: 1.531 |  Val. Acc: 35.43% 

174
	Train Loss: 1.384 | Train Acc: 52.13%
	 Val. Loss: 1.533 |  Val. Acc: 35.09% 

175
	Train Loss: 1.386 | Train Acc: 51.26%
	 Val. Loss: 1.534 |  Val. Acc: 35.26% 

176
	Train Loss: 1.384 | Train Acc: 51.82%
	 Val. Loss: 1.534 |  Val. Acc: 35.53% 

177
	Train Loss: 1.385 | Train Acc: 51.92%
	 Val. Loss: 1.532 |  Val. Acc: 35.23% 

178
	Train Loss: 1.383 | Train Acc: 52.10%
	 Val. Loss: 1.535 |  Val. Acc: 35.33% 

179
	Train Loss: 1.375 | Train Acc: 52.58%
	 Val. Loss: 1.533 |  Val. Acc: 35.29% 

180
	Train Loss: 1.383 | Train Acc: 51.88%
	 Val. Loss: 1.534 |  Val. Acc: 35.26% 

label distribution: actual vs pred: {2: {2: 61, 0: 207, 3: 66, 1: 206, 4: 16}, 0: {0: 379, 3: 74, 2: 83, 4: 19, 1: 221}, 1: {2: 59, 1: 408, 0: 106, 3: 166, 4: 13}, 4: {1: 99, 3: 42, 4: 12, 0: 207, 2: 41}, 3: {1: 236, 3: 183, 0: 44, 2: 13, 4: 3}}
	 pred vs actuals: {2: {2: 61, 0: 207, 3: 66, 1: 206, 4: 16}, 0: {0: 379, 3: 74, 2: 83, 4: 19, 1: 221}, 1: {2: 59, 1: 408, 0: 106, 3: 166, 4: 13}, 4: {1: 99, 3: 42, 4: 12, 0: 207, 2: 41}, 3: {1: 236, 3: 183, 0: 44, 2: 13, 4: 3}}
181
	Train Loss: 1.381 | Train Acc: 51.98%
	 Val. Loss: 1.532 |  Val. Acc: 35.39% 

182
	Train Loss: 1.381 | Train Acc: 51.75%
	 Val. Loss: 1.532 |  Val. Acc: 35.39% 

183
	Train Loss: 1.380 | Train Acc: 52.32%
	 Val. Loss: 1.532 |  Val. Acc: 35.23% 

184
	Train Loss: 1.376 | Train Acc: 52.62%
	 Val. Loss: 1.536 |  Val. Acc: 35.10% 

185
	Train Loss: 1.377 | Train Acc: 52.18%
	 Val. Loss: 1.534 |  Val. Acc: 35.19% 

186
	Train Loss: 1.380 | Train Acc: 51.92%
	 Val. Loss: 1.535 |  Val. Acc: 34.89% 

187
	Train Loss: 1.368 | Train Acc: 53.71%
	 Val. Loss: 1.531 |  Val. Acc: 35.63% 

188
	Train Loss: 1.371 | Train Acc: 52.69%
	 Val. Loss: 1.530 |  Val. Acc: 35.86% 

189
	Train Loss: 1.372 | Train Acc: 53.28%
	 Val. Loss: 1.535 |  Val. Acc: 35.02% 

190
	Train Loss: 1.373 | Train Acc: 52.88%
	 Val. Loss: 1.533 |  Val. Acc: 35.59% 

191
	Train Loss: 1.375 | Train Acc: 52.14%
	 Val. Loss: 1.533 |  Val. Acc: 35.16% 

192
	Train Loss: 1.373 | Train Acc: 52.44%
	 Val. Loss: 1.533 |  Val. Acc: 35.12% 

193
	Train Loss: 1.376 | Train Acc: 52.22%
	 Val. Loss: 1.533 |  Val. Acc: 35.25% 

194
	Train Loss: 1.379 | Train Acc: 52.14%
	 Val. Loss: 1.533 |  Val. Acc: 35.22% 

195
	Train Loss: 1.365 | Train Acc: 53.33%
	 Val. Loss: 1.533 |  Val. Acc: 35.19% 

196
	Train Loss: 1.366 | Train Acc: 53.40%
	 Val. Loss: 1.532 |  Val. Acc: 35.25% 

197
	Train Loss: 1.373 | Train Acc: 52.78%
	 Val. Loss: 1.533 |  Val. Acc: 35.39% 

198
	Train Loss: 1.362 | Train Acc: 53.79%
	 Val. Loss: 1.531 |  Val. Acc: 35.59% 

199
	Train Loss: 1.367 | Train Acc: 52.76%
	 Val. Loss: 1.531 |  Val. Acc: 35.86% 

200
	Train Loss: 1.367 | Train Acc: 53.37%
	 Val. Loss: 1.530 |  Val. Acc: 36.06% 

label distribution: actual vs pred: {2: {4: 19, 2: 63, 0: 220, 3: 63, 1: 191}, 0: {0: 400, 3: 68, 2: 79, 4: 23, 1: 206}, 1: {2: 58, 1: 415, 0: 115, 3: 148, 4: 16}, 4: {1: 89, 3: 31, 4: 18, 0: 223, 2: 40}, 3: {1: 239, 3: 169, 0: 49, 2: 14, 4: 8}}
	 pred vs actuals: {2: {4: 19, 2: 63, 0: 220, 3: 63, 1: 191}, 0: {0: 400, 3: 68, 2: 79, 4: 23, 1: 206}, 1: {2: 58, 1: 415, 0: 115, 3: 148, 4: 16}, 4: {1: 89, 3: 31, 4: 18, 0: 223, 2: 40}, 3: {1: 239, 3: 169, 0: 49, 2: 14, 4: 8}}


     [0.22414575449445032, 0.23647608904337342, 0.23497672040354123, 0.2360519255426797, 0.2370284880426797, 0.23484848490492863, 0.23723563763566993, 0.23787681504406713, 0.2444760101762685, 0.24186197922311045, 0.24144767995246433, 0.24399266104129227, 0.24398279672657902, 0.24660669199445032, 0.2505129419944503, 0.25171638263220136, 0.2505129419944503, 0.252071496268565, 0.25249565968459303, 0.2571811869063161, 0.2650430082258853, 0.2630405619063161, 0.2665818340399049, 0.27094184031540697, 0.27006392045454547, 0.27204663831401954, 0.28256194767626847, 0.28861860795454547, 0.2839922664517706, 0.29612531580708246, 0.289180871268565, 0.29283065027133987, 0.3002288510853594, 0.3080216226252643, 0.30475654994899576, 0.311641808768565, 0.3129833491349762, 0.31798453290354123, 0.31323982013220136, 0.32172309031540697, 0.32976246845315804, 0.3272372159090909, 0.3214666193181818, 0.33279079876162787, 0.3299794823608615, 0.3343296244063161, 0.34424321353435516, 0.3444602272727273, 0.3487215909090909, 0.3498559817671776, 0.3511087437244979, 0.3559915562244979, 0.3535550820895217, 0.3571949968622489, 0.3587436869063161, 0.3613675820895217, 0.36375473490492866, 0.3694858744063161, 0.36870659718459303, 0.3694464173167944, 0.3738360164517706, 0.37939946353435516, 0.3824869792231105, 0.37208017673004756, 0.3793008207258853, 0.3825856218622489, 0.3869357639077035, 0.38341422040354123, 0.3913154988126321, 0.3938999369063161, 0.3930614741349762, 0.39408735795454547, 0.39678030308674683, 0.39507378468459303, 0.40327099118043075, 0.4001242897727273, 0.3993154198608615, 0.39946338398890063, 0.40495778095315804, 0.3994140625, 0.3984375, 0.4023832070895217, 0.4060724431818182, 0.40465198863636365, 0.42225970632650633, 0.4191031408580867, 0.41255326704545453, 0.4187381629916755, 0.4247652305798097, 0.42601799253713, 0.4304865056818182, 0.4277738320895217, 0.4240451389077035, 0.4212436869063161, 0.43030894886363635, 0.43163076076995244, 0.43297230113636365, 0.43665167299861257, 0.4353693181818182, 0.4368884153664112, 0.4399562027644027, 0.4363952021707188, 0.43799321353435516, 0.44888336482373153, 0.44369476017626847, 0.44359611753713, 0.4407749369063161, 0.44712752527133987, 0.44882417944344605, 0.4512606534090909, 0.45075757581401954, 0.45124092494899576, 0.4491792930798097, 0.4573074494573203, 0.45557133853435516, 0.45801767673004756, 0.45140861753713, 0.4512705176391385, 0.46732954545454547, 0.4624368687244979, 0.46586963398890063, 0.4681680082258853, 0.4721827652644027, 0.46705334599722514, 0.47174873744899576, 0.45552201704545453, 0.4735933555798097, 0.4664713542231105, 0.4755859375, 0.475280145352537, 0.4767005998979915, 0.4683850221335888, 0.47360321980985726, 0.48185961185531184, 0.4723899147727273, 0.47496448863636365, 0.4758720011873679, 0.4831518307328224, 0.47431344708258455, 0.4905204386873679, 0.48039970005100424, 0.4817215119573203, 0.4859236900440671, 0.4913293085992336, 0.48328993063081394, 0.4860617897727273, 0.4903330176391385, 0.4881628789007664, 0.4969913983209567, 0.48894215595315804, 0.4908065025440671, 0.4983230744573203, 0.49353890459645877, 0.49858940968459303, 0.4909446022727273, 0.5029888732189481, 0.49650804935531184, 0.5016177398237315, 0.5008581914007664, 0.5036399147727273, 0.5079604641280391, 0.5088383839889006, 0.5052872476252642, 0.501302083446221, 0.5058692392300476, 0.5155559500510042, 0.5096768465909091, 0.5104462595825846, 0.5120738636363636, 0.5175485323098573, 0.509962910278277, 0.5066189237616279, 0.5133759470825846, 0.5212772254916754, 0.5126065340909091, 0.5181897096335888, 0.5192353220825846, 0.5210010260343552, 0.5257851957258853, 0.5187815658070825, 0.5198074494573203, 0.5174893465909091, 0.5231711647727273, 0.5262290877713398, 0.5218493528664112, 0.5191958648237315, 0.5371192392300476, 0.526909722184593, 0.5328282829035412, 0.5288135260343552, 0.5214153250510042, 0.5244337909600951, 0.5221650095825846, 0.5214449180798097, 0.5332524464889006, 0.5340021306818182, 0.5277975062755021, 0.5379182449118658, 0.5276100852272727, 0.5337062027644027]
[0.2615026597012865, 0.2608377660842652, 0.25950797885022264, 0.2745345745314943, 0.26988031921234534, 0.2761968085740475, 0.27779255331830777, 0.28045212778639284, 0.26848404268000986, 0.27825797872340424, 0.28211436182894606, 0.27925531914893614, 0.2796542555727857, 0.28623670212765956, 0.28231383004087085, 0.2813164896153389, 0.28617021282936667, 0.2911569149570262, 0.2856382981259772, 0.2859042553191489, 0.29188829787234044, 0.2915558510638298, 0.2935505319148936, 0.2939494683387432, 0.29986702127659576, 0.29893617046640275, 0.29461436195576446, 0.295944149189807, 0.2992686172749134, 0.3005984045089559, 0.30226063855150914, 0.30398936157530926, 0.30904255331830777, 0.31070478736086093, 0.306981382851905, 0.30871010650979713, 0.30738031927575454, 0.306981382851905, 0.30871010650979713, 0.31010638335917856, 0.3023271275327561, 0.3056515956178625, 0.3015957449344879, 0.3103058509370114, 0.3119680849795646, 0.30591755344512617, 0.3156249998731816, 0.31163563817105394, 0.30890957472172187, 0.31050531914893614, 0.315226064083424, 0.3158909577004453, 0.3162234045089559, 0.3181515957446808, 0.31117021276595747, 0.3158244680851064, 0.3158244680851064, 0.3214760638297872, 0.31449468085106386, 0.3192154257855517, 0.31748670212765956, 0.3158909577004453, 0.3209441488093518, 0.3182845743412667, 0.3182845743412667, 0.3222739360433944, 0.322606382851905, 0.3192819147667986, 0.32626329774552204, 0.3259308509370114, 0.31988031940257294, 0.32626329774552204, 0.3335771275327561, 0.3339095743412667, 0.3329122339157348, 0.334574467958288, 0.3456781918698169, 0.34873670212765956, 0.34368351101875305, 0.3432845745949035, 0.3462765958714992, 0.3439494682119248, 0.34660904268000986, 0.3452792554459673, 0.3449468086374567, 0.3442819150204354, 0.34228723416937157, 0.3396276597012865, 0.3476063831055418, 0.34361702140341416, 0.3449468086374567, 0.3442154254050965, 0.34248670238129636, 0.3466755322953488, 0.34454787221360716, 0.34793882991405245, 0.3492021275327561, 0.34753989349020287, 0.34621010625616033, 0.3432180849795646, 0.3443484046357743, 0.347406914893617, 0.3454787236578921, 0.3440159578272637, 0.34308510638297873, 0.3410239365506679, 0.3471409577004453, 0.3443484046357743, 0.3432845745949035, 0.3435505317880752, 0.33936170250811476, 0.34242021276595747, 0.3453457450613062, 0.34601063867832754, 0.3486702131464126, 0.3476728727208807, 0.3442154254050965, 0.3503989361702128, 0.3442819150204354, 0.3472739362970312, 0.34893617033958435, 0.3449468086374567, 0.3509308511906482, 0.3496010639566056, 0.35226063842469074, 0.3482712767225631, 0.3472739362970312, 0.3517287234042553, 0.3500000003804552, 0.3490691489361702, 0.3513962765957447, 0.34893617033958435, 0.3482712767225631, 0.34893617033958435, 0.3496010639566056, 0.35126329799915884, 0.34361702140341416, 0.3449468086374567, 0.3458776594476497, 0.3476063831055418, 0.3468749998731816, 0.3452792554459673, 0.34554521263913907, 0.3449468086374567, 0.3493351067634339, 0.35226063842469074, 0.3539228724672439, 0.3476063831055418, 0.3486702131464126, 0.3515957448076695, 0.348337766337902, 0.35126329799915884, 0.35498670250811476, 0.3529920216570509, 0.3543218088910935, 0.3529920216570509, 0.35232712804002964, 0.35398936208258286, 0.35106382978723405, 0.35325797885022264, 0.34700797910385944, 0.35206117021276595, 0.3540558510638298, 0.35033244718896583, 0.3515957448076695, 0.3509308511906482, 0.3515957448076695, 0.3485372339157348, 0.3505319147667986, 0.34893617033958435, 0.35492021289277587, 0.35492021289277587, 0.3543218088910935, 0.3509308511906482, 0.3525930852332014, 0.3552526597012865, 0.35226063842469074, 0.35325797885022264, 0.352925532041712, 0.3525930852332014, 0.3539228724672439, 0.3539228724672439, 0.35226063842469074, 0.3509973408059871, 0.3519281916161801, 0.34886968072424546, 0.3563164897421573, 0.3586436174017318, 0.350199467958288, 0.35591755331830777, 0.3515957448076695, 0.3511968083838199, 0.3525265956178625, 0.3521941488093518, 0.35186170200084116, 0.3525265956178625, 0.353856382851905, 0.35591755331830777, 0.35857712778639284, 0.3606382982527956]


1
	Train Loss: 1.606 | Train Acc: 22.73%
	 Val. Loss: 1.597 |  Val. Acc: 25.55% 

2
	Train Loss: 1.603 | Train Acc: 23.21%
	 Val. Loss: 1.593 |  Val. Acc: 25.47% 

3
	Train Loss: 1.601 | Train Acc: 23.99%
	 Val. Loss: 1.591 |  Val. Acc: 26.35% 

4
	Train Loss: 1.600 | Train Acc: 24.35%
	 Val. Loss: 1.589 |  Val. Acc: 26.06% 

5
	Train Loss: 1.602 | Train Acc: 23.76%
	 Val. Loss: 1.591 |  Val. Acc: 26.95% 

6
	Train Loss: 1.599 | Train Acc: 24.22%
	 Val. Loss: 1.589 |  Val. Acc: 27.43% 

7
	Train Loss: 1.597 | Train Acc: 25.35%
	 Val. Loss: 1.587 |  Val. Acc: 28.01% 

8
	Train Loss: 1.596 | Train Acc: 25.44%
	 Val. Loss: 1.586 |  Val. Acc: 28.44% 

9
	Train Loss: 1.592 | Train Acc: 26.05%
	 Val. Loss: 1.583 |  Val. Acc: 28.91% 

10
	Train Loss: 1.586 | Train Acc: 27.44%
	 Val. Loss: 1.579 |  Val. Acc: 29.58% 

11
	Train Loss: 1.583 | Train Acc: 27.71%
	 Val. Loss: 1.575 |  Val. Acc: 29.01% 

12
	Train Loss: 1.575 | Train Acc: 29.50%
	 Val. Loss: 1.568 |  Val. Acc: 30.20% 

13
	Train Loss: 1.568 | Train Acc: 29.53%
	 Val. Loss: 1.562 |  Val. Acc: 30.98% 

14
	Train Loss: 1.562 | Train Acc: 30.68%
	 Val. Loss: 1.564 |  Val. Acc: 29.97% 

15
	Train Loss: 1.556 | Train Acc: 31.42%
	 Val. Loss: 1.552 |  Val. Acc: 31.91% 

16
	Train Loss: 1.552 | Train Acc: 32.41%
	 Val. Loss: 1.547 |  Val. Acc: 32.21% 

17
	Train Loss: 1.541 | Train Acc: 33.18%
	 Val. Loss: 1.547 |  Val. Acc: 32.24% 

18
	Train Loss: 1.535 | Train Acc: 33.86%
	 Val. Loss: 1.543 |  Val. Acc: 33.07% 

19
	Train Loss: 1.535 | Train Acc: 34.20%
	 Val. Loss: 1.535 |  Val. Acc: 34.23% 

20
	Train Loss: 1.525 | Train Acc: 35.51%
	 Val. Loss: 1.536 |  Val. Acc: 33.53% 

label distribution: actual vs pred: {2: {0: 332, 2: 46, 3: 67, 1: 111}, 0: {2: 43, 0: 528, 1: 129, 3: 76}, 1: {2: 46, 0: 273, 1: 257, 3: 176}, 4: {1: 51, 0: 280, 3: 57, 2: 13}, 3: {0: 121, 3: 171, 1: 178, 2: 9}}
	 pred vs actuals: {2: {0: 332, 2: 46, 3: 67, 1: 111}, 0: {2: 43, 0: 528, 1: 129, 3: 76}, 1: {2: 46, 0: 273, 1: 257, 3: 176}, 4: {1: 51, 0: 280, 3: 57, 2: 13}, 3: {0: 121, 3: 171, 1: 178, 2: 9}}
21
	Train Loss: 1.525 | Train Acc: 35.33%
	 Val. Loss: 1.534 |  Val. Acc: 34.03% 

22
	Train Loss: 1.519 | Train Acc: 35.92%
	 Val. Loss: 1.535 |  Val. Acc: 34.06% 

23
	Train Loss: 1.517 | Train Acc: 36.06%
	 Val. Loss: 1.538 |  Val. Acc: 33.84% 

24
	Train Loss: 1.511 | Train Acc: 36.80%
	 Val. Loss: 1.529 |  Val. Acc: 35.21% 

25
	Train Loss: 1.506 | Train Acc: 37.47%
	 Val. Loss: 1.528 |  Val. Acc: 35.59% 

26
	Train Loss: 1.502 | Train Acc: 38.21%
	 Val. Loss: 1.530 |  Val. Acc: 35.23% 

27
	Train Loss: 1.495 | Train Acc: 38.63%
	 Val. Loss: 1.530 |  Val. Acc: 34.77% 

28
	Train Loss: 1.494 | Train Acc: 39.00%
	 Val. Loss: 1.526 |  Val. Acc: 35.44% 

29
	Train Loss: 1.492 | Train Acc: 38.89%
	 Val. Loss: 1.529 |  Val. Acc: 35.66% 

30
	Train Loss: 1.495 | Train Acc: 38.55%
	 Val. Loss: 1.530 |  Val. Acc: 35.57% 

31
	Train Loss: 1.486 | Train Acc: 40.21%
	 Val. Loss: 1.532 |  Val. Acc: 34.73% 

32
	Train Loss: 1.485 | Train Acc: 39.96%
	 Val. Loss: 1.529 |  Val. Acc: 35.36% 

33
	Train Loss: 1.481 | Train Acc: 40.31%
	 Val. Loss: 1.530 |  Val. Acc: 35.19% 

34
	Train Loss: 1.474 | Train Acc: 41.31%
	 Val. Loss: 1.531 |  Val. Acc: 35.16% 

35
	Train Loss: 1.476 | Train Acc: 41.03%
	 Val. Loss: 1.535 |  Val. Acc: 35.11% 

36
	Train Loss: 1.472 | Train Acc: 41.78%
	 Val. Loss: 1.530 |  Val. Acc: 35.29% 

37
	Train Loss: 1.465 | Train Acc: 41.99%
	 Val. Loss: 1.532 |  Val. Acc: 35.42% 

38
	Train Loss: 1.464 | Train Acc: 42.48%
	 Val. Loss: 1.526 |  Val. Acc: 35.90% 

39
	Train Loss: 1.462 | Train Acc: 42.79%
	 Val. Loss: 1.535 |  Val. Acc: 34.85% 

40
	Train Loss: 1.457 | Train Acc: 43.19%
	 Val. Loss: 1.526 |  Val. Acc: 36.19% 

label distribution: actual vs pred: {2: {2: 96, 0: 256, 3: 59, 1: 145}, 0: {2: 79, 0: 472, 1: 159, 3: 65, 4: 1}, 1: {2: 73, 3: 162, 1: 335, 0: 181, 4: 1}, 4: {1: 65, 0: 261, 2: 32, 3: 43}, 3: {0: 73, 3: 179, 2: 18, 1: 208, 4: 1}}
	 pred vs actuals: {2: {2: 96, 0: 256, 3: 59, 1: 145}, 0: {2: 79, 0: 472, 1: 159, 3: 65, 4: 1}, 1: {2: 73, 3: 162, 1: 335, 0: 181, 4: 1}, 4: {1: 65, 0: 261, 2: 32, 3: 43}, 3: {0: 73, 3: 179, 2: 18, 1: 208, 4: 1}}
41
	Train Loss: 1.454 | Train Acc: 43.87%
	 Val. Loss: 1.521 |  Val. Acc: 36.70% 

42
	Train Loss: 1.450 | Train Acc: 44.02%
	 Val. Loss: 1.525 |  Val. Acc: 36.13% 

43
	Train Loss: 1.447 | Train Acc: 44.74%
	 Val. Loss: 1.533 |  Val. Acc: 35.26% 

44
	Train Loss: 1.444 | Train Acc: 44.42%
	 Val. Loss: 1.528 |  Val. Acc: 35.93% 

45
	Train Loss: 1.445 | Train Acc: 44.74%
	 Val. Loss: 1.527 |  Val. Acc: 36.30% 

46
	Train Loss: 1.440 | Train Acc: 45.18%
	 Val. Loss: 1.532 |  Val. Acc: 35.29% 

47
	Train Loss: 1.431 | Train Acc: 46.24%
	 Val. Loss: 1.532 |  Val. Acc: 35.29% 

48
	Train Loss: 1.434 | Train Acc: 46.05%
	 Val. Loss: 1.526 |  Val. Acc: 36.00% 

49
	Train Loss: 1.432 | Train Acc: 45.64%
	 Val. Loss: 1.524 |  Val. Acc: 36.56% 

50
	Train Loss: 1.430 | Train Acc: 46.18%
	 Val. Loss: 1.528 |  Val. Acc: 35.77% 

51
	Train Loss: 1.432 | Train Acc: 45.56%
	 Val. Loss: 1.525 |  Val. Acc: 35.93% 

52
	Train Loss: 1.425 | Train Acc: 47.00%
	 Val. Loss: 1.527 |  Val. Acc: 36.04% 

53
	Train Loss: 1.422 | Train Acc: 47.25%
	 Val. Loss: 1.523 |  Val. Acc: 36.23% 

54
	Train Loss: 1.423 | Train Acc: 47.09%
	 Val. Loss: 1.525 |  Val. Acc: 35.96% 

55
	Train Loss: 1.420 | Train Acc: 47.52%
	 Val. Loss: 1.527 |  Val. Acc: 36.40% 

56
	Train Loss: 1.416 | Train Acc: 47.94%
	 Val. Loss: 1.528 |  Val. Acc: 36.39% 

57
	Train Loss: 1.413 | Train Acc: 48.22%
	 Val. Loss: 1.520 |  Val. Acc: 37.29% 

58
	Train Loss: 1.410 | Train Acc: 48.13%
	 Val. Loss: 1.520 |  Val. Acc: 37.07% 

59
	Train Loss: 1.409 | Train Acc: 48.40%
	 Val. Loss: 1.523 |  Val. Acc: 36.57% 

60
	Train Loss: 1.405 | Train Acc: 49.03%
	 Val. Loss: 1.522 |  Val. Acc: 36.72% 

label distribution: actual vs pred: {2: {2: 101, 0: 263, 3: 61, 4: 4, 1: 127}, 0: {2: 92, 4: 6, 0: 496, 3: 61, 1: 121}, 1: {2: 92, 3: 173, 0: 186, 1: 297, 4: 4}, 4: {4: 9, 0: 263, 2: 40, 3: 33, 1: 56}, 3: {0: 72, 3: 195, 2: 24, 1: 184, 4: 4}}
	 pred vs actuals: {2: {2: 101, 0: 263, 3: 61, 4: 4, 1: 127}, 0: {2: 92, 4: 6, 0: 496, 3: 61, 1: 121}, 1: {2: 92, 3: 173, 0: 186, 1: 297, 4: 4}, 4: {4: 9, 0: 263, 2: 40, 3: 33, 1: 56}, 3: {0: 72, 3: 195, 2: 24, 1: 184, 4: 4}}
61
	Train Loss: 1.402 | Train Acc: 49.41%
	 Val. Loss: 1.518 |  Val. Acc: 37.21% 

62
	Train Loss: 1.403 | Train Acc: 49.08%
	 Val. Loss: 1.522 |  Val. Acc: 36.62% 

63
	Train Loss: 1.403 | Train Acc: 49.22%
	 Val. Loss: 1.520 |  Val. Acc: 36.76% 

64
	Train Loss: 1.398 | Train Acc: 49.64%
	 Val. Loss: 1.522 |  Val. Acc: 36.68% 

65
	Train Loss: 1.393 | Train Acc: 50.04%
	 Val. Loss: 1.524 |  Val. Acc: 36.32% 

66
	Train Loss: 1.391 | Train Acc: 50.88%
	 Val. Loss: 1.527 |  Val. Acc: 36.29% 

67
	Train Loss: 1.390 | Train Acc: 50.92%
	 Val. Loss: 1.527 |  Val. Acc: 36.26% 

68
	Train Loss: 1.388 | Train Acc: 51.06%
	 Val. Loss: 1.524 |  Val. Acc: 36.58% 

69
	Train Loss: 1.379 | Train Acc: 51.69%
	 Val. Loss: 1.522 |  Val. Acc: 36.97% 

70
	Train Loss: 1.380 | Train Acc: 51.56%
	 Val. Loss: 1.520 |  Val. Acc: 37.21% 

71
	Train Loss: 1.390 | Train Acc: 50.79%
	 Val. Loss: 1.524 |  Val. Acc: 36.54% 

72
	Train Loss: 1.373 | Train Acc: 52.81%
	 Val. Loss: 1.528 |  Val. Acc: 36.34% 

73
	Train Loss: 1.377 | Train Acc: 52.20%
	 Val. Loss: 1.530 |  Val. Acc: 35.92% 

74
	Train Loss: 1.377 | Train Acc: 51.55%
	 Val. Loss: 1.526 |  Val. Acc: 36.40% 

75
	Train Loss: 1.366 | Train Acc: 53.33%
	 Val. Loss: 1.530 |  Val. Acc: 36.02% 

76
	Train Loss: 1.373 | Train Acc: 52.65%
	 Val. Loss: 1.533 |  Val. Acc: 35.72% 

77
	Train Loss: 1.368 | Train Acc: 52.41%
	 Val. Loss: 1.526 |  Val. Acc: 36.56% 

78
	Train Loss: 1.361 | Train Acc: 54.22%
	 Val. Loss: 1.522 |  Val. Acc: 37.17% 

79
	Train Loss: 1.368 | Train Acc: 52.67%
	 Val. Loss: 1.522 |  Val. Acc: 37.04% 

80
	Train Loss: 1.359 | Train Acc: 54.17%
	 Val. Loss: 1.528 |  Val. Acc: 36.10% 

label distribution: actual vs pred: {2: {2: 77, 3: 80, 0: 261, 4: 7, 1: 131}, 0: {2: 81, 4: 18, 0: 468, 3: 78, 1: 131}, 1: {2: 72, 3: 210, 0: 173, 4: 8, 1: 289}, 4: {4: 19, 0: 253, 2: 32, 3: 46, 1: 51}, 3: {0: 56, 3: 222, 2: 18, 1: 177, 4: 6}}
	 pred vs actuals: {2: {2: 77, 3: 80, 0: 261, 4: 7, 1: 131}, 0: {2: 81, 4: 18, 0: 468, 3: 78, 1: 131}, 1: {2: 72, 3: 210, 0: 173, 4: 8, 1: 289}, 4: {4: 19, 0: 253, 2: 32, 3: 46, 1: 51}, 3: {0: 56, 3: 222, 2: 18, 1: 177, 4: 6}}
81
	Train Loss: 1.363 | Train Acc: 53.71%
	 Val. Loss: 1.526 |  Val. Acc: 36.50% 

82
	Train Loss: 1.358 | Train Acc: 54.00%
	 Val. Loss: 1.526 |  Val. Acc: 36.93% 

83
	Train Loss: 1.357 | Train Acc: 54.40%
	 Val. Loss: 1.521 |  Val. Acc: 36.88% 

84
	Train Loss: 1.357 | Train Acc: 54.12%
	 Val. Loss: 1.520 |  Val. Acc: 37.29% 

85
	Train Loss: 1.350 | Train Acc: 55.04%
	 Val. Loss: 1.527 |  Val. Acc: 36.20% 

86
	Train Loss: 1.358 | Train Acc: 54.22%
	 Val. Loss: 1.524 |  Val. Acc: 36.73% 

87
	Train Loss: 1.354 | Train Acc: 54.52%
	 Val. Loss: 1.524 |  Val. Acc: 36.63% 

88
	Train Loss: 1.348 | Train Acc: 55.22%
	 Val. Loss: 1.520 |  Val. Acc: 36.89% 

89
	Train Loss: 1.350 | Train Acc: 55.40%
	 Val. Loss: 1.523 |  Val. Acc: 36.93% 

90
	Train Loss: 1.344 | Train Acc: 55.50%
	 Val. Loss: 1.522 |  Val. Acc: 36.93% 

91
	Train Loss: 1.346 | Train Acc: 55.20%
	 Val. Loss: 1.523 |  Val. Acc: 37.11% 

92
	Train Loss: 1.343 | Train Acc: 55.77%
	 Val. Loss: 1.524 |  Val. Acc: 36.47% 

93
	Train Loss: 1.345 | Train Acc: 55.29%
	 Val. Loss: 1.519 |  Val. Acc: 36.94% 

94
	Train Loss: 1.335 | Train Acc: 56.71%
	 Val. Loss: 1.520 |  Val. Acc: 37.18% 

95
	Train Loss: 1.334 | Train Acc: 56.75%
	 Val. Loss: 1.521 |  Val. Acc: 37.37% 

96
	Train Loss: 1.338 | Train Acc: 56.51%
	 Val. Loss: 1.523 |  Val. Acc: 36.87% 

97
	Train Loss: 1.331 | Train Acc: 57.07%
	 Val. Loss: 1.527 |  Val. Acc: 36.02% 

98
	Train Loss: 1.333 | Train Acc: 56.86%
	 Val. Loss: 1.518 |  Val. Acc: 37.30% 

99
	Train Loss: 1.330 | Train Acc: 57.52%
	 Val. Loss: 1.523 |  Val. Acc: 36.64% 

100
	Train Loss: 1.329 | Train Acc: 56.69%
	 Val. Loss: 1.527 |  Val. Acc: 36.56% 

label distribution: actual vs pred: {2: {2: 91, 3: 82, 0: 246, 4: 13, 1: 124}, 0: {2: 94, 4: 23, 0: 470, 1: 122, 3: 67}, 1: {2: 93, 3: 209, 1: 278, 0: 159, 4: 13}, 4: {4: 26, 0: 245, 2: 40, 3: 40, 1: 50}, 3: {0: 50, 3: 228, 2: 27, 1: 168, 4: 6}}
	 pred vs actuals: {2: {2: 91, 3: 82, 0: 246, 4: 13, 1: 124}, 0: {2: 94, 4: 23, 0: 470, 1: 122, 3: 67}, 1: {2: 93, 3: 209, 1: 278, 0: 159, 4: 13}, 4: {4: 26, 0: 245, 2: 40, 3: 40, 1: 50}, 3: {0: 50, 3: 228, 2: 27, 1: 168, 4: 6}}
101
	Train Loss: 1.323 | Train Acc: 57.89%
	 Val. Loss: 1.530 |  Val. Acc: 35.96% 

102
	Train Loss: 1.328 | Train Acc: 57.41%
	 Val. Loss: 1.524 |  Val. Acc: 36.96% 

103
	Train Loss: 1.321 | Train Acc: 57.90%
	 Val. Loss: 1.526 |  Val. Acc: 36.76% 

104
	Train Loss: 1.318 | Train Acc: 58.15%
	 Val. Loss: 1.522 |  Val. Acc: 37.17% 

105
	Train Loss: 1.321 | Train Acc: 58.45%
	 Val. Loss: 1.524 |  Val. Acc: 37.03% 

106
	Train Loss: 1.317 | Train Acc: 58.44%
	 Val. Loss: 1.520 |  Val. Acc: 37.28% 

107
	Train Loss: 1.317 | Train Acc: 58.23%
	 Val. Loss: 1.516 |  Val. Acc: 37.79% 

108
	Train Loss: 1.317 | Train Acc: 58.56%
	 Val. Loss: 1.518 |  Val. Acc: 37.77% 

109
	Train Loss: 1.313 | Train Acc: 58.97%
	 Val. Loss: 1.522 |  Val. Acc: 37.40% 

110
	Train Loss: 1.310 | Train Acc: 59.36%
	 Val. Loss: 1.528 |  Val. Acc: 36.34% 

111
	Train Loss: 1.311 | Train Acc: 58.87%
	 Val. Loss: 1.529 |  Val. Acc: 36.06% 

112
	Train Loss: 1.307 | Train Acc: 59.65%
	 Val. Loss: 1.525 |  Val. Acc: 36.94% 

113
	Train Loss: 1.306 | Train Acc: 59.28%
	 Val. Loss: 1.527 |  Val. Acc: 36.34% 

114
	Train Loss: 1.310 | Train Acc: 58.75%
	 Val. Loss: 1.523 |  Val. Acc: 36.88% 

115
	Train Loss: 1.313 | Train Acc: 58.67%
	 Val. Loss: 1.526 |  Val. Acc: 36.51% 

116
	Train Loss: 1.303 | Train Acc: 59.90%
	 Val. Loss: 1.524 |  Val. Acc: 37.21% 

117
	Train Loss: 1.303 | Train Acc: 59.76%
	 Val. Loss: 1.526 |  Val. Acc: 36.70% 

118
	Train Loss: 1.304 | Train Acc: 59.98%
	 Val. Loss: 1.525 |  Val. Acc: 36.50% 

119
	Train Loss: 1.302 | Train Acc: 59.94%
	 Val. Loss: 1.525 |  Val. Acc: 36.71% 

120
	Train Loss: 1.296 | Train Acc: 60.52%
	 Val. Loss: 1.527 |  Val. Acc: 36.62% 

label distribution: actual vs pred: {2: {2: 110, 3: 62, 0: 244, 4: 26, 1: 114}, 0: {2: 100, 4: 42, 0: 479, 3: 52, 1: 103}, 1: {2: 107, 3: 194, 1: 263, 0: 175, 4: 13}, 4: {4: 33, 0: 255, 2: 41, 3: 34, 1: 38}, 3: {0: 60, 3: 210, 2: 30, 1: 170, 4: 9}}
	 pred vs actuals: {2: {2: 110, 3: 62, 0: 244, 4: 26, 1: 114}, 0: {2: 100, 4: 42, 0: 479, 3: 52, 1: 103}, 1: {2: 107, 3: 194, 1: 263, 0: 175, 4: 13}, 4: {4: 33, 0: 255, 2: 41, 3: 34, 1: 38}, 3: {0: 60, 3: 210, 2: 30, 1: 170, 4: 9}}
121
	Train Loss: 1.300 | Train Acc: 60.51%
	 Val. Loss: 1.524 |  Val. Acc: 36.71% 

122
	Train Loss: 1.292 | Train Acc: 61.24%
	 Val. Loss: 1.522 |  Val. Acc: 36.86% 

123
	Train Loss: 1.302 | Train Acc: 60.42%
	 Val. Loss: 1.517 |  Val. Acc: 37.89% 

124
	Train Loss: 1.291 | Train Acc: 60.65%
	 Val. Loss: 1.520 |  Val. Acc: 37.36% 

125
	Train Loss: 1.304 | Train Acc: 59.54%
	 Val. Loss: 1.518 |  Val. Acc: 37.63% 

126
	Train Loss: 1.290 | Train Acc: 61.18%
	 Val. Loss: 1.521 |  Val. Acc: 36.84% 

127
	Train Loss: 1.292 | Train Acc: 60.84%
	 Val. Loss: 1.524 |  Val. Acc: 36.87% 

128
	Train Loss: 1.287 | Train Acc: 61.84%
	 Val. Loss: 1.523 |  Val. Acc: 37.30% 

129
	Train Loss: 1.290 | Train Acc: 60.98%
	 Val. Loss: 1.528 |  Val. Acc: 36.70% 

130
	Train Loss: 1.291 | Train Acc: 60.77%
	 Val. Loss: 1.524 |  Val. Acc: 37.02% 

131
	Train Loss: 1.286 | Train Acc: 61.42%
	 Val. Loss: 1.524 |  Val. Acc: 37.01% 

132
	Train Loss: 1.284 | Train Acc: 62.19%
	 Val. Loss: 1.529 |  Val. Acc: 36.38% 

133
	Train Loss: 1.283 | Train Acc: 61.76%
	 Val. Loss: 1.523 |  Val. Acc: 36.91% 

134
	Train Loss: 1.280 | Train Acc: 61.85%
	 Val. Loss: 1.525 |  Val. Acc: 36.58% 

135
	Train Loss: 1.278 | Train Acc: 62.85%
	 Val. Loss: 1.523 |  Val. Acc: 36.93% 

136
	Train Loss: 1.280 | Train Acc: 62.20%
	 Val. Loss: 1.526 |  Val. Acc: 36.74% 

137
	Train Loss: 1.281 | Train Acc: 62.61%
	 Val. Loss: 1.526 |  Val. Acc: 36.62% 

138
	Train Loss: 1.274 | Train Acc: 62.62%
	 Val. Loss: 1.527 |  Val. Acc: 36.81% 

139
	Train Loss: 1.276 | Train Acc: 62.56%
	 Val. Loss: 1.525 |  Val. Acc: 36.71% 

140
	Train Loss: 1.271 | Train Acc: 62.81%
	 Val. Loss: 1.527 |  Val. Acc: 36.80% 

label distribution: actual vs pred: {2: {2: 96, 3: 58, 0: 259, 4: 23, 1: 120}, 0: {2: 101, 4: 48, 0: 487, 3: 42, 1: 98}, 1: {2: 91, 1: 270, 0: 194, 3: 175, 4: 22}, 4: {4: 39, 0: 261, 2: 38, 3: 24, 1: 39}, 3: {0: 58, 3: 206, 1: 167, 2: 33, 4: 15}}
	 pred vs actuals: {2: {2: 96, 3: 58, 0: 259, 4: 23, 1: 120}, 0: {2: 101, 4: 48, 0: 487, 3: 42, 1: 98}, 1: {2: 91, 1: 270, 0: 194, 3: 175, 4: 22}, 4: {4: 39, 0: 261, 2: 38, 3: 24, 1: 39}, 3: {0: 58, 3: 206, 1: 167, 2: 33, 4: 15}}
141
	Train Loss: 1.271 | Train Acc: 63.01%
	 Val. Loss: 1.525 |  Val. Acc: 36.99% 

142
	Train Loss: 1.271 | Train Acc: 62.72%
	 Val. Loss: 1.528 |  Val. Acc: 36.66% 

143
	Train Loss: 1.269 | Train Acc: 63.33%
	 Val. Loss: 1.526 |  Val. Acc: 36.77% 

144
	Train Loss: 1.271 | Train Acc: 63.17%
	 Val. Loss: 1.529 |  Val. Acc: 36.07% 

145
	Train Loss: 1.273 | Train Acc: 62.90%
	 Val. Loss: 1.526 |  Val. Acc: 36.84% 

146
	Train Loss: 1.267 | Train Acc: 63.56%
	 Val. Loss: 1.527 |  Val. Acc: 36.84% 

147
	Train Loss: 1.264 | Train Acc: 63.79%
	 Val. Loss: 1.528 |  Val. Acc: 36.18% 

148
	Train Loss: 1.265 | Train Acc: 63.91%
	 Val. Loss: 1.526 |  Val. Acc: 36.62% 

149
	Train Loss: 1.260 | Train Acc: 63.98%
	 Val. Loss: 1.528 |  Val. Acc: 36.62% 

150
	Train Loss: 1.264 | Train Acc: 63.56%
	 Val. Loss: 1.524 |  Val. Acc: 36.69% 

151
	Train Loss: 1.260 | Train Acc: 64.23%
	 Val. Loss: 1.525 |  Val. Acc: 36.85% 

152
	Train Loss: 1.258 | Train Acc: 64.46%
	 Val. Loss: 1.524 |  Val. Acc: 36.79% 

153
	Train Loss: 1.260 | Train Acc: 64.24%
	 Val. Loss: 1.528 |  Val. Acc: 36.32% 

154
	Train Loss: 1.259 | Train Acc: 64.01%
	 Val. Loss: 1.523 |  Val. Acc: 37.11% 

155
	Train Loss: 1.263 | Train Acc: 63.91%
	 Val. Loss: 1.526 |  Val. Acc: 36.21% 

156
	Train Loss: 1.262 | Train Acc: 63.82%
	 Val. Loss: 1.521 |  Val. Acc: 37.19% 

157
	Train Loss: 1.258 | Train Acc: 64.46%
	 Val. Loss: 1.525 |  Val. Acc: 36.83% 

158
	Train Loss: 1.256 | Train Acc: 64.66%
	 Val. Loss: 1.528 |  Val. Acc: 36.46% 

159
	Train Loss: 1.253 | Train Acc: 65.05%
	 Val. Loss: 1.526 |  Val. Acc: 37.09% 

160
	Train Loss: 1.254 | Train Acc: 64.48%
	 Val. Loss: 1.524 |  Val. Acc: 37.14% 

label distribution: actual vs pred: {2: {2: 112, 3: 54, 0: 223, 1: 138, 4: 29}, 0: {2: 109, 4: 69, 0: 439, 3: 42, 1: 117}, 1: {2: 105, 3: 163, 1: 315, 4: 28, 0: 141}, 4: {4: 44, 0: 239, 2: 50, 3: 20, 1: 48}, 3: {0: 47, 3: 194, 2: 35, 1: 188, 4: 15}}
	 pred vs actuals: {2: {2: 112, 3: 54, 0: 223, 1: 138, 4: 29}, 0: {2: 109, 4: 69, 0: 439, 3: 42, 1: 117}, 1: {2: 105, 3: 163, 1: 315, 4: 28, 0: 141}, 4: {4: 44, 0: 239, 2: 50, 3: 20, 1: 48}, 3: {0: 47, 3: 194, 2: 35, 1: 188, 4: 15}}
161
	Train Loss: 1.247 | Train Acc: 65.07%
	 Val. Loss: 1.526 |  Val. Acc: 36.84% 

162
	Train Loss: 1.253 | Train Acc: 64.85%
	 Val. Loss: 1.522 |  Val. Acc: 36.91% 

163
	Train Loss: 1.249 | Train Acc: 65.44%
	 Val. Loss: 1.529 |  Val. Acc: 36.66% 

164
	Train Loss: 1.246 | Train Acc: 65.29%
	 Val. Loss: 1.527 |  Val. Acc: 36.84% 

165
	Train Loss: 1.250 | Train Acc: 64.78%
	 Val. Loss: 1.531 |  Val. Acc: 36.17% 

166
	Train Loss: 1.245 | Train Acc: 65.54%
	 Val. Loss: 1.529 |  Val. Acc: 36.24% 

167
	Train Loss: 1.246 | Train Acc: 65.12%
	 Val. Loss: 1.526 |  Val. Acc: 36.78% 

168
	Train Loss: 1.246 | Train Acc: 65.41%
	 Val. Loss: 1.532 |  Val. Acc: 36.11% 

169
	Train Loss: 1.244 | Train Acc: 65.24%
	 Val. Loss: 1.530 |  Val. Acc: 36.31% 

170
	Train Loss: 1.242 | Train Acc: 65.89%
	 Val. Loss: 1.525 |  Val. Acc: 36.85% 

171
	Train Loss: 1.246 | Train Acc: 65.65%
	 Val. Loss: 1.528 |  Val. Acc: 36.82% 

172
	Train Loss: 1.241 | Train Acc: 65.47%
	 Val. Loss: 1.527 |  Val. Acc: 36.58% 

173
	Train Loss: 1.241 | Train Acc: 65.88%
	 Val. Loss: 1.524 |  Val. Acc: 37.12% 

174
	Train Loss: 1.237 | Train Acc: 66.06%
	 Val. Loss: 1.524 |  Val. Acc: 37.11% 

175
	Train Loss: 1.237 | Train Acc: 66.03%
	 Val. Loss: 1.529 |  Val. Acc: 36.69% 

176
	Train Loss: 1.241 | Train Acc: 66.12%
	 Val. Loss: 1.532 |  Val. Acc: 36.48% 

177
	Train Loss: 1.242 | Train Acc: 66.27%
	 Val. Loss: 1.532 |  Val. Acc: 36.24% 

178
	Train Loss: 1.234 | Train Acc: 66.41%
	 Val. Loss: 1.528 |  Val. Acc: 36.57% 

179
	Train Loss: 1.234 | Train Acc: 66.76%
	 Val. Loss: 1.531 |  Val. Acc: 36.40% 

180
	Train Loss: 1.239 | Train Acc: 66.22%
	 Val. Loss: 1.532 |  Val. Acc: 36.27% 

label distribution: actual vs pred: {2: {2: 120, 3: 68, 0: 210, 1: 122, 4: 36}, 0: {2: 119, 4: 67, 0: 428, 3: 58, 1: 104}, 1: {2: 121, 1: 269, 3: 190, 0: 138, 4: 34}, 4: {4: 49, 0: 229, 2: 55, 3: 30, 1: 38}, 3: {0: 43, 3: 214, 2: 32, 1: 170, 4: 20}}
	 pred vs actuals: {2: {2: 120, 3: 68, 0: 210, 1: 122, 4: 36}, 0: {2: 119, 4: 67, 0: 428, 3: 58, 1: 104}, 1: {2: 121, 1: 269, 3: 190, 0: 138, 4: 34}, 4: {4: 49, 0: 229, 2: 55, 3: 30, 1: 38}, 3: {0: 43, 3: 214, 2: 32, 1: 170, 4: 20}}
181
	Train Loss: 1.237 | Train Acc: 66.48%
	 Val. Loss: 1.532 |  Val. Acc: 35.86% 

182
	Train Loss: 1.230 | Train Acc: 67.07%
	 Val. Loss: 1.530 |  Val. Acc: 36.24% 

183
	Train Loss: 1.231 | Train Acc: 66.42%
	 Val. Loss: 1.533 |  Val. Acc: 35.97% 

184
	Train Loss: 1.235 | Train Acc: 66.77%
	 Val. Loss: 1.533 |  Val. Acc: 35.97% 

185
	Train Loss: 1.230 | Train Acc: 67.13%
	 Val. Loss: 1.530 |  Val. Acc: 36.71% 

186
	Train Loss: 1.229 | Train Acc: 66.61%
	 Val. Loss: 1.532 |  Val. Acc: 36.23% 

187
	Train Loss: 1.230 | Train Acc: 67.16%
	 Val. Loss: 1.531 |  Val. Acc: 36.27% 

188
	Train Loss: 1.232 | Train Acc: 67.01%
	 Val. Loss: 1.532 |  Val. Acc: 36.48% 

189
	Train Loss: 1.229 | Train Acc: 67.23%
	 Val. Loss: 1.534 |  Val. Acc: 36.39% 

190
	Train Loss: 1.234 | Train Acc: 66.60%
	 Val. Loss: 1.532 |  Val. Acc: 36.34% 

191
	Train Loss: 1.229 | Train Acc: 67.03%
	 Val. Loss: 1.532 |  Val. Acc: 36.36% 

192
	Train Loss: 1.227 | Train Acc: 67.33%
	 Val. Loss: 1.532 |  Val. Acc: 36.40% 

193
	Train Loss: 1.230 | Train Acc: 66.90%
	 Val. Loss: 1.531 |  Val. Acc: 36.56% 

194
	Train Loss: 1.230 | Train Acc: 67.08%
	 Val. Loss: 1.526 |  Val. Acc: 37.21% 

195
	Train Loss: 1.220 | Train Acc: 68.29%
	 Val. Loss: 1.529 |  Val. Acc: 36.58% 

196
	Train Loss: 1.222 | Train Acc: 67.40%
	 Val. Loss: 1.529 |  Val. Acc: 36.51% 

197
	Train Loss: 1.220 | Train Acc: 67.95%
	 Val. Loss: 1.531 |  Val. Acc: 36.34% 

198
	Train Loss: 1.223 | Train Acc: 67.22%
	 Val. Loss: 1.530 |  Val. Acc: 36.61% 

199
	Train Loss: 1.223 | Train Acc: 67.24%
	 Val. Loss: 1.534 |  Val. Acc: 36.22% 

200
	Train Loss: 1.224 | Train Acc: 67.62%
	 Val. Loss: 1.532 |  Val. Acc: 36.33% 

label distribution: actual vs pred: {2: {2: 115, 3: 47, 0: 218, 1: 145, 4: 31}, 0: {2: 117, 4: 64, 0: 446, 3: 35, 1: 114}, 1: {2: 110, 1: 311, 3: 153, 0: 145, 4: 33}, 4: {1: 45, 0: 242, 2: 51, 3: 17, 4: 46}, 3: {0: 56, 3: 166, 2: 33, 1: 207, 4: 17}}
	 pred vs actuals: {2: {2: 115, 3: 47, 0: 218, 1: 145, 4: 31}, 0: {2: 117, 4: 64, 0: 446, 3: 35, 1: 114}, 1: {2: 110, 1: 311, 3: 153, 0: 145, 4: 33}, 4: {1: 45, 0: 242, 2: 51, 3: 17, 4: 46}, 3: {0: 56, 3: 166, 2: 33, 1: 207, 4: 17}}


     [0.2272628630426797, 0.2320864899083972, 0.23990885422311045, 0.24353890468112446, 0.2375808869573203, 0.2421579072251916, 0.2534820864993063, 0.25437973490492866, 0.2605153093622489, 0.27440419826995244, 0.27710700763220136, 0.29495146785947407, 0.29525726017626847, 0.30676886049861257, 0.31421638263220136, 0.32405105741186574, 0.3318043718622489, 0.3385909880426797, 0.3419744318181818, 0.3551432292231105, 0.3533380681818182, 0.3592467646707188, 0.3605981692671776, 0.3680358271707188, 0.3746744792231105, 0.38209240849722514, 0.38627485795454547, 0.38996409421617334, 0.3889480744573203, 0.38550544513220136, 0.40210700763220136, 0.39964094080708246, 0.40308357013220136, 0.41310566604476084, 0.4103140783580867, 0.4177911931818182, 0.41987255368043075, 0.4247652305798097, 0.42786261049861257, 0.43187736753713, 0.43871330503713, 0.44017321650277486, 0.4474037248979915, 0.4442175664007664, 0.4474431818181818, 0.451842645352537, 0.4623974116349762, 0.46045415103435516, 0.45640980113636365, 0.4618252840909091, 0.45561079545454547, 0.47002249058674683, 0.4725082860412923, 0.4709102745083245, 0.4752406880936839, 0.47935408777133987, 0.4822443181818182, 0.4812874841419133, 0.48398042944344605, 0.4902935607189482, 0.4941011680798097, 0.49081636694344605, 0.49219736423004756, 0.4963798136873679, 0.5003551136363636, 0.5087890625, 0.5091540403664112, 0.5105744949118658, 0.5169073548167944, 0.5156151357699524, 0.5078618214889006, 0.5280737057328224, 0.522026909684593, 0.5155460858209566, 0.5333017676391385, 0.5264756943691861, 0.5241082703525369, 0.5421500158580866, 0.5267321653664112, 0.5417357165027749, 0.5370896465399049, 0.5400489267300476, 0.5440044982189481, 0.5411635891280391, 0.5504360005936839, 0.5421796085482294, 0.5451980744573203, 0.5521622476252642, 0.5539871369573203, 0.5549538352272727, 0.551955097778277, 0.5577256943691861, 0.552931660278277, 0.5670967488126322, 0.5674913193691861, 0.5650943023237315, 0.5707366635853593, 0.56859611753713, 0.5752150409600951, 0.566948784684593, 0.5789338699118658, 0.5741102431308139, 0.5790423767810519, 0.5814788510853593, 0.584526909684593, 0.5843986743553118, 0.5823271779174154, 0.5855922505936839, 0.5897253789007664, 0.5935921716419134, 0.5887488164007664, 0.5964725379916754, 0.5927537090399049, 0.5875355113636364, 0.5867069129916754, 0.5990076545964588, 0.5975773358209566, 0.599806660278277, 0.5993923612616279, 0.6051728220825846, 0.6051235005936839, 0.6123638732189481, 0.6042357165027749, 0.6064650409600951, 0.5954170613126322, 0.6118410669944503, 0.6084181659600951, 0.6184106692671776, 0.6098287562755021, 0.6076980744573203, 0.6141887625510042, 0.6218631629916754, 0.6175623420964588, 0.6184994476762685, 0.6285314079035412, 0.6220012625510042, 0.6260949335992336, 0.6261837120083246, 0.6255523989146407, 0.6280776516280391, 0.6300702335482294, 0.6271997317671776, 0.6332662562755021, 0.6316682449118658, 0.6290048926391385, 0.6355942233719609, 0.6378630050881342, 0.6391453597355973, 0.6398062658580866, 0.6355547664517706, 0.6423117897727273, 0.6445608426901427, 0.6424301608719609, 0.6400528726252642, 0.6391256312755021, 0.6382477114146407, 0.6446298926391385, 0.6466027460992336, 0.6504596749489958, 0.64476799253713, 0.6506865529174154, 0.6484966857189481, 0.6544448391280391, 0.6528961489146407, 0.6477568654174154, 0.6553819443691861, 0.6512192233719609, 0.6541094539517706, 0.6524029357189481, 0.6588640311224894, 0.6565360636873678, 0.6547210385853593, 0.6587949811734937, 0.6606198705055497, 0.6603042141280391, 0.6612314551391385, 0.6626913669434461, 0.6640822284600951, 0.6676037721335888, 0.6621685607189481, 0.6648220487616279, 0.67066169496287, 0.6641808710992336, 0.6676728220825846, 0.671332465315407, 0.6661142676391385, 0.67163825746287, 0.6700797033580866, 0.6723386205055497, 0.6660452176901427, 0.6703460385853593, 0.673285590315407, 0.6690439551391385, 0.6708293875510042, 0.6829229799861257, 0.6740352745083246, 0.67945075746287, 0.6722498420964588, 0.6723879419944503, 0.676215277815407]
[0.255518617148095, 0.2547207449344879, 0.2634973405523503, 0.26063829787234044, 0.2694813831055418, 0.274335106398831, 0.2800531915210663, 0.2844414894885205, 0.2890957448076695, 0.29581117027617515, 0.2900930852332014, 0.30199468072424546, 0.309773936233622, 0.29966755322319394, 0.31908244687191983, 0.3220744681485156, 0.3224069149570262, 0.3307180851697922, 0.34228723416937157, 0.3353058511906482, 0.34029255331830777, 0.3405585106700025, 0.33836436176553686, 0.35212765982810484, 0.35591755331830777, 0.35226063842469074, 0.3476728724038347, 0.35438829787234044, 0.356648936233622, 0.35565159580809, 0.3473404255953241, 0.3535904256587333, 0.3518617021593642, 0.3515957448076695, 0.3511303191647885, 0.352925532041712, 0.3541888298189386, 0.35904255319148937, 0.3485372340742578, 0.3619015958714992, 0.3670212765957447, 0.3613031915527709, 0.3525930852332014, 0.35930851070170705, 0.3629654255953241, 0.352925532041712, 0.352925532041712, 0.36003989361702127, 0.3656250000634092, 0.35771276595744683, 0.35930851070170705, 0.3603723404255319, 0.3623005319783028, 0.3596409575102177, 0.36396276602085603, 0.3638962767225631, 0.37293882985064325, 0.3706781914893617, 0.36569148936170215, 0.3672207448076695, 0.3720744683387432, 0.3662234043821375, 0.367619680914473, 0.36675531940257294, 0.3631648936487259, 0.3628989362970312, 0.3625664894885205, 0.365824467958288, 0.3697473406791687, 0.3720744683387432, 0.3654255321685304, 0.36336436170212766, 0.35917553194659824, 0.3640292553191489, 0.36023936182894606, 0.3571808510955344, 0.3656250000634092, 0.37174202153023256, 0.37041223429618997, 0.36103723404255317, 0.36496010644638793, 0.3692819149570262, 0.3687500002536368, 0.37293882985064325, 0.3619680851697922, 0.36728723410596237, 0.36628989368043047, 0.36894946846556154, 0.3693484042553192, 0.3693484042553192, 0.3711436168944582, 0.3646941489361702, 0.36941489387065807, 0.3718085105114795, 0.37367021276595747, 0.36868351063829785, 0.36017287237213014, 0.37300531914893614, 0.3663563829787234, 0.36555851076511625, 0.3596409575102177, 0.36961436176553686, 0.367619680914473, 0.3716755319148936, 0.3702792553825581, 0.3728058509370114, 0.37785904268000986, 0.377726064083424, 0.3740026595744681, 0.36336436170212766, 0.36063829793574964, 0.36941489387065807, 0.36343085131746655, 0.36881648923488375, 0.3650930853600198, 0.3720744683387432, 0.3670212765957447, 0.3650265957446808, 0.3670877662110836, 0.3662234043821375, 0.3670877662110836, 0.36861702134000496, 0.3788563831055418, 0.3736037234676645, 0.3762632982527956, 0.3683510638297872, 0.36868351063829785, 0.37300531914893614, 0.36695478729745173, 0.3702127660842652, 0.37007978748767933, 0.3637632981259772, 0.36908244706214743, 0.36575797897704104, 0.3693484042553192, 0.3674202130195942, 0.3661569147667986, 0.36808510663661553, 0.3670877662110836, 0.3679521277229837, 0.3699468085740475, 0.3665558511906482, 0.36768617021276595, 0.36070478723404253, 0.3683510638297872, 0.3683510638297872, 0.36183510625616033, 0.3661569147667986, 0.3661569147667986, 0.36688829799915884, 0.3684840424263731, 0.36788563842469074, 0.36316489349020287, 0.37107712791321124, 0.362101064083424, 0.3718750001268184, 0.36828457484854027, 0.36456117033958435, 0.3708776597012865, 0.37140957472172187, 0.36841755344512617, 0.3691489360433944, 0.3665558511906482, 0.36841755344512617, 0.3617021276595745, 0.36236702127659576, 0.36775265982810484, 0.3611037236578921, 0.3630984045089559, 0.3684840424263731, 0.3681515956178625, 0.36575797897704104, 0.37121010650979713, 0.3711436168944582, 0.36688829799915884, 0.3648271275327561, 0.36243351089193465, 0.36569148936170215, 0.3640292553191489, 0.3626994680851064, 0.3586436170846858, 0.36236702127659576, 0.35970744680851063, 0.35970744680851063, 0.3670877662110836, 0.3623005319783028, 0.3626994680851064, 0.36476063855150914, 0.3638962767225631, 0.36343085131746655, 0.36363031921234534, 0.3640292553191489, 0.36555851076511625, 0.3720744683387432, 0.36582446811681096, 0.3650930853600198, 0.36343085131746655, 0.3660904257855517, 0.36223404268000986, 0.3632978724038347]







size_of_vocab = len(Review.vocab)
embedding_dim = 400
num_hidden_nodes = 200
num_output_nodes = 5
num_layers = 2
dropout = 0.4
lr = 1e-4


1
	Train Loss: 1.605 | Train Acc: 22.65%
	 Val. Loss: 1.592 |  Val. Acc: 25.25% 

2
	Train Loss: 1.602 | Train Acc: 24.01%
	 Val. Loss: 1.590 |  Val. Acc: 26.89% 

3
	Train Loss: 1.601 | Train Acc: 23.82%
	 Val. Loss: 1.590 |  Val. Acc: 26.12% 

4
	Train Loss: 1.599 | Train Acc: 24.67%
	 Val. Loss: 1.587 |  Val. Acc: 28.44% 

5
	Train Loss: 1.597 | Train Acc: 25.15%
	 Val. Loss: 1.584 |  Val. Acc: 29.21% 

6
	Train Loss: 1.593 | Train Acc: 26.36%
	 Val. Loss: 1.579 |  Val. Acc: 29.53% 

7
	Train Loss: 1.587 | Train Acc: 27.07%
	 Val. Loss: 1.573 |  Val. Acc: 29.26% 

8
	Train Loss: 1.580 | Train Acc: 28.38%
	 Val. Loss: 1.574 |  Val. Acc: 28.04% 

9
	Train Loss: 1.572 | Train Acc: 29.53%
	 Val. Loss: 1.562 |  Val. Acc: 30.36% 

10
	Train Loss: 1.562 | Train Acc: 31.27%
	 Val. Loss: 1.555 |  Val. Acc: 31.92% 

11
	Train Loss: 1.557 | Train Acc: 31.67%
	 Val. Loss: 1.556 |  Val. Acc: 31.72% 

12
	Train Loss: 1.550 | Train Acc: 32.69%
	 Val. Loss: 1.546 |  Val. Acc: 33.06% 

13
	Train Loss: 1.545 | Train Acc: 33.31%
	 Val. Loss: 1.545 |  Val. Acc: 32.77% 

14
	Train Loss: 1.540 | Train Acc: 34.57%
	 Val. Loss: 1.547 |  Val. Acc: 33.07% 

15
	Train Loss: 1.530 | Train Acc: 35.45%
	 Val. Loss: 1.543 |  Val. Acc: 33.76% 

16
	Train Loss: 1.527 | Train Acc: 35.50%
	 Val. Loss: 1.540 |  Val. Acc: 33.70% 

17
	Train Loss: 1.520 | Train Acc: 36.35%
	 Val. Loss: 1.537 |  Val. Acc: 34.06% 

18
	Train Loss: 1.516 | Train Acc: 37.14%
	 Val. Loss: 1.538 |  Val. Acc: 34.87% 

19
	Train Loss: 1.510 | Train Acc: 38.37%
	 Val. Loss: 1.535 |  Val. Acc: 34.87% 

20
	Train Loss: 1.506 | Train Acc: 37.97%
	 Val. Loss: 1.534 |  Val. Acc: 35.27% 

label distribution: actual vs pred: {2: {2: 34, 1: 132, 3: 50, 0: 340}, 0: {2: 31, 0: 561, 1: 131, 3: 53}, 1: {1: 292, 0: 280, 2: 30, 3: 150}, 4: {1: 62, 0: 297, 3: 32, 2: 10}, 3: {1: 196, 3: 165, 2: 12, 0: 106}}
	 pred vs actuals: {2: {2: 34, 1: 132, 3: 50, 0: 340}, 0: {2: 31, 0: 561, 1: 131, 3: 53}, 1: {1: 292, 0: 280, 2: 30, 3: 150}, 4: {1: 62, 0: 297, 3: 32, 2: 10}, 3: {1: 196, 3: 165, 2: 12, 0: 106}}
21
	Train Loss: 1.504 | Train Acc: 39.01%
	 Val. Loss: 1.531 |  Val. Acc: 36.02% 

22
	Train Loss: 1.499 | Train Acc: 39.46%
	 Val. Loss: 1.531 |  Val. Acc: 35.83% 

23
	Train Loss: 1.492 | Train Acc: 40.38%
	 Val. Loss: 1.531 |  Val. Acc: 35.72% 

24
	Train Loss: 1.488 | Train Acc: 40.23%
	 Val. Loss: 1.532 |  Val. Acc: 35.76% 

25
	Train Loss: 1.488 | Train Acc: 40.33%
	 Val. Loss: 1.527 |  Val. Acc: 35.95% 

26
	Train Loss: 1.481 | Train Acc: 41.86%
	 Val. Loss: 1.531 |  Val. Acc: 35.61% 

27
	Train Loss: 1.476 | Train Acc: 41.88%
	 Val. Loss: 1.528 |  Val. Acc: 36.03% 

28
	Train Loss: 1.474 | Train Acc: 41.99%
	 Val. Loss: 1.541 |  Val. Acc: 34.43% 

29
	Train Loss: 1.477 | Train Acc: 41.62%
	 Val. Loss: 1.528 |  Val. Acc: 35.51% 

30
	Train Loss: 1.463 | Train Acc: 43.15%
	 Val. Loss: 1.531 |  Val. Acc: 35.76% 

31
	Train Loss: 1.460 | Train Acc: 44.03%
	 Val. Loss: 1.532 |  Val. Acc: 35.44% 

32
	Train Loss: 1.458 | Train Acc: 44.24%
	 Val. Loss: 1.531 |  Val. Acc: 35.20% 

33
	Train Loss: 1.454 | Train Acc: 44.62%
	 Val. Loss: 1.526 |  Val. Acc: 36.64% 

34
	Train Loss: 1.445 | Train Acc: 45.58%
	 Val. Loss: 1.530 |  Val. Acc: 35.71% 

35
	Train Loss: 1.445 | Train Acc: 45.07%
	 Val. Loss: 1.535 |  Val. Acc: 35.23% 

36
	Train Loss: 1.443 | Train Acc: 45.57%
	 Val. Loss: 1.525 |  Val. Acc: 36.52% 

37
	Train Loss: 1.436 | Train Acc: 46.61%
	 Val. Loss: 1.530 |  Val. Acc: 35.72% 

38
	Train Loss: 1.438 | Train Acc: 46.50%
	 Val. Loss: 1.538 |  Val. Acc: 35.21% 

39
	Train Loss: 1.427 | Train Acc: 47.56%
	 Val. Loss: 1.540 |  Val. Acc: 34.63% 

40
	Train Loss: 1.433 | Train Acc: 47.20%
	 Val. Loss: 1.533 |  Val. Acc: 35.72% 

label distribution: actual vs pred: {2: {2: 50, 1: 154, 3: 60, 0: 291, 4: 1}, 0: {2: 46, 3: 44, 0: 514, 1: 170, 4: 2}, 1: {2: 41, 0: 225, 1: 332, 3: 152, 4: 2}, 4: {1: 80, 0: 281, 3: 21, 2: 18, 4: 1}, 3: {0: 79, 3: 171, 2: 13, 1: 216}}
	 pred vs actuals: {2: {2: 50, 1: 154, 3: 60, 0: 291, 4: 1}, 0: {2: 46, 3: 44, 0: 514, 1: 170, 4: 2}, 1: {2: 41, 0: 225, 1: 332, 3: 152, 4: 2}, 4: {1: 80, 0: 281, 3: 21, 2: 18, 4: 1}, 3: {0: 79, 3: 171, 2: 13, 1: 216}}
41
	Train Loss: 1.429 | Train Acc: 47.12%
	 Val. Loss: 1.537 |  Val. Acc: 34.66% 

42
	Train Loss: 1.421 | Train Acc: 48.21%
	 Val. Loss: 1.537 |  Val. Acc: 35.18% 

43
	Train Loss: 1.415 | Train Acc: 48.70%
	 Val. Loss: 1.529 |  Val. Acc: 35.98% 

44
	Train Loss: 1.416 | Train Acc: 49.00%
	 Val. Loss: 1.529 |  Val. Acc: 35.72% 

45
	Train Loss: 1.418 | Train Acc: 48.98%
	 Val. Loss: 1.528 |  Val. Acc: 36.25% 

46
	Train Loss: 1.409 | Train Acc: 49.28%
	 Val. Loss: 1.527 |  Val. Acc: 36.74% 

47
	Train Loss: 1.407 | Train Acc: 50.07%
	 Val. Loss: 1.529 |  Val. Acc: 36.12% 

48
	Train Loss: 1.407 | Train Acc: 49.63%
	 Val. Loss: 1.521 |  Val. Acc: 37.19% 

49
	Train Loss: 1.399 | Train Acc: 50.32%
	 Val. Loss: 1.536 |  Val. Acc: 35.10% 

50
	Train Loss: 1.405 | Train Acc: 49.56%
	 Val. Loss: 1.533 |  Val. Acc: 35.76% 

51
	Train Loss: 1.392 | Train Acc: 51.24%
	 Val. Loss: 1.531 |  Val. Acc: 35.13% 

52
	Train Loss: 1.394 | Train Acc: 50.97%
	 Val. Loss: 1.533 |  Val. Acc: 35.77% 

53
	Train Loss: 1.391 | Train Acc: 51.62%
	 Val. Loss: 1.533 |  Val. Acc: 35.83% 

54
	Train Loss: 1.383 | Train Acc: 52.39%
	 Val. Loss: 1.526 |  Val. Acc: 36.48% 

55
	Train Loss: 1.384 | Train Acc: 52.41%
	 Val. Loss: 1.534 |  Val. Acc: 35.40% 

56
	Train Loss: 1.379 | Train Acc: 52.46%
	 Val. Loss: 1.540 |  Val. Acc: 34.63% 

57
	Train Loss: 1.380 | Train Acc: 52.29%
	 Val. Loss: 1.535 |  Val. Acc: 35.57% 

58
	Train Loss: 1.374 | Train Acc: 53.18%
	 Val. Loss: 1.532 |  Val. Acc: 35.88% 

59
	Train Loss: 1.371 | Train Acc: 53.31%
	 Val. Loss: 1.530 |  Val. Acc: 35.64% 

60
	Train Loss: 1.363 | Train Acc: 54.50%
	 Val. Loss: 1.530 |  Val. Acc: 36.32% 

label distribution: actual vs pred: {2: {2: 46, 3: 64, 0: 286, 1: 153, 4: 7}, 0: {2: 51, 4: 12, 0: 490, 1: 169, 3: 54}, 1: {2: 46, 0: 183, 1: 328, 3: 188, 4: 7}, 4: {1: 83, 0: 265, 3: 25, 2: 20, 4: 8}, 3: {1: 188, 3: 214, 2: 10, 0: 63, 4: 4}}
	 pred vs actuals: {2: {2: 46, 3: 64, 0: 286, 1: 153, 4: 7}, 0: {2: 51, 4: 12, 0: 490, 1: 169, 3: 54}, 1: {2: 46, 0: 183, 1: 328, 3: 188, 4: 7}, 4: {1: 83, 0: 265, 3: 25, 2: 20, 4: 8}, 3: {1: 188, 3: 214, 2: 10, 0: 63, 4: 4}}
61
	Train Loss: 1.368 | Train Acc: 53.74%
	 Val. Loss: 1.534 |  Val. Acc: 35.27% 

62
	Train Loss: 1.362 | Train Acc: 54.27%
	 Val. Loss: 1.530 |  Val. Acc: 36.10% 

63
	Train Loss: 1.361 | Train Acc: 54.07%
	 Val. Loss: 1.529 |  Val. Acc: 36.51% 

64
	Train Loss: 1.362 | Train Acc: 54.46%
	 Val. Loss: 1.530 |  Val. Acc: 35.88% 

65
	Train Loss: 1.354 | Train Acc: 54.90%
	 Val. Loss: 1.533 |  Val. Acc: 36.06% 

66
	Train Loss: 1.357 | Train Acc: 54.89%
	 Val. Loss: 1.531 |  Val. Acc: 35.84% 

67
	Train Loss: 1.349 | Train Acc: 55.91%
	 Val. Loss: 1.531 |  Val. Acc: 36.20% 

68
	Train Loss: 1.352 | Train Acc: 55.93%
	 Val. Loss: 1.529 |  Val. Acc: 35.81% 

69
	Train Loss: 1.346 | Train Acc: 55.95%
	 Val. Loss: 1.526 |  Val. Acc: 36.44% 

70
	Train Loss: 1.343 | Train Acc: 56.27%
	 Val. Loss: 1.531 |  Val. Acc: 35.90% 

71
	Train Loss: 1.341 | Train Acc: 56.51%
	 Val. Loss: 1.530 |  Val. Acc: 36.15% 

72
	Train Loss: 1.337 | Train Acc: 56.88%
	 Val. Loss: 1.531 |  Val. Acc: 35.96% 

73
	Train Loss: 1.344 | Train Acc: 55.95%
	 Val. Loss: 1.534 |  Val. Acc: 35.78% 

74
	Train Loss: 1.338 | Train Acc: 56.90%
	 Val. Loss: 1.529 |  Val. Acc: 36.32% 

75
	Train Loss: 1.334 | Train Acc: 57.09%
	 Val. Loss: 1.527 |  Val. Acc: 36.71% 

76
	Train Loss: 1.337 | Train Acc: 57.10%
	 Val. Loss: 1.528 |  Val. Acc: 36.18% 

77
	Train Loss: 1.338 | Train Acc: 57.06%
	 Val. Loss: 1.535 |  Val. Acc: 35.58% 

78
	Train Loss: 1.331 | Train Acc: 57.19%
	 Val. Loss: 1.535 |  Val. Acc: 35.54% 

79
	Train Loss: 1.321 | Train Acc: 58.69%
	 Val. Loss: 1.537 |  Val. Acc: 35.48% 

80
	Train Loss: 1.324 | Train Acc: 58.25%
	 Val. Loss: 1.528 |  Val. Acc: 36.54% 

label distribution: actual vs pred: {2: {2: 61, 3: 51, 0: 262, 1: 168, 4: 14}, 0: {2: 79, 1: 176, 0: 476, 4: 17, 3: 28}, 1: {2: 62, 0: 175, 1: 360, 3: 147, 4: 8}, 4: {1: 71, 0: 265, 3: 20, 2: 34, 4: 11}, 3: {0: 61, 3: 178, 1: 216, 2: 20, 4: 4}}
	 pred vs actuals: {2: {2: 61, 3: 51, 0: 262, 1: 168, 4: 14}, 0: {2: 79, 1: 176, 0: 476, 4: 17, 3: 28}, 1: {2: 62, 0: 175, 1: 360, 3: 147, 4: 8}, 4: {1: 71, 0: 265, 3: 20, 2: 34, 4: 11}, 3: {0: 61, 3: 178, 1: 216, 2: 20, 4: 4}}
81
	Train Loss: 1.321 | Train Acc: 58.75%
	 Val. Loss: 1.530 |  Val. Acc: 36.38% 

82
	Train Loss: 1.320 | Train Acc: 58.99%
	 Val. Loss: 1.527 |  Val. Acc: 36.64% 

83
	Train Loss: 1.317 | Train Acc: 59.20%
	 Val. Loss: 1.535 |  Val. Acc: 35.92% 

84
	Train Loss: 1.321 | Train Acc: 58.09%
	 Val. Loss: 1.529 |  Val. Acc: 36.48% 

85
	Train Loss: 1.313 | Train Acc: 59.47%
	 Val. Loss: 1.531 |  Val. Acc: 36.04% 

86
	Train Loss: 1.310 | Train Acc: 59.74%
	 Val. Loss: 1.534 |  Val. Acc: 35.86% 

87
	Train Loss: 1.313 | Train Acc: 59.79%
	 Val. Loss: 1.531 |  Val. Acc: 35.98% 

88
	Train Loss: 1.315 | Train Acc: 59.14%
	 Val. Loss: 1.534 |  Val. Acc: 35.92% 

89
	Train Loss: 1.303 | Train Acc: 60.73%
	 Val. Loss: 1.537 |  Val. Acc: 35.65% 

90
	Train Loss: 1.310 | Train Acc: 59.78%
	 Val. Loss: 1.527 |  Val. Acc: 36.34% 

91
	Train Loss: 1.303 | Train Acc: 60.37%
	 Val. Loss: 1.535 |  Val. Acc: 35.66% 

92
	Train Loss: 1.305 | Train Acc: 60.12%
	 Val. Loss: 1.515 |  Val. Acc: 37.84% 

93
	Train Loss: 1.299 | Train Acc: 61.33%
	 Val. Loss: 1.526 |  Val. Acc: 36.58% 

94
	Train Loss: 1.303 | Train Acc: 60.57%
	 Val. Loss: 1.529 |  Val. Acc: 36.58% 

95
	Train Loss: 1.293 | Train Acc: 61.30%
	 Val. Loss: 1.533 |  Val. Acc: 36.07% 

96
	Train Loss: 1.294 | Train Acc: 61.17%
	 Val. Loss: 1.524 |  Val. Acc: 36.95% 

97
	Train Loss: 1.292 | Train Acc: 61.40%
	 Val. Loss: 1.527 |  Val. Acc: 36.64% 

98
	Train Loss: 1.289 | Train Acc: 61.71%
	 Val. Loss: 1.524 |  Val. Acc: 36.89% 

99
	Train Loss: 1.293 | Train Acc: 61.27%
	 Val. Loss: 1.524 |  Val. Acc: 37.19% 

100
	Train Loss: 1.290 | Train Acc: 62.06%
	 Val. Loss: 1.527 |  Val. Acc: 36.68% 

label distribution: actual vs pred: {2: {2: 74, 3: 63, 4: 32, 1: 176, 0: 211}, 0: {2: 105, 4: 35, 0: 403, 1: 193, 3: 40}, 1: {2: 57, 0: 131, 1: 361, 4: 17, 3: 186}, 4: {1: 82, 0: 230, 3: 22, 2: 34, 4: 33}, 3: {1: 195, 3: 217, 2: 21, 0: 34, 4: 12}}
	 pred vs actuals: {2: {2: 74, 3: 63, 4: 32, 1: 176, 0: 211}, 0: {2: 105, 4: 35, 0: 403, 1: 193, 3: 40}, 1: {2: 57, 0: 131, 1: 361, 4: 17, 3: 186}, 4: {1: 82, 0: 230, 3: 22, 2: 34, 4: 33}, 3: {1: 195, 3: 217, 2: 21, 0: 34, 4: 12}}
101
	Train Loss: 1.289 | Train Acc: 61.86%
	 Val. Loss: 1.527 |  Val. Acc: 36.86% 

102
	Train Loss: 1.281 | Train Acc: 62.26%
	 Val. Loss: 1.529 |  Val. Acc: 36.68% 

103
	Train Loss: 1.285 | Train Acc: 61.99%
	 Val. Loss: 1.528 |  Val. Acc: 36.42% 

104
	Train Loss: 1.279 | Train Acc: 63.04%
	 Val. Loss: 1.529 |  Val. Acc: 36.34% 

105
	Train Loss: 1.284 | Train Acc: 62.28%
	 Val. Loss: 1.529 |  Val. Acc: 36.76% 

106
	Train Loss: 1.284 | Train Acc: 62.43%
	 Val. Loss: 1.529 |  Val. Acc: 36.52% 

107
	Train Loss: 1.279 | Train Acc: 63.12%
	 Val. Loss: 1.528 |  Val. Acc: 36.68% 

108
	Train Loss: 1.276 | Train Acc: 63.17%
	 Val. Loss: 1.537 |  Val. Acc: 35.75% 

109
	Train Loss: 1.271 | Train Acc: 63.22%
	 Val. Loss: 1.523 |  Val. Acc: 37.43% 

110
	Train Loss: 1.269 | Train Acc: 64.08%
	 Val. Loss: 1.522 |  Val. Acc: 37.32% 

111
	Train Loss: 1.278 | Train Acc: 62.77%
	 Val. Loss: 1.529 |  Val. Acc: 36.61% 

112
	Train Loss: 1.272 | Train Acc: 63.57%
	 Val. Loss: 1.539 |  Val. Acc: 35.34% 

113
	Train Loss: 1.272 | Train Acc: 63.37%
	 Val. Loss: 1.534 |  Val. Acc: 36.00% 

114
	Train Loss: 1.270 | Train Acc: 63.32%
	 Val. Loss: 1.535 |  Val. Acc: 35.47% 

115
	Train Loss: 1.265 | Train Acc: 63.93%
	 Val. Loss: 1.525 |  Val. Acc: 36.75% 

116
	Train Loss: 1.265 | Train Acc: 63.64%
	 Val. Loss: 1.528 |  Val. Acc: 37.05% 

117
	Train Loss: 1.264 | Train Acc: 64.07%
	 Val. Loss: 1.527 |  Val. Acc: 36.78% 

118
	Train Loss: 1.261 | Train Acc: 64.07%
	 Val. Loss: 1.522 |  Val. Acc: 37.29% 

119
	Train Loss: 1.260 | Train Acc: 64.67%
	 Val. Loss: 1.530 |  Val. Acc: 36.37% 

120
	Train Loss: 1.260 | Train Acc: 64.57%
	 Val. Loss: 1.523 |  Val. Acc: 37.31% 

label distribution: actual vs pred: {2: {2: 76, 3: 44, 4: 29, 1: 163, 0: 244}, 0: {2: 96, 4: 35, 0: 448, 1: 175, 3: 22}, 1: {2: 60, 1: 399, 4: 20, 3: 125, 0: 148}, 4: {1: 66, 2: 35, 0: 256, 3: 16, 4: 28}, 3: {1: 237, 3: 156, 2: 32, 0: 45, 4: 9}}
	 pred vs actuals: {2: {2: 76, 3: 44, 4: 29, 1: 163, 0: 244}, 0: {2: 96, 4: 35, 0: 448, 1: 175, 3: 22}, 1: {2: 60, 1: 399, 4: 20, 3: 125, 0: 148}, 4: {1: 66, 2: 35, 0: 256, 3: 16, 4: 28}, 3: {1: 237, 3: 156, 2: 32, 0: 45, 4: 9}}
121
	Train Loss: 1.259 | Train Acc: 64.64%
	 Val. Loss: 1.525 |  Val. Acc: 36.77% 

122
	Train Loss: 1.262 | Train Acc: 64.57%
	 Val. Loss: 1.528 |  Val. Acc: 36.67% 

123
	Train Loss: 1.261 | Train Acc: 65.08%
	 Val. Loss: 1.526 |  Val. Acc: 36.78% 

124
	Train Loss: 1.258 | Train Acc: 64.16%
	 Val. Loss: 1.523 |  Val. Acc: 36.92% 

125
	Train Loss: 1.256 | Train Acc: 65.44%
	 Val. Loss: 1.532 |  Val. Acc: 36.41% 

126
	Train Loss: 1.253 | Train Acc: 64.69%
	 Val. Loss: 1.532 |  Val. Acc: 36.44% 

127
	Train Loss: 1.245 | Train Acc: 66.16%
	 Val. Loss: 1.521 |  Val. Acc: 37.47% 

128
	Train Loss: 1.251 | Train Acc: 65.07%
	 Val. Loss: 1.524 |  Val. Acc: 37.09% 

129
	Train Loss: 1.248 | Train Acc: 65.72%
	 Val. Loss: 1.518 |  Val. Acc: 37.48% 

130
	Train Loss: 1.245 | Train Acc: 66.15%
	 Val. Loss: 1.518 |  Val. Acc: 37.59% 

131
	Train Loss: 1.248 | Train Acc: 65.53%
	 Val. Loss: 1.521 |  Val. Acc: 37.61% 

132
	Train Loss: 1.245 | Train Acc: 65.79%
	 Val. Loss: 1.529 |  Val. Acc: 36.45% 

133
	Train Loss: 1.245 | Train Acc: 65.82%
	 Val. Loss: 1.526 |  Val. Acc: 36.72% 

134
	Train Loss: 1.240 | Train Acc: 67.01%
	 Val. Loss: 1.529 |  Val. Acc: 36.71% 

135
	Train Loss: 1.240 | Train Acc: 66.72%
	 Val. Loss: 1.529 |  Val. Acc: 36.51% 

136
	Train Loss: 1.241 | Train Acc: 65.98%
	 Val. Loss: 1.532 |  Val. Acc: 36.48% 

137
	Train Loss: 1.241 | Train Acc: 66.09%
	 Val. Loss: 1.529 |  Val. Acc: 36.86% 

138
	Train Loss: 1.238 | Train Acc: 66.24%
	 Val. Loss: 1.524 |  Val. Acc: 37.05% 

139
	Train Loss: 1.233 | Train Acc: 66.95%
	 Val. Loss: 1.535 |  Val. Acc: 36.08% 

140
	Train Loss: 1.242 | Train Acc: 66.31%
	 Val. Loss: 1.532 |  Val. Acc: 36.12% 

label distribution: actual vs pred: {2: {2: 82, 3: 58, 4: 37, 1: 165, 0: 214}, 0: {2: 115, 3: 44, 0: 398, 1: 181, 4: 38}, 1: {2: 76, 1: 340, 4: 21, 3: 181, 0: 134}, 4: {1: 70, 0: 223, 2: 41, 4: 43, 3: 24}, 3: {1: 193, 3: 206, 2: 29, 0: 39, 4: 12}}
	 pred vs actuals: {2: {2: 82, 3: 58, 4: 37, 1: 165, 0: 214}, 0: {2: 115, 3: 44, 0: 398, 1: 181, 4: 38}, 1: {2: 76, 1: 340, 4: 21, 3: 181, 0: 134}, 4: {1: 70, 0: 223, 2: 41, 4: 43, 3: 24}, 3: {1: 193, 3: 206, 2: 29, 0: 39, 4: 12}}
141
	Train Loss: 1.238 | Train Acc: 66.78%
	 Val. Loss: 1.526 |  Val. Acc: 36.90% 

142
	Train Loss: 1.232 | Train Acc: 67.12%
	 Val. Loss: 1.525 |  Val. Acc: 36.73% 

143
	Train Loss: 1.234 | Train Acc: 66.92%
	 Val. Loss: 1.534 |  Val. Acc: 36.57% 

144
	Train Loss: 1.230 | Train Acc: 67.72%
	 Val. Loss: 1.532 |  Val. Acc: 36.45% 

145
	Train Loss: 1.235 | Train Acc: 67.45%
	 Val. Loss: 1.523 |  Val. Acc: 37.20% 

146
	Train Loss: 1.227 | Train Acc: 67.38%
	 Val. Loss: 1.530 |  Val. Acc: 36.62% 

147
	Train Loss: 1.231 | Train Acc: 67.39%
	 Val. Loss: 1.527 |  Val. Acc: 36.96% 

148
	Train Loss: 1.227 | Train Acc: 67.86%
	 Val. Loss: 1.529 |  Val. Acc: 36.72% 

149
	Train Loss: 1.232 | Train Acc: 67.08%
	 Val. Loss: 1.526 |  Val. Acc: 36.99% 

150
	Train Loss: 1.225 | Train Acc: 67.63%
	 Val. Loss: 1.531 |  Val. Acc: 36.68% 

151
	Train Loss: 1.231 | Train Acc: 67.57%
	 Val. Loss: 1.527 |  Val. Acc: 37.15% 

152
	Train Loss: 1.226 | Train Acc: 67.81%
	 Val. Loss: 1.525 |  Val. Acc: 36.76% 

153
	Train Loss: 1.222 | Train Acc: 67.63%
	 Val. Loss: 1.530 |  Val. Acc: 36.42% 

154
	Train Loss: 1.225 | Train Acc: 67.84%
	 Val. Loss: 1.530 |  Val. Acc: 36.40% 

155
	Train Loss: 1.217 | Train Acc: 68.62%
	 Val. Loss: 1.528 |  Val. Acc: 36.54% 

156
	Train Loss: 1.224 | Train Acc: 67.76%
	 Val. Loss: 1.528 |  Val. Acc: 36.66% 

157
	Train Loss: 1.220 | Train Acc: 68.65%
	 Val. Loss: 1.531 |  Val. Acc: 36.40% 

158
	Train Loss: 1.218 | Train Acc: 68.06%
	 Val. Loss: 1.532 |  Val. Acc: 36.30% 

159
	Train Loss: 1.220 | Train Acc: 68.51%
	 Val. Loss: 1.532 |  Val. Acc: 36.32% 

160
	Train Loss: 1.220 | Train Acc: 68.09%
	 Val. Loss: 1.531 |  Val. Acc: 36.46% 

label distribution: actual vs pred: {2: {2: 87, 3: 30, 4: 36, 1: 188, 0: 215}, 0: {2: 119, 1: 193, 0: 411, 4: 34, 3: 19}, 1: {2: 68, 1: 408, 3: 114, 0: 146, 4: 16}, 4: {1: 70, 2: 39, 0: 239, 4: 44, 3: 9}, 3: {1: 253, 3: 129, 2: 30, 0: 51, 4: 16}}
	 pred vs actuals: {2: {2: 87, 3: 30, 4: 36, 1: 188, 0: 215}, 0: {2: 119, 1: 193, 0: 411, 4: 34, 3: 19}, 1: {2: 68, 1: 408, 3: 114, 0: 146, 4: 16}, 4: {1: 70, 2: 39, 0: 239, 4: 44, 3: 9}, 3: {1: 253, 3: 129, 2: 30, 0: 51, 4: 16}}
161
	Train Loss: 1.215 | Train Acc: 69.09%
	 Val. Loss: 1.528 |  Val. Acc: 36.62% 

162
	Train Loss: 1.217 | Train Acc: 68.66%
	 Val. Loss: 1.534 |  Val. Acc: 35.98% 

163
	Train Loss: 1.217 | Train Acc: 68.28%
	 Val. Loss: 1.527 |  Val. Acc: 36.69% 

164
	Train Loss: 1.214 | Train Acc: 68.98%
	 Val. Loss: 1.527 |  Val. Acc: 36.79% 

165
	Train Loss: 1.211 | Train Acc: 69.50%
	 Val. Loss: 1.525 |  Val. Acc: 37.39% 

166
	Train Loss: 1.216 | Train Acc: 68.93%
	 Val. Loss: 1.533 |  Val. Acc: 36.28% 

167
	Train Loss: 1.213 | Train Acc: 69.03%
	 Val. Loss: 1.536 |  Val. Acc: 35.98% 

168
	Train Loss: 1.210 | Train Acc: 68.88%
	 Val. Loss: 1.532 |  Val. Acc: 36.05% 

169
	Train Loss: 1.209 | Train Acc: 69.10%
	 Val. Loss: 1.535 |  Val. Acc: 35.88% 

170
	Train Loss: 1.206 | Train Acc: 69.43%
	 Val. Loss: 1.530 |  Val. Acc: 36.74% 

171
	Train Loss: 1.208 | Train Acc: 69.22%
	 Val. Loss: 1.536 |  Val. Acc: 36.16% 

172
	Train Loss: 1.210 | Train Acc: 69.09%
	 Val. Loss: 1.528 |  Val. Acc: 36.84% 

173
	Train Loss: 1.208 | Train Acc: 69.09%
	 Val. Loss: 1.536 |  Val. Acc: 35.53% 

174
	Train Loss: 1.207 | Train Acc: 69.58%
	 Val. Loss: 1.534 |  Val. Acc: 36.03% 

175
	Train Loss: 1.208 | Train Acc: 69.37%
	 Val. Loss: 1.534 |  Val. Acc: 36.20% 

176
	Train Loss: 1.203 | Train Acc: 69.62%
	 Val. Loss: 1.542 |  Val. Acc: 35.51% 

177
	Train Loss: 1.201 | Train Acc: 70.08%
	 Val. Loss: 1.538 |  Val. Acc: 35.86% 

178
	Train Loss: 1.204 | Train Acc: 69.96%
	 Val. Loss: 1.532 |  Val. Acc: 36.36% 

179
	Train Loss: 1.199 | Train Acc: 70.00%
	 Val. Loss: 1.535 |  Val. Acc: 35.92% 

180
	Train Loss: 1.199 | Train Acc: 69.62%
	 Val. Loss: 1.536 |  Val. Acc: 35.92% 

label distribution: actual vs pred: {2: {2: 106, 3: 50, 4: 45, 1: 152, 0: 203}, 0: {2: 143, 4: 54, 0: 383, 1: 172, 3: 24}, 1: {2: 89, 1: 360, 3: 143, 0: 137, 4: 23}, 4: {3: 17, 2: 50, 0: 224, 4: 47, 1: 63}, 3: {1: 212, 3: 169, 2: 44, 0: 35, 4: 19}}
	 pred vs actuals: {2: {2: 106, 3: 50, 4: 45, 1: 152, 0: 203}, 0: {2: 143, 4: 54, 0: 383, 1: 172, 3: 24}, 1: {2: 89, 1: 360, 3: 143, 0: 137, 4: 23}, 4: {3: 17, 2: 50, 0: 224, 4: 47, 1: 63}, 3: {1: 212, 3: 169, 2: 44, 0: 35, 4: 19}}
181
	Train Loss: 1.204 | Train Acc: 69.66%
	 Val. Loss: 1.539 |  Val. Acc: 35.51% 

182
	Train Loss: 1.198 | Train Acc: 70.14%
	 Val. Loss: 1.538 |  Val. Acc: 35.83% 

183
	Train Loss: 1.199 | Train Acc: 70.46%
	 Val. Loss: 1.540 |  Val. Acc: 35.47% 

184
	Train Loss: 1.197 | Train Acc: 70.07%
	 Val. Loss: 1.531 |  Val. Acc: 36.68% 

185
	Train Loss: 1.200 | Train Acc: 69.92%
	 Val. Loss: 1.536 |  Val. Acc: 36.00% 

186
	Train Loss: 1.195 | Train Acc: 70.44%
	 Val. Loss: 1.539 |  Val. Acc: 35.35% 

187
	Train Loss: 1.196 | Train Acc: 70.55%
	 Val. Loss: 1.540 |  Val. Acc: 35.53% 

188
	Train Loss: 1.197 | Train Acc: 70.39%
	 Val. Loss: 1.541 |  Val. Acc: 35.21% 

189
	Train Loss: 1.194 | Train Acc: 70.36%
	 Val. Loss: 1.540 |  Val. Acc: 35.57% 

190
	Train Loss: 1.196 | Train Acc: 70.17%
	 Val. Loss: 1.532 |  Val. Acc: 36.08% 

191
	Train Loss: 1.193 | Train Acc: 70.63%
	 Val. Loss: 1.535 |  Val. Acc: 36.12% 

192
	Train Loss: 1.193 | Train Acc: 70.64%
	 Val. Loss: 1.536 |  Val. Acc: 36.12% 

193
	Train Loss: 1.198 | Train Acc: 70.20%
	 Val. Loss: 1.545 |  Val. Acc: 35.02% 

194
	Train Loss: 1.191 | Train Acc: 70.83%
	 Val. Loss: 1.550 |  Val. Acc: 34.38% 

195
	Train Loss: 1.187 | Train Acc: 71.57%
	 Val. Loss: 1.543 |  Val. Acc: 35.15% 

196
	Train Loss: 1.195 | Train Acc: 70.18%
	 Val. Loss: 1.537 |  Val. Acc: 35.88% 

197
	Train Loss: 1.187 | Train Acc: 70.69%
	 Val. Loss: 1.531 |  Val. Acc: 36.65% 

198
	Train Loss: 1.190 | Train Acc: 70.84%
	 Val. Loss: 1.544 |  Val. Acc: 35.05% 

199
	Train Loss: 1.190 | Train Acc: 71.13%
	 Val. Loss: 1.538 |  Val. Acc: 35.94% 

200
	Train Loss: 1.187 | Train Acc: 71.52%
	 Val. Loss: 1.537 |  Val. Acc: 35.65% 

label distribution: actual vs pred: {2: {2: 102, 3: 36, 4: 67, 1: 153, 0: 198}, 0: {4: 85, 0: 386, 1: 139, 2: 141, 3: 25}, 1: {2: 103, 1: 341, 3: 133, 4: 35, 0: 140}, 4: {3: 16, 2: 45, 0: 217, 4: 72, 1: 51}, 3: {4: 29, 3: 156, 1: 218, 2: 36, 0: 40}}
	 pred vs actuals: {2: {2: 102, 3: 36, 4: 67, 1: 153, 0: 198}, 0: {4: 85, 0: 386, 1: 139, 2: 141, 3: 25}, 1: {2: 103, 1: 341, 3: 133, 4: 35, 0: 140}, 4: {3: 16, 2: 45, 0: 217, 4: 72, 1: 51}, 3: {4: 29, 3: 156, 1: 218, 2: 36, 0: 40}}


     [0.2265329072251916, 0.24007654672657902, 0.2382220644503832, 0.2467349274083972, 0.25145004740492866, 0.26356336813081394, 0.2707149621776559, 0.28376538831401954, 0.29525726017626847, 0.3127071496776559, 0.31670217808674683, 0.3269018308174881, 0.333126183768565, 0.34569326076995244, 0.3544527305798097, 0.3550051294944503, 0.36350812830708246, 0.37144886363636365, 0.38373974118043075, 0.3796953914517706, 0.39009232954545453, 0.3946200284090909, 0.4037839330055497, 0.40229442868043075, 0.40327099118043075, 0.41862965595315804, 0.4188170771707188, 0.4198626894503832, 0.4162326389077035, 0.4314532039517706, 0.4403310448608615, 0.44239267673004756, 0.44623974118043075, 0.45575875958258455, 0.45068852603435516, 0.4556502525440671, 0.4660965119573203, 0.4650311710482294, 0.4755859375, 0.47199534421617334, 0.4711568812755021, 0.4820766255936839, 0.4869594380936839, 0.4899877682328224, 0.48979048295454547, 0.49280894886363635, 0.5006806345825846, 0.4962811710482294, 0.5032157511873678, 0.4956202652644027, 0.5123993845825846, 0.5097458965399049, 0.5161773989146407, 0.5239307135343552, 0.5241181345825846, 0.5246409408070825, 0.5229146937755021, 0.5318319919434461, 0.5331242108209566, 0.5450402460992336, 0.5373855744573203, 0.5427221432328224, 0.5407295613126322, 0.5446160828525369, 0.5490155460482294, 0.5488577176901427, 0.5591066919944503, 0.5592743845825846, 0.5594519414007664, 0.5626874210482294, 0.5650844380936839, 0.5687835385853593, 0.5595407198098573, 0.5689808238636364, 0.5709438130936839, 0.5709536773237315, 0.5706182923167944, 0.5719105113636364, 0.5868943340399049, 0.5825047347355973, 0.5874960544434461, 0.5899029357189481, 0.5920040245083246, 0.5809363164007664, 0.5946673767810519, 0.5973504578525369, 0.5979423136873678, 0.5914121686734937, 0.6073133681308139, 0.5977943498979915, 0.6037227745083246, 0.6011777936734937, 0.6133305714889006, 0.6057153567671776, 0.6129853220825846, 0.6117424243553118, 0.6140013415027749, 0.6170691289007664, 0.6126598011363636, 0.6206202652644027, 0.618598090315407, 0.6226227114146407, 0.619919902221723, 0.6303957544944503, 0.6227706755426797, 0.6242700442671776, 0.6311553029174154, 0.6317274306308139, 0.6322107795964588, 0.640822285278277, 0.6276830807328224, 0.6357125948098573, 0.6337298767810519, 0.6331873420964588, 0.6392735954035412, 0.6364228220825846, 0.6406545926901427, 0.6406841857189481, 0.6466717960482294, 0.645705097778277, 0.6464251892810519, 0.645705097778277, 0.6507654670964588, 0.6416311551901427, 0.6543659249489958, 0.6469381312755021, 0.6616358902644027, 0.650716145946221, 0.6571575125510042, 0.6615076545964588, 0.6553424874489958, 0.6579071970825846, 0.6581932607699524, 0.6700895675881342, 0.667189472778277, 0.6597814079035412, 0.6608862057328224, 0.6624447601762685, 0.6695075756446882, 0.6630760732699524, 0.6678207858719609, 0.671223958446221, 0.6691524620083246, 0.677191840315407, 0.6744693023237315, 0.6738478534600951, 0.6738577176901427, 0.6786122948608615, 0.6708293875510042, 0.6762941919944503, 0.6756727431308139, 0.678059895946221, 0.676323784684593, 0.678385416553779, 0.6861584596335888, 0.6775666824118658, 0.6864543875510042, 0.6806344698098573, 0.6850832544944503, 0.680861347778277, 0.6908538510853593, 0.6866319443691861, 0.6827848800881342, 0.6897885101762685, 0.6950165720825846, 0.6893150254406712, 0.6902817233719609, 0.6887626261873678, 0.690972222184593, 0.6943063448098573, 0.6922249841419134, 0.690863715315407, 0.6908933080055497, 0.6958155777644027, 0.6936553029174154, 0.6961805556308139, 0.7008364898237315, 0.6995541351762685, 0.7000473483719609, 0.6962298767810519, 0.6965652619573203, 0.7013790245083246, 0.7045651830055497, 0.7006687972355973, 0.6992384784600951, 0.7043974904174154, 0.7055417455055497, 0.7039240056818182, 0.703559027815407, 0.7016749527644027, 0.706251972778277, 0.7063506154174154, 0.7019906091419134, 0.7082938761873678, 0.7156624841419134, 0.7017735954035412, 0.7068734216419134, 0.7084221118553118, 0.711263020946221, 0.7151791351762685]
[0.2524601064146833, 0.26888297878681344, 0.26117021289277587, 0.2843749998731816, 0.2920877660842652, 0.2953457445540327, 0.2925531914893617, 0.2803856383295769, 0.3035904255477672, 0.3192154255477672, 0.3171542553191489, 0.3305851064146833, 0.3276595745949035, 0.3306515958714992, 0.33763297885022264, 0.3369680852332014, 0.3405585106700025, 0.34867021282936667, 0.34867021282936667, 0.3526595745314943, 0.36023936182894606, 0.35831117027617515, 0.3572473405523503, 0.3576462766591539, 0.35950797859658584, 0.35611702153023256, 0.360305851127239, 0.3442819150204354, 0.3550531914893617, 0.3576462766591539, 0.35438829787234044, 0.351994680914473, 0.3664228725940623, 0.35711436195576446, 0.3523271277229837, 0.3652260639566056, 0.3571808509370114, 0.35206117021276595, 0.3462765958714992, 0.3572473405523503, 0.34660904268000986, 0.3517952130195942, 0.3598404254050965, 0.3571808509370114, 0.3624999998731816, 0.3673537234042553, 0.36117021263913907, 0.3718750001268184, 0.3509973404889411, 0.3576462766591539, 0.35132978729745173, 0.35771276595744683, 0.35831117027617515, 0.36476063855150914, 0.35398936176553686, 0.3463430851697922, 0.35565159580809, 0.3588430849795646, 0.3564494683387432, 0.3632313831055418, 0.3526595745314943, 0.36103723404255317, 0.3650930853600198, 0.3587765959983176, 0.36063829793574964, 0.358444149189807, 0.3620345744680851, 0.35811170238129636, 0.36436170212765956, 0.3589760638931964, 0.3615026594476497, 0.3596409575102177, 0.3577792555727857, 0.36316489349020287, 0.3670877662110836, 0.3617686172749134, 0.35578457472172187, 0.35538563829787234, 0.35478723429618997, 0.3654255321685304, 0.3637632981259772, 0.3664228725940623, 0.35924202140341416, 0.36476063855150914, 0.36043883004087085, 0.3586436170846858, 0.3597739364238495, 0.35917553194659824, 0.3565159574785131, 0.36343085131746655, 0.35658244693532903, 0.3783909577004453, 0.365824467958288, 0.36575797897704104, 0.36070478723404253, 0.3695478724672439, 0.3664228725940623, 0.36888297885022264, 0.3719414897421573, 0.3668218083838199, 0.368550532041712, 0.3668218083838199, 0.3642287235310737, 0.36343085131746655, 0.3675531916161801, 0.3652260639566056, 0.3668218083838199, 0.35751329774552204, 0.3742686174017318, 0.37320478736086093, 0.3660904257855517, 0.3533909574468085, 0.36003989361702127, 0.3546542553825581, 0.36748670200084116, 0.3704787232774369, 0.3678191488093518, 0.3728723405523503, 0.3636968085106383, 0.37313829774552204, 0.36768617021276595, 0.36668882978723405, 0.36775265982810484, 0.3692154256587333, 0.3640957449344879, 0.36436170212765956, 0.37466755319148937, 0.3708776597012865, 0.3748005317880752, 0.37593085144428495, 0.3761303190221178, 0.36449468072424546, 0.3672207448076695, 0.3670877662110836, 0.3650930853600198, 0.3648271275327561, 0.368550532041712, 0.3704787232774369, 0.36083776583062843, 0.36123670225447796, 0.3690159574468085, 0.36728723442300837, 0.36569148936170215, 0.36449468072424546, 0.37200797872340424, 0.3661569147667986, 0.36961436208258286, 0.3671542551923305, 0.3699468088910935, 0.36675531940257294, 0.37154255331830777, 0.3675531916161801, 0.3642287235310737, 0.363962766337902, 0.3653590425531915, 0.3666223408059871, 0.363962766337902, 0.3629654259123701, 0.36316489349020287, 0.36456117033958435, 0.3662234043821375, 0.3598404254050965, 0.36688829799915884, 0.36788563842469074, 0.3738696809778822, 0.36283244668169223, 0.3597739364238495, 0.3605053190221178, 0.3588430849795646, 0.3674202130195942, 0.3615691490629886, 0.3683510638297872, 0.3553191493166254, 0.36030585144428495, 0.3619680854868382, 0.3551196811047006, 0.3586436170846858, 0.36356382991405245, 0.35924202140341416, 0.3591755317880752, 0.3550531914893617, 0.35831117027617515, 0.3547207446808511, 0.36675531940257294, 0.36003989361702127, 0.35345744706214743, 0.3553191489995794, 0.35212765982810484, 0.355718085106383, 0.36083776583062843, 0.36117021263913907, 0.36117021263913907, 0.350199467958288, 0.3438164896153389, 0.3514627662110836, 0.3587765959983176, 0.36648936157530926, 0.3505319147667986, 0.3594414896153389, 0.35651595731999014]