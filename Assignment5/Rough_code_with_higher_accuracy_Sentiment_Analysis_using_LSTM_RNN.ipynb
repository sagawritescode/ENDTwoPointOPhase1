{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sagar: Sentiment Analysis using LSTM RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagawritescode/ENDTwoPointOPhase1/blob/main/Sagar_Sentiment_Analysis_using_LSTM_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# 0 TorchText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5IzBGsPGHs"
      },
      "source": [
        "## Dataset Preview\n",
        "\n",
        "Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines. \n",
        "\n",
        "We will be using previous session tweet dataset. Let's just preview the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwd9mTxm2fBj"
      },
      "source": [
        "import random\n",
        "import torch, torchtext.legacy\n",
        "from torchtext.legacy import data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrdMw3L8oRlC"
      },
      "source": [
        "PyTreebank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-diBAzDzjvDH",
        "outputId": "440d217b-2643-4112-c43c-ca0e85c10e6f"
      },
      "source": [
        "!pip install pytreebank"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytreebank\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/12/626ead6f6c0a0a9617396796b965961e9dfa5e78b36c17a81ea4c43554b1/pytreebank-0.2.7.tar.gz\n",
            "Building wheels for collected packages: pytreebank\n",
            "  Building wheel for pytreebank (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytreebank: filename=pytreebank-0.2.7-cp37-none-any.whl size=37070 sha256=6f7f1cef95bc243d565ebc1a848264a4b7596c5c7875a1d32f32653a9bc19c73\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/b6/91/e9edcdbf464f623628d5c3aa9de28888c726e270b9a29f2368\n",
            "Successfully built pytreebank\n",
            "Installing collected packages: pytreebank\n",
            "Successfully installed pytreebank-0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg3cweb21wqO"
      },
      "source": [
        "import pytreebank\n",
        "dataset = pytreebank.load_sst()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Um8focl2aAD"
      },
      "source": [
        "Review = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ParVL3_2qSE"
      },
      "source": [
        "fields = [('reviews', Review),('labels',Label)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY_oq21v2A8H"
      },
      "source": [
        "# Use list to collect data as it is more efficient for appending\n",
        "totaldata = []\n",
        "\n",
        "for databatch in [dataset[\"train\"], dataset[\"dev\"], dataset[\"test\"]]:\n",
        "  for example in databatch:\n",
        "    i= 0\n",
        "    for label, sentence in example.to_labeled_lines():\n",
        "      if i == 0:\n",
        "        totaldata.append([sentence, label])\n",
        "        break"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67DcUWictLlV"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining\n",
        " \n",
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ1pQqb_mWt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c260cbe4-cc98-4d91-9b1a-ab1bdf20fc2b"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.7MB/s \n",
            "\u001b[?25hCollecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 25.1MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=605042fa2e3cc6295920f5e8d6ba87d63e7e4cfefa483ef04506911964006d4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hstspreload, sniffio, h11, hpack, hyperframe, h2, httpcore, rfc3986, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRHkaRMDmCFP"
      },
      "source": [
        "import random\n",
        "import googletrans\n",
        "\n",
        "def backtranslate(sentence):\n",
        "  translator = googletrans.Translator()\n",
        "\n",
        "  available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "  trans_lang = random.choice(available_langs) \n",
        "  #print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "  translations = translator.translate(sentence, dest=trans_lang).text \n",
        "  #print(translations)\n",
        "\n",
        "  translations_en_random = translator.translate(translations, src=trans_lang, dest='en').text\n",
        "  #en_text = [t.text for t in translations_en_random]\n",
        "  return translations_en_random\n",
        "  print(en_text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfDAFB_-pVkm",
        "outputId": "dce1a659-2e37-4265-aeac-15020ea30647"
      },
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def print_frequency(data):\n",
        "  x, y = list(zip(*data))\n",
        "  # frequency of labels\n",
        "  label_frequency = collections.Counter(y)\n",
        "  print(\"frequency distribution: \", label_frequency)\n",
        "\n",
        "print_frequency(totaldata)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequency distribution:  Counter({1: 3140, 3: 3111, 2: 2242, 4: 1852, 0: 1510})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO34E_--iGCd"
      },
      "source": [
        "split_percentage = 0.75"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM9PzULSi8zk"
      },
      "source": [
        "def augment_data(sentence):\n",
        "  n = random.randint(1,3)\n",
        "  if n == 1:\n",
        "    sentence = ''.join(random_deletion(list(sentence)))\n",
        "  elif n == 2:\n",
        "    sentence = ''.join(random_swap(list(sentence)))\n",
        "  else:\n",
        "    sentence = backtranslate(sentence)\n",
        "  return sentence\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9gZjT_4Iat3"
      },
      "source": [
        "def get_label_elements(trainlist, label):\n",
        "   return [x for x in trainlist if x[1] == label]\n",
        "\n",
        "def get_int_index(split, list_to_sample):\n",
        "  return int(split*len(list_to_sample))"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT_HFdjUam15",
        "outputId": "bef2da4d-50a5-434c-e6e3-47c2e4f93af1"
      },
      "source": [
        "import random\n",
        "random.shuffle(totaldata)\n",
        "\n",
        "split_index = get_int_index(split_percentage,totaldata )\n",
        "\n",
        "trainlist = totaldata[:split_index]\n",
        "testlist = totaldata[split_index:]\n",
        "\n",
        "iterator = 0\n",
        " \n",
        "changedsent = 0\n",
        "length = len(trainlist)\n",
        "iterator = 0\n",
        "\n",
        "print_frequency(trainlist)"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequency distribution:  Counter({1: 2360, 3: 2317, 2: 1672, 4: 1392, 0: 1150})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l18M_8eon9Pz",
        "outputId": "a927b1e2-d4a3-425e-fa51-56b001643908"
      },
      "source": [
        "train_list_aug = random.sample(trainlist, int(augmentation_percentage*len(trainlist)))\n",
        "\n",
        "train_very_negative = get_label_elements(trainlist, 0)\n",
        "train_negative = get_label_elements(trainlist, 1)\n",
        "train_neutral = get_label_elements(trainlist, 2)\n",
        "train_positive = get_label_elements(trainlist, 3)\n",
        "train_very_positive = get_label_elements(trainlist, 4)\n",
        "\n",
        "train_list_aug = [x for label_list in [random.sample(train_very_negative, get_int_index(1, train_very_negative)), \n",
        "                              random.sample(train_negative, get_int_index(0.2, train_negative)), random.sample(train_neutral, get_int_index(0.2, train_neutral)), \n",
        "                              random.sample(train_positive, get_int_index(0.5, train_positive)), random.sample(train_very_positive, get_int_index(0.8, train_very_positive))] for x in label_list ]\n",
        "len(train_list_aug)\n",
        "\n",
        "for i in train_list_aug:\n",
        "    if iterator%100 == 0:\n",
        "        print(\"sentences changed: \", iterator)\n",
        "    iterator +=1\n",
        "    x,y = i\n",
        "    x = augment_data(x)\n",
        "    trainlist.append((x, y))\n",
        "\n",
        "print_frequency(trainlist)\n"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentences changed:  0\n",
            "sentences changed:  100\n",
            "sentences changed:  200\n",
            "sentences changed:  300\n",
            "sentences changed:  400\n",
            "sentences changed:  500\n",
            "sentences changed:  600\n",
            "sentences changed:  700\n",
            "sentences changed:  800\n",
            "sentences changed:  900\n",
            "sentences changed:  1000\n",
            "sentences changed:  1100\n",
            "sentences changed:  1200\n",
            "sentences changed:  1300\n",
            "sentences changed:  1400\n",
            "sentences changed:  1500\n",
            "sentences changed:  1600\n",
            "sentences changed:  1700\n",
            "sentences changed:  1800\n",
            "sentences changed:  1900\n",
            "sentences changed:  2000\n",
            "sentences changed:  2100\n",
            "sentences changed:  2200\n",
            "sentences changed:  2300\n",
            "sentences changed:  2400\n",
            "sentences changed:  2500\n",
            "sentences changed:  2600\n",
            "sentences changed:  2700\n",
            "sentences changed:  2800\n",
            "sentences changed:  2900\n",
            "sentences changed:  3000\n",
            "sentences changed:  3100\n",
            "sentences changed:  3200\n",
            "sentences changed:  3300\n",
            "sentences changed:  3400\n",
            "sentences changed:  3500\n",
            "sentences changed:  3600\n",
            "sentences changed:  3700\n",
            "sentences changed:  3800\n",
            "sentences changed:  3900\n",
            "sentences changed:  4000\n",
            "sentences changed:  4100\n",
            "sentences changed:  4200\n",
            "frequency distribution:  Counter({3: 3475, 1: 2832, 4: 2505, 0: 2300, 2: 2006})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNQDbXqtEm1F"
      },
      "source": [
        "example1 = [data.Example.fromlist([trainlist[i][0], trainlist[i][1]], fields) for i in range(len(trainlist))]"
      ],
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6lvWT4RNCRB"
      },
      "source": [
        "example2 = [data.Example.fromlist([testlist[i][0], testlist[i][1]], fields) for i in range(len(testlist))]"
      ],
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq-J1txtFncT",
        "outputId": "87312e4c-a3a1-41e9-8cd8-c6df5944bd8b"
      },
      "source": [
        "SEED = 230\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2a6b5d9b50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKj-ztcvFxNr"
      },
      "source": [
        "train = data.Dataset(example1, fields)\n",
        "valid = data.Dataset(example2, fields)"
      ],
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeXWILQzJZQR",
        "outputId": "4cb5310b-e38b-48f9-a948-d971b27b8816"
      },
      "source": [
        "(len(train), len(valid))"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14009, 2964)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAzDVixJjth4"
      },
      "source": [
        "Review.build_vocab(train)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8ER7VMIIRrE",
        "outputId": "9e9bc5b3-e04d-4e52-998d-a3a9b0836791"
      },
      "source": [
        "print('Size of input vocab : ', len(Review.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Review.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)\n",
        "\n",
        "print('label freq:', list(Label.vocab.freqs.most_common(6)))"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  34974\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 12238), (',', 11137), ('the', 8499), ('a', 6559), ('of', 6303), ('and', 6255), ('to', 4353), ('-', 4272), (\"'s\", 3669), ('is', 3604)]\n",
            "Labels :  defaultdict(None, {1: 0, 3: 1, 2: 2, 4: 3, 0: 4})\n",
            "label freq: [(1, 3055), (3, 3023), (2, 2827), (4, 2776), (0, 2328)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sli3x8PBIgq8"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72zB41yiIjRg"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = 64, \n",
        "                                                            sort_key = lambda x: len(x.reviews),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw-8icYQXuAV",
        "outputId": "31e8a002-5ae1-4ac9-abd4-eedb8ccb2fa8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEo7zJ96XwD8"
      },
      "source": [
        "!cd /content/drive/MyDrive/assign5"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORRSFhHnJk_q"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Review.vocab.stoi, tokens)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJPc6liGxZUT"
      },
      "source": [
        "with open('train.pkl', 'wb') as tokens: \n",
        "    pickle.dump(train.examples, tokens)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxWe4aFJxnWP"
      },
      "source": [
        "with open('valid.pkl', 'wb') as tokens: \n",
        "    pickle.dump(valid.examples, tokens)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISw_Sux5Jrgk"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        " \n",
        "class classifierLSTM(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True,\n",
        "                           bidirectional = False\n",
        "                           )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        embedded = self.dropout(embedded)\n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "\n",
        "        dense_outputs = self.dropout(dense_outputs)\n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output#dense_outputs[0]"
      ],
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je_qE7DaZ27m"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        " \n",
        "class classifierLSTM_v2(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True,\n",
        "                           bidirectional = False\n",
        "                           )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc1 = nn.Linear(hidden_dim, output_dim*5)\n",
        "\n",
        "        self.fc2 = nn.Linear(output_dim*5, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        embedded = self.dropout(embedded)\n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs1 = self.fc1(hidden)   \n",
        "\n",
        "        dense_outputs1 = self.dropout(dense_outputs1)\n",
        "\n",
        "        dense_outputs = self.fc2(dense_outputs1)\n",
        "        dense_outputs = self.dropout(dense_outputs)\n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output#dense_outputs[0]"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuYNXwUnJ0QS"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Review.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = 5\n",
        "num_layers = 2\n",
        "dropout = 0.35\n",
        "lr = 1e-4\n",
        "# Instantiate the model\n",
        "model = classifierLSTM(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPhOqXivKb6Z",
        "outputId": "39d56320-29da-4b59-c38e-c848cbc3cfbf"
      },
      "source": [
        "print(model)\n",
        " \n",
        "#No. of trainable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierLSTM(\n",
            "  (embedding): Embedding(34974, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.35)\n",
            "  (dropout): Dropout(p=0.35, inplace=False)\n",
            "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
            ")\n",
            "The model has 10,734,305 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBmoKGbXKi9y"
      },
      "source": [
        "import torch.optim as optim\n",
        " \n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHE561Q4iij_"
      },
      "source": [
        "def get_pred_vs_act_per_label(pred_act):\n",
        "  preds, act = list(zip(*pred_act))\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  for i in preds:\n",
        "    _, pred = torch.max(i, 1)\n",
        "    for j in pred:\n",
        "      predictions.append(j.item())\n",
        "  for i in act:\n",
        "    for j in i:\n",
        "      actuals.append(j.item())\n",
        "\n",
        "  assert len(predictions) == len(actuals)\n",
        "\n",
        "  label_dict = {}\n",
        "  \n",
        "  for i in range(0, len(actuals)):\n",
        "    predic = predictions[i]\n",
        "    actual = actuals[i]\n",
        "    if label_dict.get(actual, None) != None:\n",
        "      in_dict = label_dict[actual]\n",
        "      if in_dict.get(predic, None) != None:\n",
        "        in_dict[predic] += 1\n",
        "      else:\n",
        "        in_dict[predic] = 1\n",
        "    else:\n",
        "      in_dict = {predic: 1}\n",
        "    label_dict[actual] = in_dict\n",
        "  print(\"label distribution: actual vs pred:\", label_dict)\n",
        "  return label_dict"
      ],
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz5LzuJOKtSb"
      },
      "source": [
        "import torch.nn.functional as Fun\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        review, review_lengths = batch.reviews   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        beforesqueeze = model(review, review_lengths)  \n",
        "        predictions = beforesqueeze.squeeze()\n",
        "        # compute the loss       \n",
        "        \n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ll9CV0Ku7O"
      },
      "source": [
        "def evaluate(model, iterator, criterion, preds_actual_tup):\n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        " \n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            review, review_lengths = batch.reviews\n",
        "            #print(review)\n",
        "            # convert to 1d tensor\n",
        "            predictions = model(review, review_lengths).squeeze()\n",
        "            #print(predictions)\n",
        "            # compute loss and accuracy\n",
        "            preds_actual_tup.append((predictions, batch.labels))\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "\n",
        "            #print(batch.labels)\n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKfRzd9YKz1D",
        "outputId": "bca5f180-c9d2-46c3-bad7-ff892664c17b"
      },
      "source": [
        "N_EPOCHS = 300\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "\n",
        "epochi = 0\n",
        "train_acc_list = []\n",
        "valid_acc_list = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    epochi += 1 \n",
        "    \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    preds_actual_tup = []\n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, preds_actual_tup)\n",
        "    valid_acc_list.append(valid_acc)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    print(epochi)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')\n",
        "    if epochi%20 == 0:\n",
        "      print(f'\\t pred vs actuals: {get_pred_vs_act_per_label(preds_actual_tup)}')"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "\tTrain Loss: 1.607 | Train Acc: 22.11%\n",
            "\t Val. Loss: 1.593 |  Val. Acc: 26.97% \n",
            "\n",
            "2\n",
            "\tTrain Loss: 1.605 | Train Acc: 22.50%\n",
            "\t Val. Loss: 1.589 |  Val. Acc: 27.76% \n",
            "\n",
            "3\n",
            "\tTrain Loss: 1.604 | Train Acc: 23.13%\n",
            "\t Val. Loss: 1.588 |  Val. Acc: 28.59% \n",
            "\n",
            "4\n",
            "\tTrain Loss: 1.603 | Train Acc: 23.62%\n",
            "\t Val. Loss: 1.585 |  Val. Acc: 29.61% \n",
            "\n",
            "5\n",
            "\tTrain Loss: 1.601 | Train Acc: 24.35%\n",
            "\t Val. Loss: 1.584 |  Val. Acc: 29.16% \n",
            "\n",
            "6\n",
            "\tTrain Loss: 1.598 | Train Acc: 25.25%\n",
            "\t Val. Loss: 1.581 |  Val. Acc: 29.57% \n",
            "\n",
            "7\n",
            "\tTrain Loss: 1.596 | Train Acc: 25.77%\n",
            "\t Val. Loss: 1.579 |  Val. Acc: 29.93% \n",
            "\n",
            "8\n",
            "\tTrain Loss: 1.592 | Train Acc: 26.73%\n",
            "\t Val. Loss: 1.576 |  Val. Acc: 30.80% \n",
            "\n",
            "9\n",
            "\tTrain Loss: 1.586 | Train Acc: 27.76%\n",
            "\t Val. Loss: 1.570 |  Val. Acc: 30.23% \n",
            "\n",
            "10\n",
            "\tTrain Loss: 1.579 | Train Acc: 28.54%\n",
            "\t Val. Loss: 1.570 |  Val. Acc: 30.21% \n",
            "\n",
            "11\n",
            "\tTrain Loss: 1.571 | Train Acc: 30.07%\n",
            "\t Val. Loss: 1.567 |  Val. Acc: 30.28% \n",
            "\n",
            "12\n",
            "\tTrain Loss: 1.566 | Train Acc: 30.35%\n",
            "\t Val. Loss: 1.564 |  Val. Acc: 30.15% \n",
            "\n",
            "13\n",
            "\tTrain Loss: 1.559 | Train Acc: 31.26%\n",
            "\t Val. Loss: 1.555 |  Val. Acc: 31.15% \n",
            "\n",
            "14\n",
            "\tTrain Loss: 1.551 | Train Acc: 32.53%\n",
            "\t Val. Loss: 1.561 |  Val. Acc: 31.28% \n",
            "\n",
            "15\n",
            "\tTrain Loss: 1.546 | Train Acc: 33.22%\n",
            "\t Val. Loss: 1.558 |  Val. Acc: 31.41% \n",
            "\n",
            "16\n",
            "\tTrain Loss: 1.538 | Train Acc: 34.04%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 33.64% \n",
            "\n",
            "17\n",
            "\tTrain Loss: 1.530 | Train Acc: 34.59%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 33.82% \n",
            "\n",
            "18\n",
            "\tTrain Loss: 1.525 | Train Acc: 35.59%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 32.85% \n",
            "\n",
            "19\n",
            "\tTrain Loss: 1.522 | Train Acc: 35.67%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 34.69% \n",
            "\n",
            "20\n",
            "\tTrain Loss: 1.515 | Train Acc: 36.84%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 34.92% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 64, 1: 225, 0: 400, 4: 16, 3: 85}, 4: {3: 45, 2: 17, 0: 175, 1: 102, 4: 7}, 1: {2: 40, 1: 356, 0: 173, 4: 11, 3: 205}, 3: {3: 208, 2: 18, 4: 2, 1: 175, 0: 61}, 2: {0: 247, 1: 186, 2: 64, 4: 4, 3: 78}}\n",
            "\t pred vs actuals: {0: {2: 64, 1: 225, 0: 400, 4: 16, 3: 85}, 4: {3: 45, 2: 17, 0: 175, 1: 102, 4: 7}, 1: {2: 40, 1: 356, 0: 173, 4: 11, 3: 205}, 3: {3: 208, 2: 18, 4: 2, 1: 175, 0: 61}, 2: {0: 247, 1: 186, 2: 64, 4: 4, 3: 78}}\n",
            "21\n",
            "\tTrain Loss: 1.509 | Train Acc: 37.29%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 35.11% \n",
            "\n",
            "22\n",
            "\tTrain Loss: 1.507 | Train Acc: 37.89%\n",
            "\t Val. Loss: 1.527 |  Val. Acc: 35.65% \n",
            "\n",
            "23\n",
            "\tTrain Loss: 1.500 | Train Acc: 38.09%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 35.05% \n",
            "\n",
            "24\n",
            "\tTrain Loss: 1.500 | Train Acc: 38.12%\n",
            "\t Val. Loss: 1.528 |  Val. Acc: 36.08% \n",
            "\n",
            "25\n",
            "\tTrain Loss: 1.493 | Train Acc: 39.43%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 34.75% \n",
            "\n",
            "26\n",
            "\tTrain Loss: 1.482 | Train Acc: 40.47%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 35.05% \n",
            "\n",
            "27\n",
            "\tTrain Loss: 1.483 | Train Acc: 40.00%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 35.32% \n",
            "\n",
            "28\n",
            "\tTrain Loss: 1.479 | Train Acc: 40.48%\n",
            "\t Val. Loss: 1.526 |  Val. Acc: 35.85% \n",
            "\n",
            "29\n",
            "\tTrain Loss: 1.472 | Train Acc: 41.41%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 35.13% \n",
            "\n",
            "30\n",
            "\tTrain Loss: 1.471 | Train Acc: 41.54%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 35.46% \n",
            "\n",
            "31\n",
            "\tTrain Loss: 1.466 | Train Acc: 41.86%\n",
            "\t Val. Loss: 1.522 |  Val. Acc: 36.49% \n",
            "\n",
            "32\n",
            "\tTrain Loss: 1.461 | Train Acc: 42.62%\n",
            "\t Val. Loss: 1.525 |  Val. Acc: 36.09% \n",
            "\n",
            "33\n",
            "\tTrain Loss: 1.457 | Train Acc: 43.78%\n",
            "\t Val. Loss: 1.523 |  Val. Acc: 36.50% \n",
            "\n",
            "34\n",
            "\tTrain Loss: 1.455 | Train Acc: 43.12%\n",
            "\t Val. Loss: 1.524 |  Val. Acc: 36.29% \n",
            "\n",
            "35\n",
            "\tTrain Loss: 1.444 | Train Acc: 44.83%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 37.33% \n",
            "\n",
            "36\n",
            "\tTrain Loss: 1.443 | Train Acc: 44.57%\n",
            "\t Val. Loss: 1.525 |  Val. Acc: 35.99% \n",
            "\n",
            "37\n",
            "\tTrain Loss: 1.437 | Train Acc: 45.42%\n",
            "\t Val. Loss: 1.528 |  Val. Acc: 35.76% \n",
            "\n",
            "38\n",
            "\tTrain Loss: 1.436 | Train Acc: 45.47%\n",
            "\t Val. Loss: 1.527 |  Val. Acc: 36.02% \n",
            "\n",
            "39\n",
            "\tTrain Loss: 1.433 | Train Acc: 46.02%\n",
            "\t Val. Loss: 1.524 |  Val. Acc: 35.93% \n",
            "\n",
            "40\n",
            "\tTrain Loss: 1.428 | Train Acc: 46.50%\n",
            "\t Val. Loss: 1.524 |  Val. Acc: 35.93% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 66, 4: 57, 1: 235, 0: 381, 3: 51}, 4: {4: 34, 2: 29, 1: 95, 3: 19, 0: 169}, 1: {1: 382, 2: 50, 4: 28, 3: 182, 0: 143}, 3: {3: 187, 2: 24, 4: 12, 1: 201, 0: 40}, 2: {2: 77, 1: 201, 4: 19, 0: 226, 3: 56}}\n",
            "\t pred vs actuals: {0: {2: 66, 4: 57, 1: 235, 0: 381, 3: 51}, 4: {4: 34, 2: 29, 1: 95, 3: 19, 0: 169}, 1: {1: 382, 2: 50, 4: 28, 3: 182, 0: 143}, 3: {3: 187, 2: 24, 4: 12, 1: 201, 0: 40}, 2: {2: 77, 1: 201, 4: 19, 0: 226, 3: 56}}\n",
            "41\n",
            "\tTrain Loss: 1.425 | Train Acc: 46.99%\n",
            "\t Val. Loss: 1.522 |  Val. Acc: 36.42% \n",
            "\n",
            "42\n",
            "\tTrain Loss: 1.419 | Train Acc: 47.51%\n",
            "\t Val. Loss: 1.527 |  Val. Acc: 35.86% \n",
            "\n",
            "43\n",
            "\tTrain Loss: 1.414 | Train Acc: 48.29%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.32% \n",
            "\n",
            "44\n",
            "\tTrain Loss: 1.411 | Train Acc: 48.54%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 37.29% \n",
            "\n",
            "45\n",
            "\tTrain Loss: 1.408 | Train Acc: 48.92%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 36.85% \n",
            "\n",
            "46\n",
            "\tTrain Loss: 1.405 | Train Acc: 49.25%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 37.13% \n",
            "\n",
            "47\n",
            "\tTrain Loss: 1.402 | Train Acc: 49.03%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 36.96% \n",
            "\n",
            "48\n",
            "\tTrain Loss: 1.400 | Train Acc: 49.52%\n",
            "\t Val. Loss: 1.521 |  Val. Acc: 36.48% \n",
            "\n",
            "49\n",
            "\tTrain Loss: 1.394 | Train Acc: 50.17%\n",
            "\t Val. Loss: 1.520 |  Val. Acc: 37.37% \n",
            "\n",
            "50\n",
            "\tTrain Loss: 1.392 | Train Acc: 50.62%\n",
            "\t Val. Loss: 1.525 |  Val. Acc: 36.12% \n",
            "\n",
            "51\n",
            "\tTrain Loss: 1.389 | Train Acc: 50.88%\n",
            "\t Val. Loss: 1.520 |  Val. Acc: 36.95% \n",
            "\n",
            "52\n",
            "\tTrain Loss: 1.388 | Train Acc: 51.02%\n",
            "\t Val. Loss: 1.521 |  Val. Acc: 36.95% \n",
            "\n",
            "53\n",
            "\tTrain Loss: 1.379 | Train Acc: 52.03%\n",
            "\t Val. Loss: 1.523 |  Val. Acc: 35.98% \n",
            "\n",
            "54\n",
            "\tTrain Loss: 1.377 | Train Acc: 52.32%\n",
            "\t Val. Loss: 1.521 |  Val. Acc: 36.58% \n",
            "\n",
            "55\n",
            "\tTrain Loss: 1.377 | Train Acc: 52.33%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 36.56% \n",
            "\n",
            "56\n",
            "\tTrain Loss: 1.370 | Train Acc: 52.91%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.06% \n",
            "\n",
            "57\n",
            "\tTrain Loss: 1.371 | Train Acc: 52.56%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.25% \n",
            "\n",
            "58\n",
            "\tTrain Loss: 1.367 | Train Acc: 52.97%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.29% \n",
            "\n",
            "59\n",
            "\tTrain Loss: 1.365 | Train Acc: 53.14%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.29% \n",
            "\n",
            "60\n",
            "\tTrain Loss: 1.359 | Train Acc: 53.95%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 37.52% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 101, 4: 68, 1: 213, 0: 376, 3: 32}, 4: {4: 50, 1: 67, 2: 51, 0: 169, 3: 9}, 1: {1: 398, 4: 29, 2: 82, 3: 153, 0: 123}, 3: {3: 180, 1: 210, 4: 14, 2: 23, 0: 37}, 2: {2: 107, 1: 193, 0: 206, 4: 28, 3: 45}}\n",
            "\t pred vs actuals: {0: {2: 101, 4: 68, 1: 213, 0: 376, 3: 32}, 4: {4: 50, 1: 67, 2: 51, 0: 169, 3: 9}, 1: {1: 398, 4: 29, 2: 82, 3: 153, 0: 123}, 3: {3: 180, 1: 210, 4: 14, 2: 23, 0: 37}, 2: {2: 107, 1: 193, 0: 206, 4: 28, 3: 45}}\n",
            "61\n",
            "\tTrain Loss: 1.357 | Train Acc: 54.30%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.29% \n",
            "\n",
            "62\n",
            "\tTrain Loss: 1.352 | Train Acc: 54.72%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.69% \n",
            "\n",
            "63\n",
            "\tTrain Loss: 1.348 | Train Acc: 54.98%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 37.23% \n",
            "\n",
            "64\n",
            "\tTrain Loss: 1.344 | Train Acc: 55.79%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 37.03% \n",
            "\n",
            "65\n",
            "\tTrain Loss: 1.344 | Train Acc: 55.99%\n",
            "\t Val. Loss: 1.520 |  Val. Acc: 36.65% \n",
            "\n",
            "66\n",
            "\tTrain Loss: 1.343 | Train Acc: 55.39%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.46% \n",
            "\n",
            "67\n",
            "\tTrain Loss: 1.338 | Train Acc: 56.17%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.19% \n",
            "\n",
            "68\n",
            "\tTrain Loss: 1.338 | Train Acc: 56.37%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.30% \n",
            "\n",
            "69\n",
            "\tTrain Loss: 1.330 | Train Acc: 57.08%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 37.26% \n",
            "\n",
            "70\n",
            "\tTrain Loss: 1.329 | Train Acc: 57.29%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.01% \n",
            "\n",
            "71\n",
            "\tTrain Loss: 1.328 | Train Acc: 57.28%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 36.80% \n",
            "\n",
            "72\n",
            "\tTrain Loss: 1.324 | Train Acc: 57.29%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.71% \n",
            "\n",
            "73\n",
            "\tTrain Loss: 1.321 | Train Acc: 58.02%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.10% \n",
            "\n",
            "74\n",
            "\tTrain Loss: 1.321 | Train Acc: 58.17%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.44% \n",
            "\n",
            "75\n",
            "\tTrain Loss: 1.314 | Train Acc: 58.45%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 37.67% \n",
            "\n",
            "76\n",
            "\tTrain Loss: 1.315 | Train Acc: 58.30%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 36.91% \n",
            "\n",
            "77\n",
            "\tTrain Loss: 1.318 | Train Acc: 58.44%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.13% \n",
            "\n",
            "78\n",
            "\tTrain Loss: 1.309 | Train Acc: 59.09%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.17% \n",
            "\n",
            "79\n",
            "\tTrain Loss: 1.305 | Train Acc: 59.77%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 38.15% \n",
            "\n",
            "80\n",
            "\tTrain Loss: 1.301 | Train Acc: 59.79%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.81% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 126, 1: 191, 4: 85, 0: 354, 3: 34}, 4: {4: 67, 1: 50, 2: 53, 0: 162, 3: 14}, 1: {1: 366, 0: 120, 3: 155, 2: 114, 4: 30}, 3: {3: 198, 2: 40, 1: 180, 4: 13, 0: 33}, 2: {2: 128, 1: 167, 0: 215, 4: 25, 3: 44}}\n",
            "\t pred vs actuals: {0: {2: 126, 1: 191, 4: 85, 0: 354, 3: 34}, 4: {4: 67, 1: 50, 2: 53, 0: 162, 3: 14}, 1: {1: 366, 0: 120, 3: 155, 2: 114, 4: 30}, 3: {3: 198, 2: 40, 1: 180, 4: 13, 0: 33}, 2: {2: 128, 1: 167, 0: 215, 4: 25, 3: 44}}\n",
            "81\n",
            "\tTrain Loss: 1.300 | Train Acc: 59.73%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.71% \n",
            "\n",
            "82\n",
            "\tTrain Loss: 1.301 | Train Acc: 60.13%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 37.75% \n",
            "\n",
            "83\n",
            "\tTrain Loss: 1.297 | Train Acc: 60.30%\n",
            "\t Val. Loss: 1.505 |  Val. Acc: 38.24% \n",
            "\n",
            "84\n",
            "\tTrain Loss: 1.293 | Train Acc: 60.54%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 37.89% \n",
            "\n",
            "85\n",
            "\tTrain Loss: 1.292 | Train Acc: 61.06%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 38.09% \n",
            "\n",
            "86\n",
            "\tTrain Loss: 1.286 | Train Acc: 61.37%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.61% \n",
            "\n",
            "87\n",
            "\tTrain Loss: 1.282 | Train Acc: 61.60%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.70% \n",
            "\n",
            "88\n",
            "\tTrain Loss: 1.285 | Train Acc: 61.71%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.77% \n",
            "\n",
            "89\n",
            "\tTrain Loss: 1.281 | Train Acc: 62.10%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.01% \n",
            "\n",
            "90\n",
            "\tTrain Loss: 1.286 | Train Acc: 61.32%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.33% \n",
            "\n",
            "91\n",
            "\tTrain Loss: 1.279 | Train Acc: 61.90%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.16% \n",
            "\n",
            "92\n",
            "\tTrain Loss: 1.274 | Train Acc: 62.61%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.24% \n",
            "\n",
            "93\n",
            "\tTrain Loss: 1.277 | Train Acc: 62.29%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 37.94% \n",
            "\n",
            "94\n",
            "\tTrain Loss: 1.277 | Train Acc: 62.27%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.44% \n",
            "\n",
            "95\n",
            "\tTrain Loss: 1.271 | Train Acc: 62.81%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 37.20% \n",
            "\n",
            "96\n",
            "\tTrain Loss: 1.268 | Train Acc: 63.26%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.54% \n",
            "\n",
            "97\n",
            "\tTrain Loss: 1.269 | Train Acc: 62.73%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 37.48% \n",
            "\n",
            "98\n",
            "\tTrain Loss: 1.269 | Train Acc: 63.46%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.94% \n",
            "\n",
            "99\n",
            "\tTrain Loss: 1.265 | Train Acc: 63.73%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.51% \n",
            "\n",
            "100\n",
            "\tTrain Loss: 1.265 | Train Acc: 63.28%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.37% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 126, 1: 216, 4: 86, 0: 336, 3: 26}, 4: {4: 63, 1: 70, 2: 54, 0: 152, 3: 7}, 1: {1: 409, 3: 141, 0: 107, 2: 105, 4: 23}, 3: {3: 179, 1: 212, 4: 10, 2: 29, 0: 34}, 2: {2: 113, 1: 198, 0: 203, 4: 29, 3: 36}}\n",
            "\t pred vs actuals: {0: {2: 126, 1: 216, 4: 86, 0: 336, 3: 26}, 4: {4: 63, 1: 70, 2: 54, 0: 152, 3: 7}, 1: {1: 409, 3: 141, 0: 107, 2: 105, 4: 23}, 3: {3: 179, 1: 212, 4: 10, 2: 29, 0: 34}, 2: {2: 113, 1: 198, 0: 203, 4: 29, 3: 36}}\n",
            "101\n",
            "\tTrain Loss: 1.262 | Train Acc: 64.04%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.37% \n",
            "\n",
            "102\n",
            "\tTrain Loss: 1.256 | Train Acc: 64.80%\n",
            "\t Val. Loss: 1.504 |  Val. Acc: 38.45% \n",
            "\n",
            "103\n",
            "\tTrain Loss: 1.255 | Train Acc: 64.33%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 37.99% \n",
            "\n",
            "104\n",
            "\tTrain Loss: 1.253 | Train Acc: 64.84%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 38.05% \n",
            "\n",
            "105\n",
            "\tTrain Loss: 1.252 | Train Acc: 64.33%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 38.24% \n",
            "\n",
            "106\n",
            "\tTrain Loss: 1.254 | Train Acc: 64.46%\n",
            "\t Val. Loss: 1.506 |  Val. Acc: 38.18% \n",
            "\n",
            "107\n",
            "\tTrain Loss: 1.250 | Train Acc: 64.81%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.52% \n",
            "\n",
            "108\n",
            "\tTrain Loss: 1.248 | Train Acc: 65.40%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.44% \n",
            "\n",
            "109\n",
            "\tTrain Loss: 1.246 | Train Acc: 65.11%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.04% \n",
            "\n",
            "110\n",
            "\tTrain Loss: 1.240 | Train Acc: 65.88%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.31% \n",
            "\n",
            "111\n",
            "\tTrain Loss: 1.245 | Train Acc: 65.44%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.24% \n",
            "\n",
            "112\n",
            "\tTrain Loss: 1.242 | Train Acc: 65.55%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 37.88% \n",
            "\n",
            "113\n",
            "\tTrain Loss: 1.240 | Train Acc: 65.89%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.77% \n",
            "\n",
            "114\n",
            "\tTrain Loss: 1.240 | Train Acc: 66.29%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.31% \n",
            "\n",
            "115\n",
            "\tTrain Loss: 1.238 | Train Acc: 65.97%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.21% \n",
            "\n",
            "116\n",
            "\tTrain Loss: 1.234 | Train Acc: 66.53%\n",
            "\t Val. Loss: 1.521 |  Val. Acc: 36.58% \n",
            "\n",
            "117\n",
            "\tTrain Loss: 1.236 | Train Acc: 66.36%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.57% \n",
            "\n",
            "118\n",
            "\tTrain Loss: 1.232 | Train Acc: 66.50%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.61% \n",
            "\n",
            "119\n",
            "\tTrain Loss: 1.229 | Train Acc: 67.32%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 37.37% \n",
            "\n",
            "120\n",
            "\tTrain Loss: 1.233 | Train Acc: 66.81%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 38.48% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 113, 4: 93, 1: 166, 0: 390, 3: 28}, 4: {4: 69, 1: 57, 2: 42, 0: 172, 3: 6}, 1: {1: 380, 0: 134, 3: 139, 2: 104, 4: 28}, 3: {3: 177, 1: 200, 2: 38, 0: 35, 4: 14}, 2: {2: 113, 1: 185, 0: 221, 4: 30, 3: 30}}\n",
            "\t pred vs actuals: {0: {2: 113, 4: 93, 1: 166, 0: 390, 3: 28}, 4: {4: 69, 1: 57, 2: 42, 0: 172, 3: 6}, 1: {1: 380, 0: 134, 3: 139, 2: 104, 4: 28}, 3: {3: 177, 1: 200, 2: 38, 0: 35, 4: 14}, 2: {2: 113, 1: 185, 0: 221, 4: 30, 3: 30}}\n",
            "121\n",
            "\tTrain Loss: 1.229 | Train Acc: 66.74%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.08% \n",
            "\n",
            "122\n",
            "\tTrain Loss: 1.229 | Train Acc: 66.76%\n",
            "\t Val. Loss: 1.521 |  Val. Acc: 36.81% \n",
            "\n",
            "123\n",
            "\tTrain Loss: 1.223 | Train Acc: 67.58%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.63% \n",
            "\n",
            "124\n",
            "\tTrain Loss: 1.227 | Train Acc: 67.10%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 38.18% \n",
            "\n",
            "125\n",
            "\tTrain Loss: 1.223 | Train Acc: 67.48%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.98% \n",
            "\n",
            "126\n",
            "\tTrain Loss: 1.225 | Train Acc: 67.15%\n",
            "\t Val. Loss: 1.522 |  Val. Acc: 36.36% \n",
            "\n",
            "127\n",
            "\tTrain Loss: 1.220 | Train Acc: 67.97%\n",
            "\t Val. Loss: 1.518 |  Val. Acc: 37.51% \n",
            "\n",
            "128\n",
            "\tTrain Loss: 1.219 | Train Acc: 67.51%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 37.11% \n",
            "\n",
            "129\n",
            "\tTrain Loss: 1.215 | Train Acc: 68.29%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.47% \n",
            "\n",
            "130\n",
            "\tTrain Loss: 1.216 | Train Acc: 68.20%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.50% \n",
            "\n",
            "131\n",
            "\tTrain Loss: 1.217 | Train Acc: 68.09%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.44% \n",
            "\n",
            "132\n",
            "\tTrain Loss: 1.216 | Train Acc: 68.41%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.79% \n",
            "\n",
            "133\n",
            "\tTrain Loss: 1.213 | Train Acc: 68.54%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.01% \n",
            "\n",
            "134\n",
            "\tTrain Loss: 1.211 | Train Acc: 68.78%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 38.28% \n",
            "\n",
            "135\n",
            "\tTrain Loss: 1.211 | Train Acc: 68.88%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.37% \n",
            "\n",
            "136\n",
            "\tTrain Loss: 1.209 | Train Acc: 68.73%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 37.95% \n",
            "\n",
            "137\n",
            "\tTrain Loss: 1.210 | Train Acc: 68.24%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 37.85% \n",
            "\n",
            "138\n",
            "\tTrain Loss: 1.206 | Train Acc: 69.19%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.64% \n",
            "\n",
            "139\n",
            "\tTrain Loss: 1.206 | Train Acc: 69.36%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.08% \n",
            "\n",
            "140\n",
            "\tTrain Loss: 1.203 | Train Acc: 69.74%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 37.25% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 145, 4: 118, 1: 153, 0: 338, 3: 36}, 4: {4: 80, 1: 53, 2: 53, 0: 151, 3: 9}, 1: {1: 352, 2: 119, 3: 171, 0: 113, 4: 30}, 3: {3: 198, 0: 33, 2: 36, 1: 181, 4: 16}, 2: {2: 126, 1: 162, 0: 194, 4: 45, 3: 52}}\n",
            "\t pred vs actuals: {0: {2: 145, 4: 118, 1: 153, 0: 338, 3: 36}, 4: {4: 80, 1: 53, 2: 53, 0: 151, 3: 9}, 1: {1: 352, 2: 119, 3: 171, 0: 113, 4: 30}, 3: {3: 198, 0: 33, 2: 36, 1: 181, 4: 16}, 2: {2: 126, 1: 162, 0: 194, 4: 45, 3: 52}}\n",
            "141\n",
            "\tTrain Loss: 1.205 | Train Acc: 69.27%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.87% \n",
            "\n",
            "142\n",
            "\tTrain Loss: 1.205 | Train Acc: 69.47%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 37.83% \n",
            "\n",
            "143\n",
            "\tTrain Loss: 1.202 | Train Acc: 69.54%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.04% \n",
            "\n",
            "144\n",
            "\tTrain Loss: 1.200 | Train Acc: 69.98%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 37.69% \n",
            "\n",
            "145\n",
            "\tTrain Loss: 1.195 | Train Acc: 70.20%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.18% \n",
            "\n",
            "146\n",
            "\tTrain Loss: 1.198 | Train Acc: 69.85%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.05% \n",
            "\n",
            "147\n",
            "\tTrain Loss: 1.195 | Train Acc: 70.52%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.95% \n",
            "\n",
            "148\n",
            "\tTrain Loss: 1.198 | Train Acc: 69.75%\n",
            "\t Val. Loss: 1.518 |  Val. Acc: 37.31% \n",
            "\n",
            "149\n",
            "\tTrain Loss: 1.196 | Train Acc: 70.24%\n",
            "\t Val. Loss: 1.521 |  Val. Acc: 36.88% \n",
            "\n",
            "150\n",
            "\tTrain Loss: 1.192 | Train Acc: 70.88%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 37.92% \n",
            "\n",
            "151\n",
            "\tTrain Loss: 1.188 | Train Acc: 70.75%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 36.91% \n",
            "\n",
            "152\n",
            "\tTrain Loss: 1.194 | Train Acc: 70.35%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 37.01% \n",
            "\n",
            "153\n",
            "\tTrain Loss: 1.195 | Train Acc: 70.52%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.85% \n",
            "\n",
            "154\n",
            "\tTrain Loss: 1.194 | Train Acc: 70.05%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 37.41% \n",
            "\n",
            "155\n",
            "\tTrain Loss: 1.189 | Train Acc: 70.88%\n",
            "\t Val. Loss: 1.518 |  Val. Acc: 37.28% \n",
            "\n",
            "156\n",
            "\tTrain Loss: 1.190 | Train Acc: 70.99%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 37.27% \n",
            "\n",
            "157\n",
            "\tTrain Loss: 1.187 | Train Acc: 71.35%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 37.41% \n",
            "\n",
            "158\n",
            "\tTrain Loss: 1.184 | Train Acc: 71.60%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 37.58% \n",
            "\n",
            "159\n",
            "\tTrain Loss: 1.186 | Train Acc: 71.03%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.71% \n",
            "\n",
            "160\n",
            "\tTrain Loss: 1.185 | Train Acc: 71.26%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.21% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 129, 4: 113, 1: 132, 0: 388, 3: 28}, 4: {4: 80, 1: 43, 2: 45, 0: 170, 3: 8}, 1: {1: 341, 3: 154, 0: 131, 2: 128, 4: 31}, 3: {3: 187, 0: 43, 2: 38, 1: 184, 4: 12}, 2: {2: 127, 1: 158, 0: 212, 4: 47, 3: 35}}\n",
            "\t pred vs actuals: {0: {2: 129, 4: 113, 1: 132, 0: 388, 3: 28}, 4: {4: 80, 1: 43, 2: 45, 0: 170, 3: 8}, 1: {1: 341, 3: 154, 0: 131, 2: 128, 4: 31}, 3: {3: 187, 0: 43, 2: 38, 1: 184, 4: 12}, 2: {2: 127, 1: 158, 0: 212, 4: 47, 3: 35}}\n",
            "161\n",
            "\tTrain Loss: 1.181 | Train Acc: 71.54%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.78% \n",
            "\n",
            "162\n",
            "\tTrain Loss: 1.181 | Train Acc: 71.56%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.61% \n",
            "\n",
            "163\n",
            "\tTrain Loss: 1.184 | Train Acc: 71.30%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.61% \n",
            "\n",
            "164\n",
            "\tTrain Loss: 1.181 | Train Acc: 71.71%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 37.54% \n",
            "\n",
            "165\n",
            "\tTrain Loss: 1.179 | Train Acc: 71.78%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 37.40% \n",
            "\n",
            "166\n",
            "\tTrain Loss: 1.174 | Train Acc: 72.35%\n",
            "\t Val. Loss: 1.518 |  Val. Acc: 37.38% \n",
            "\n",
            "167\n",
            "\tTrain Loss: 1.178 | Train Acc: 72.01%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.32% \n",
            "\n",
            "168\n",
            "\tTrain Loss: 1.177 | Train Acc: 72.14%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 38.04% \n",
            "\n",
            "169\n",
            "\tTrain Loss: 1.178 | Train Acc: 71.89%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.45% \n",
            "\n",
            "170\n",
            "\tTrain Loss: 1.171 | Train Acc: 72.67%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.45% \n",
            "\n",
            "171\n",
            "\tTrain Loss: 1.180 | Train Acc: 71.07%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.11% \n",
            "\n",
            "172\n",
            "\tTrain Loss: 1.176 | Train Acc: 71.61%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.31% \n",
            "\n",
            "173\n",
            "\tTrain Loss: 1.172 | Train Acc: 72.64%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.71% \n",
            "\n",
            "174\n",
            "\tTrain Loss: 1.172 | Train Acc: 72.46%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.14% \n",
            "\n",
            "175\n",
            "\tTrain Loss: 1.171 | Train Acc: 72.79%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.61% \n",
            "\n",
            "176\n",
            "\tTrain Loss: 1.170 | Train Acc: 72.90%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.21% \n",
            "\n",
            "177\n",
            "\tTrain Loss: 1.165 | Train Acc: 73.17%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 37.63% \n",
            "\n",
            "178\n",
            "\tTrain Loss: 1.169 | Train Acc: 72.46%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 38.57% \n",
            "\n",
            "179\n",
            "\tTrain Loss: 1.168 | Train Acc: 72.94%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.74% \n",
            "\n",
            "180\n",
            "\tTrain Loss: 1.167 | Train Acc: 73.30%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 38.82% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 148, 4: 106, 1: 152, 0: 357, 3: 27}, 4: {4: 77, 1: 57, 2: 45, 0: 157, 3: 10}, 1: {1: 376, 2: 122, 3: 149, 0: 113, 4: 25}, 3: {3: 192, 0: 35, 2: 34, 1: 193, 4: 10}, 2: {2: 137, 1: 175, 0: 187, 4: 39, 3: 41}}\n",
            "\t pred vs actuals: {0: {2: 148, 4: 106, 1: 152, 0: 357, 3: 27}, 4: {4: 77, 1: 57, 2: 45, 0: 157, 3: 10}, 1: {1: 376, 2: 122, 3: 149, 0: 113, 4: 25}, 3: {3: 192, 0: 35, 2: 34, 1: 193, 4: 10}, 2: {2: 137, 1: 175, 0: 187, 4: 39, 3: 41}}\n",
            "181\n",
            "\tTrain Loss: 1.169 | Train Acc: 72.51%\n",
            "\t Val. Loss: 1.504 |  Val. Acc: 38.95% \n",
            "\n",
            "182\n",
            "\tTrain Loss: 1.164 | Train Acc: 73.41%\n",
            "\t Val. Loss: 1.504 |  Val. Acc: 39.12% \n",
            "\n",
            "183\n",
            "\tTrain Loss: 1.167 | Train Acc: 72.95%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 38.71% \n",
            "\n",
            "184\n",
            "\tTrain Loss: 1.168 | Train Acc: 72.82%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 38.75% \n",
            "\n",
            "185\n",
            "\tTrain Loss: 1.164 | Train Acc: 73.38%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 38.98% \n",
            "\n",
            "186\n",
            "\tTrain Loss: 1.164 | Train Acc: 73.25%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.98% \n",
            "\n",
            "187\n",
            "\tTrain Loss: 1.160 | Train Acc: 73.88%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.37% \n",
            "\n",
            "188\n",
            "\tTrain Loss: 1.165 | Train Acc: 73.06%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 39.05% \n",
            "\n",
            "189\n",
            "\tTrain Loss: 1.156 | Train Acc: 74.07%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.78% \n",
            "\n",
            "190\n",
            "\tTrain Loss: 1.163 | Train Acc: 73.32%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.67% \n",
            "\n",
            "191\n",
            "\tTrain Loss: 1.160 | Train Acc: 73.67%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 39.01% \n",
            "\n",
            "192\n",
            "\tTrain Loss: 1.159 | Train Acc: 73.57%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 38.70% \n",
            "\n",
            "193\n",
            "\tTrain Loss: 1.161 | Train Acc: 73.76%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.24% \n",
            "\n",
            "194\n",
            "\tTrain Loss: 1.160 | Train Acc: 73.46%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 38.13% \n",
            "\n",
            "195\n",
            "\tTrain Loss: 1.158 | Train Acc: 73.78%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.27% \n",
            "\n",
            "196\n",
            "\tTrain Loss: 1.154 | Train Acc: 74.19%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.91% \n",
            "\n",
            "197\n",
            "\tTrain Loss: 1.161 | Train Acc: 73.53%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.48% \n",
            "\n",
            "198\n",
            "\tTrain Loss: 1.159 | Train Acc: 73.37%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.61% \n",
            "\n",
            "199\n",
            "\tTrain Loss: 1.156 | Train Acc: 74.16%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.48% \n",
            "\n",
            "200\n",
            "\tTrain Loss: 1.156 | Train Acc: 74.17%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.75% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 157, 1: 156, 4: 106, 0: 333, 3: 38}, 4: {4: 83, 1: 54, 2: 54, 0: 143, 3: 12}, 1: {1: 363, 3: 168, 0: 100, 2: 128, 4: 26}, 3: {3: 205, 0: 31, 2: 34, 1: 183, 4: 11}, 2: {2: 153, 1: 165, 0: 171, 4: 38, 3: 52}}\n",
            "\t pred vs actuals: {0: {2: 157, 1: 156, 4: 106, 0: 333, 3: 38}, 4: {4: 83, 1: 54, 2: 54, 0: 143, 3: 12}, 1: {1: 363, 3: 168, 0: 100, 2: 128, 4: 26}, 3: {3: 205, 0: 31, 2: 34, 1: 183, 4: 11}, 2: {2: 153, 1: 165, 0: 171, 4: 38, 3: 52}}\n",
            "201\n",
            "\tTrain Loss: 1.152 | Train Acc: 74.37%\n",
            "\t Val. Loss: 1.506 |  Val. Acc: 39.14% \n",
            "\n",
            "202\n",
            "\tTrain Loss: 1.153 | Train Acc: 74.42%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 37.91% \n",
            "\n",
            "203\n",
            "\tTrain Loss: 1.154 | Train Acc: 74.40%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.24% \n",
            "\n",
            "204\n",
            "\tTrain Loss: 1.151 | Train Acc: 74.28%\n",
            "\t Val. Loss: 1.505 |  Val. Acc: 39.14% \n",
            "\n",
            "205\n",
            "\tTrain Loss: 1.151 | Train Acc: 74.85%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 38.94% \n",
            "\n",
            "206\n",
            "\tTrain Loss: 1.151 | Train Acc: 74.84%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.80% \n",
            "\n",
            "207\n",
            "\tTrain Loss: 1.155 | Train Acc: 73.97%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 38.60% \n",
            "\n",
            "208\n",
            "\tTrain Loss: 1.146 | Train Acc: 75.02%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.37% \n",
            "\n",
            "209\n",
            "\tTrain Loss: 1.150 | Train Acc: 74.33%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.30% \n",
            "\n",
            "210\n",
            "\tTrain Loss: 1.150 | Train Acc: 74.58%\n",
            "\t Val. Loss: 1.504 |  Val. Acc: 39.23% \n",
            "\n",
            "211\n",
            "\tTrain Loss: 1.147 | Train Acc: 74.91%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.36% \n",
            "\n",
            "212\n",
            "\tTrain Loss: 1.151 | Train Acc: 74.46%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.76% \n",
            "\n",
            "213\n",
            "\tTrain Loss: 1.146 | Train Acc: 74.87%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.47% \n",
            "\n",
            "214\n",
            "\tTrain Loss: 1.147 | Train Acc: 74.92%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.40% \n",
            "\n",
            "215\n",
            "\tTrain Loss: 1.146 | Train Acc: 75.08%\n",
            "\t Val. Loss: 1.506 |  Val. Acc: 39.11% \n",
            "\n",
            "216\n",
            "\tTrain Loss: 1.147 | Train Acc: 74.76%\n",
            "\t Val. Loss: 1.505 |  Val. Acc: 39.30% \n",
            "\n",
            "217\n",
            "\tTrain Loss: 1.144 | Train Acc: 75.48%\n",
            "\t Val. Loss: 1.506 |  Val. Acc: 39.41% \n",
            "\n",
            "218\n",
            "\tTrain Loss: 1.143 | Train Acc: 75.41%\n",
            "\t Val. Loss: 1.504 |  Val. Acc: 39.34% \n",
            "\n",
            "219\n",
            "\tTrain Loss: 1.144 | Train Acc: 75.01%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.81% \n",
            "\n",
            "220\n",
            "\tTrain Loss: 1.146 | Train Acc: 74.57%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.84% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 161, 0: 361, 1: 144, 4: 93, 3: 31}, 4: {4: 74, 1: 46, 2: 57, 0: 160, 3: 9}, 1: {1: 353, 3: 158, 0: 120, 2: 133, 4: 21}, 3: {3: 194, 0: 35, 2: 33, 1: 193, 4: 9}, 2: {2: 160, 1: 157, 0: 180, 4: 37, 3: 45}}\n",
            "\t pred vs actuals: {0: {2: 161, 0: 361, 1: 144, 4: 93, 3: 31}, 4: {4: 74, 1: 46, 2: 57, 0: 160, 3: 9}, 1: {1: 353, 3: 158, 0: 120, 2: 133, 4: 21}, 3: {3: 194, 0: 35, 2: 33, 1: 193, 4: 9}, 2: {2: 160, 1: 157, 0: 180, 4: 37, 3: 45}}\n",
            "221\n",
            "\tTrain Loss: 1.140 | Train Acc: 75.39%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.51% \n",
            "\n",
            "222\n",
            "\tTrain Loss: 1.139 | Train Acc: 75.64%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.98% \n",
            "\n",
            "223\n",
            "\tTrain Loss: 1.139 | Train Acc: 75.56%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 39.02% \n",
            "\n",
            "224\n",
            "\tTrain Loss: 1.137 | Train Acc: 76.07%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 38.44% \n",
            "\n",
            "225\n",
            "\tTrain Loss: 1.140 | Train Acc: 75.61%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 38.00% \n",
            "\n",
            "226\n",
            "\tTrain Loss: 1.144 | Train Acc: 75.13%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.16% \n",
            "\n",
            "227\n",
            "\tTrain Loss: 1.140 | Train Acc: 75.45%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 38.96% \n",
            "\n",
            "228\n",
            "\tTrain Loss: 1.139 | Train Acc: 75.75%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.66% \n",
            "\n",
            "229\n",
            "\tTrain Loss: 1.138 | Train Acc: 76.08%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.37% \n",
            "\n",
            "230\n",
            "\tTrain Loss: 1.143 | Train Acc: 75.08%\n",
            "\t Val. Loss: 1.505 |  Val. Acc: 39.47% \n",
            "\n",
            "231\n",
            "\tTrain Loss: 1.136 | Train Acc: 75.89%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.62% \n",
            "\n",
            "232\n",
            "\tTrain Loss: 1.139 | Train Acc: 75.55%\n",
            "\t Val. Loss: 1.506 |  Val. Acc: 39.08% \n",
            "\n",
            "233\n",
            "\tTrain Loss: 1.140 | Train Acc: 75.56%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.74% \n",
            "\n",
            "234\n",
            "\tTrain Loss: 1.137 | Train Acc: 75.94%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.61% \n",
            "\n",
            "235\n",
            "\tTrain Loss: 1.138 | Train Acc: 75.77%\n",
            "\t Val. Loss: 1.505 |  Val. Acc: 39.38% \n",
            "\n",
            "236\n",
            "\tTrain Loss: 1.136 | Train Acc: 76.08%\n",
            "\t Val. Loss: 1.504 |  Val. Acc: 39.31% \n",
            "\n",
            "237\n",
            "\tTrain Loss: 1.137 | Train Acc: 76.23%\n",
            "\t Val. Loss: 1.505 |  Val. Acc: 39.41% \n",
            "\n",
            "238\n",
            "\tTrain Loss: 1.135 | Train Acc: 76.01%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.80% \n",
            "\n",
            "239\n",
            "\tTrain Loss: 1.131 | Train Acc: 76.42%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.47% \n",
            "\n",
            "240\n",
            "\tTrain Loss: 1.135 | Train Acc: 76.05%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.40% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 180, 0: 332, 1: 145, 4: 100, 3: 33}, 4: {0: 150, 4: 83, 1: 48, 2: 57, 3: 8}, 1: {1: 371, 2: 129, 3: 154, 0: 103, 4: 28}, 3: {3: 187, 0: 33, 1: 198, 2: 35, 4: 11}, 2: {2: 158, 1: 164, 0: 175, 4: 38, 3: 44}}\n",
            "\t pred vs actuals: {0: {2: 180, 0: 332, 1: 145, 4: 100, 3: 33}, 4: {0: 150, 4: 83, 1: 48, 2: 57, 3: 8}, 1: {1: 371, 2: 129, 3: 154, 0: 103, 4: 28}, 3: {3: 187, 0: 33, 1: 198, 2: 35, 4: 11}, 2: {2: 158, 1: 164, 0: 175, 4: 38, 3: 44}}\n",
            "241\n",
            "\tTrain Loss: 1.129 | Train Acc: 76.85%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.57% \n",
            "\n",
            "242\n",
            "\tTrain Loss: 1.132 | Train Acc: 76.28%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.50% \n",
            "\n",
            "243\n",
            "\tTrain Loss: 1.130 | Train Acc: 76.60%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.38% \n",
            "\n",
            "244\n",
            "\tTrain Loss: 1.133 | Train Acc: 76.28%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.78% \n",
            "\n",
            "245\n",
            "\tTrain Loss: 1.131 | Train Acc: 76.18%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.98% \n",
            "\n",
            "246\n",
            "\tTrain Loss: 1.133 | Train Acc: 76.05%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 39.08% \n",
            "\n",
            "247\n",
            "\tTrain Loss: 1.129 | Train Acc: 76.68%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.44% \n",
            "\n",
            "248\n",
            "\tTrain Loss: 1.132 | Train Acc: 75.98%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.04% \n",
            "\n",
            "249\n",
            "\tTrain Loss: 1.130 | Train Acc: 76.40%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.28% \n",
            "\n",
            "250\n",
            "\tTrain Loss: 1.132 | Train Acc: 76.29%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.71% \n",
            "\n",
            "251\n",
            "\tTrain Loss: 1.129 | Train Acc: 76.35%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 38.01% \n",
            "\n",
            "252\n",
            "\tTrain Loss: 1.128 | Train Acc: 76.52%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 38.74% \n",
            "\n",
            "253\n",
            "\tTrain Loss: 1.128 | Train Acc: 76.36%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 38.07% \n",
            "\n",
            "254\n",
            "\tTrain Loss: 1.125 | Train Acc: 77.07%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 38.05% \n",
            "\n",
            "255\n",
            "\tTrain Loss: 1.124 | Train Acc: 77.21%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.14% \n",
            "\n",
            "256\n",
            "\tTrain Loss: 1.130 | Train Acc: 76.44%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.87% \n",
            "\n",
            "257\n",
            "\tTrain Loss: 1.125 | Train Acc: 76.95%\n",
            "\t Val. Loss: 1.509 |  Val. Acc: 38.75% \n",
            "\n",
            "258\n",
            "\tTrain Loss: 1.126 | Train Acc: 76.86%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 38.05% \n",
            "\n",
            "259\n",
            "\tTrain Loss: 1.124 | Train Acc: 77.03%\n",
            "\t Val. Loss: 1.518 |  Val. Acc: 37.65% \n",
            "\n",
            "260\n",
            "\tTrain Loss: 1.126 | Train Acc: 76.55%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 38.21% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 168, 0: 358, 1: 139, 4: 89, 3: 36}, 4: {4: 74, 1: 50, 2: 51, 0: 161, 3: 10}, 1: {1: 368, 2: 123, 3: 148, 0: 117, 4: 29}, 3: {3: 179, 0: 36, 1: 206, 2: 33, 4: 10}, 2: {2: 144, 1: 178, 0: 182, 4: 39, 3: 36}}\n",
            "\t pred vs actuals: {0: {2: 168, 0: 358, 1: 139, 4: 89, 3: 36}, 4: {4: 74, 1: 50, 2: 51, 0: 161, 3: 10}, 1: {1: 368, 2: 123, 3: 148, 0: 117, 4: 29}, 3: {3: 179, 0: 36, 1: 206, 2: 33, 4: 10}, 2: {2: 144, 1: 178, 0: 182, 4: 39, 3: 36}}\n",
            "261\n",
            "\tTrain Loss: 1.123 | Train Acc: 77.54%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 37.84% \n",
            "\n",
            "262\n",
            "\tTrain Loss: 1.128 | Train Acc: 76.36%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.58% \n",
            "\n",
            "263\n",
            "\tTrain Loss: 1.121 | Train Acc: 77.19%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 37.44% \n",
            "\n",
            "264\n",
            "\tTrain Loss: 1.127 | Train Acc: 76.61%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 38.08% \n",
            "\n",
            "265\n",
            "\tTrain Loss: 1.125 | Train Acc: 77.32%\n",
            "\t Val. Loss: 1.519 |  Val. Acc: 37.24% \n",
            "\n",
            "266\n",
            "\tTrain Loss: 1.124 | Train Acc: 77.09%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 38.27% \n",
            "\n",
            "267\n",
            "\tTrain Loss: 1.126 | Train Acc: 77.16%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.30% \n",
            "\n",
            "268\n",
            "\tTrain Loss: 1.121 | Train Acc: 77.36%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 38.08% \n",
            "\n",
            "269\n",
            "\tTrain Loss: 1.124 | Train Acc: 76.64%\n",
            "\t Val. Loss: 1.518 |  Val. Acc: 37.61% \n",
            "\n",
            "270\n",
            "\tTrain Loss: 1.125 | Train Acc: 77.18%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.11% \n",
            "\n",
            "271\n",
            "\tTrain Loss: 1.124 | Train Acc: 77.24%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.44% \n",
            "\n",
            "272\n",
            "\tTrain Loss: 1.121 | Train Acc: 77.46%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 38.01% \n",
            "\n",
            "273\n",
            "\tTrain Loss: 1.119 | Train Acc: 77.49%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 38.52% \n",
            "\n",
            "274\n",
            "\tTrain Loss: 1.119 | Train Acc: 77.36%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.55% \n",
            "\n",
            "275\n",
            "\tTrain Loss: 1.118 | Train Acc: 77.48%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.15% \n",
            "\n",
            "276\n",
            "\tTrain Loss: 1.118 | Train Acc: 77.65%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.91% \n",
            "\n",
            "277\n",
            "\tTrain Loss: 1.119 | Train Acc: 77.83%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 37.68% \n",
            "\n",
            "278\n",
            "\tTrain Loss: 1.118 | Train Acc: 77.70%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 38.32% \n",
            "\n",
            "279\n",
            "\tTrain Loss: 1.120 | Train Acc: 77.48%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.25% \n",
            "\n",
            "280\n",
            "\tTrain Loss: 1.121 | Train Acc: 77.43%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 38.02% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 172, 0: 336, 1: 158, 4: 86, 3: 38}, 4: {0: 153, 4: 69, 1: 58, 2: 59, 3: 7}, 1: {1: 376, 3: 160, 0: 99, 2: 127, 4: 23}, 3: {3: 179, 0: 33, 1: 212, 2: 31, 4: 9}, 2: {2: 155, 1: 183, 0: 169, 4: 31, 3: 41}}\n",
            "\t pred vs actuals: {0: {2: 172, 0: 336, 1: 158, 4: 86, 3: 38}, 4: {0: 153, 4: 69, 1: 58, 2: 59, 3: 7}, 1: {1: 376, 3: 160, 0: 99, 2: 127, 4: 23}, 3: {3: 179, 0: 33, 1: 212, 2: 31, 4: 9}, 2: {2: 155, 1: 183, 0: 169, 4: 31, 3: 41}}\n",
            "281\n",
            "\tTrain Loss: 1.118 | Train Acc: 77.90%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.55% \n",
            "\n",
            "282\n",
            "\tTrain Loss: 1.117 | Train Acc: 78.07%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.05% \n",
            "\n",
            "283\n",
            "\tTrain Loss: 1.116 | Train Acc: 77.62%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.22% \n",
            "\n",
            "284\n",
            "\tTrain Loss: 1.118 | Train Acc: 77.48%\n",
            "\t Val. Loss: 1.508 |  Val. Acc: 38.58% \n",
            "\n",
            "285\n",
            "\tTrain Loss: 1.118 | Train Acc: 77.66%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 37.89% \n",
            "\n",
            "286\n",
            "\tTrain Loss: 1.118 | Train Acc: 77.65%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.05% \n",
            "\n",
            "287\n",
            "\tTrain Loss: 1.116 | Train Acc: 77.69%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.95% \n",
            "\n",
            "288\n",
            "\tTrain Loss: 1.113 | Train Acc: 78.24%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 38.08% \n",
            "\n",
            "289\n",
            "\tTrain Loss: 1.114 | Train Acc: 77.88%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 37.91% \n",
            "\n",
            "290\n",
            "\tTrain Loss: 1.114 | Train Acc: 78.06%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.21% \n",
            "\n",
            "291\n",
            "\tTrain Loss: 1.116 | Train Acc: 77.55%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.44% \n",
            "\n",
            "292\n",
            "\tTrain Loss: 1.115 | Train Acc: 77.65%\n",
            "\t Val. Loss: 1.511 |  Val. Acc: 38.41% \n",
            "\n",
            "293\n",
            "\tTrain Loss: 1.117 | Train Acc: 77.72%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.42% \n",
            "\n",
            "294\n",
            "\tTrain Loss: 1.112 | Train Acc: 78.28%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.24% \n",
            "\n",
            "295\n",
            "\tTrain Loss: 1.115 | Train Acc: 77.64%\n",
            "\t Val. Loss: 1.507 |  Val. Acc: 38.91% \n",
            "\n",
            "296\n",
            "\tTrain Loss: 1.115 | Train Acc: 78.02%\n",
            "\t Val. Loss: 1.512 |  Val. Acc: 38.28% \n",
            "\n",
            "297\n",
            "\tTrain Loss: 1.113 | Train Acc: 78.05%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 38.01% \n",
            "\n",
            "298\n",
            "\tTrain Loss: 1.116 | Train Acc: 77.65%\n",
            "\t Val. Loss: 1.520 |  Val. Acc: 37.24% \n",
            "\n",
            "299\n",
            "\tTrain Loss: 1.119 | Train Acc: 77.19%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 38.14% \n",
            "\n",
            "300\n",
            "\tTrain Loss: 1.113 | Train Acc: 77.97%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 38.31% \n",
            "\n",
            "label distribution: actual vs pred: {0: {2: 173, 0: 352, 1: 136, 4: 98, 3: 31}, 4: {4: 79, 1: 47, 2: 53, 0: 161, 3: 6}, 1: {1: 362, 0: 120, 3: 152, 2: 125, 4: 26}, 3: {3: 176, 0: 34, 2: 40, 1: 200, 4: 14}, 2: {2: 157, 1: 166, 4: 38, 0: 181, 3: 37}}\n",
            "\t pred vs actuals: {0: {2: 173, 0: 352, 1: 136, 4: 98, 3: 31}, 4: {4: 79, 1: 47, 2: 53, 0: 161, 3: 6}, 1: {1: 362, 0: 120, 3: 152, 2: 125, 4: 26}, 3: {3: 176, 0: 34, 2: 40, 1: 200, 4: 14}, 2: {2: 157, 1: 166, 4: 38, 0: 181, 3: 37}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVFVWAa-j3OD",
        "outputId": "7694eb48-b442-4636-9a57-211c8fea1ce1"
      },
      "source": [
        "print(train_acc_list)\n",
        "print(valid_acc_list)"
      ],
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.21265671310359485, 0.22230608632031096, 0.2361561724040062, 0.23688716648920485, 0.2396972382177501, 0.24672429501738177, 0.2605668709158353, 0.2628937855567018, 0.2733630237241858, 0.27854507127308953, 0.2926630217190747, 0.30398091401683686, 0.31339747053847467, 0.3303605403802166, 0.3279973263065565, 0.3382975846937258, 0.35068568247094, 0.34896459182103473, 0.35952395255162833, 0.37349795723614626, 0.3742389650377509, 0.38026716338989397, 0.3799367138784226, 0.39139730037619536, 0.3939294941621284, 0.4010028639068342, 0.4075755527302555, 0.4113556938628628, 0.4121242390103536, 0.42148697240167554, 0.42432332977856674, 0.4318585776847247, 0.4413665083989705, 0.43470494877802185, 0.4433642252939477, 0.45637567103181254, 0.4533790956893468, 0.454172674923727, 0.46047875108239855, 0.4656507851870637, 0.468791306154913, 0.472080779946558, 0.47868976813473113, 0.4849219940024424, 0.48837544058011545, 0.4958919128870855, 0.4939930806421254, 0.4953211366313778, 0.5026260715097053, 0.5023506969621737, 0.5098859450044153, 0.516279640263074, 0.5172872605933446, 0.5217645998414793, 0.5282133702001616, 0.5313251021245843, 0.5327520427638537, 0.5396013578323469, 0.5452991068091022, 0.5425003004945033, 0.5461289855443179, 0.5555655692266003, 0.5600867178886448, 0.5525865176496985, 0.5530058379586973, 0.5578661980149953, 0.5630570075827647, 0.5617101758582407, 0.5698875471881536, 0.574187144569066, 0.5778796664656025, 0.5769609169328593, 0.580010063572017, 0.5818212769891573, 0.5855137988856939, 0.5859043298790988, 0.587412631130654, 0.5900875191165976, 0.593957782608189, 0.6010662001561901, 0.5962058398277248, 0.6055347772493754, 0.6092623466770398, 0.6035645979724519, 0.6075324941443526, 0.6117594927957614, 0.619411149253584, 0.6160841243996468, 0.622033465398501, 0.6171468196938571, 0.6175198269216982, 0.6191069856626258, 0.6264745054179675, 0.6262792397851813, 0.6285348074621262, 0.6304348914046266, 0.6294260193768157, 0.6322273290864954, 0.6423047846310759, 0.636097592850254, 0.642669029976135, 0.6457482176284267, 0.6463890892185576, 0.6500465633117989, 0.6424387165400536, 0.6503319514396528, 0.6516687695838544, 0.6556016182246274, 0.65100036037567, 0.6588122346629836, 0.6525962809993796, 0.6563426258901482, 0.6659106386306624, 0.668318913679689, 0.664948079411842, 0.6703616919582838, 0.6700324941443526, 0.6633609209430816, 0.66980843946814, 0.6710664003951364, 0.6707547263467692, 0.6689259894362324, 0.672547164028638, 0.6762471961104162, 0.6794590642463126, 0.6794227650176444, 0.6794140031348624, 0.6781748176709702, 0.6734471380982769, 0.6868954279107046, 0.6821064148863701, 0.6824281022428922, 0.6891434851302404, 0.6857638889796114, 0.6893400024605668, 0.6898481935670931, 0.6925143193980875, 0.689661689817089, 0.6985888306953046, 0.6958676299003705, 0.6984373747485958, 0.6971080670618031, 0.6941377773676833, 0.7013525894787758, 0.7052416287056388, 0.6984098374027096, 0.7063381198334368, 0.6999882339886879, 0.6991671174084215, 0.7067574401424356, 0.7124113794331137, 0.7162816431968724, 0.7101558120283362, 0.7127606044076893, 0.7075059582653656, 0.716067602100982, 0.7143202255305634, 0.710467485804536, 0.7108517583102396, 0.7125365497314766, 0.7182981355549538, 0.7210531321834756, 0.7143465111789093, 0.7145255047436718, 0.7175834135377788, 0.7226227668866719, 0.7134277619183336, 0.7221683990465452, 0.7336377474270999, 0.7257532744102826, 0.7210794181039889, 0.7281878353798226, 0.7232648901743431, 0.7310066691272334, 0.7298288171149824, 0.7330481957082879, 0.7282854683322993, 0.7302306136584173, 0.7329768486763244, 0.7323359768140262, 0.7358933148318774, 0.7351798445122427, 0.7329242771074652, 0.7337616660279226, 0.7331646038517016, 0.7320580991435813, 0.7359997096671361, 0.7283305294437495, 0.7329505627558112, 0.7340470541557765, 0.7408074981545749, 0.7344213130811578, 0.7461410017862712, 0.7413069273783192, 0.7358757910663134, 0.7455877492961274, 0.7392466253341605, 0.7374266503064055, 0.7429917187995563, 0.7456678582108729, 0.7443122646035669, 0.7426800450233564, 0.7442496797265528, 0.7460158314879082, 0.7433221683110276, 0.7460333555256395, 0.751304273735987, 0.7398524493931635, 0.746113464440385, 0.7460158314879082, 0.7520715673220212, 0.7534446846948911, 0.7507422596352286, 0.7519726829441715, 0.7515446007523907, 0.7472011936854009, 0.7542207401637073, 0.7616495933706902, 0.7563511375422891, 0.7530791876522918, 0.7498860952516669, 0.7523294181039889, 0.7543809582653656, 0.758564147230697, 0.7557903748669036, 0.7593214272364089, 0.7633443983722495, 0.7566640632881966, 0.7564938316062161, 0.7618185733551304, 0.7581886366077754, 0.7595004205290041, 0.7581898883053156, 0.753114235455587, 0.7594102980339363, 0.7559693684316662, 0.7574401184848455, 0.7644671754205608, 0.7647087538623374, 0.7592763658527915, 0.7597232235076765, 0.7624519347600197, 0.7622917166583614, 0.7619612674190573, 0.7648702233893686, 0.7628712550690185, 0.7595179445667354, 0.7682135203113295, 0.7625157713345742, 0.7616571035559319, 0.7615156614617126, 0.767063205917132, 0.7634595550902902, 0.7628624929140692, 0.7709422315636726, 0.7685439698228008, 0.7721726548726155, 0.7716381778455761, 0.7636110110369991, 0.7730738806397948, 0.7668041037097914, 0.7746259913052598, 0.7694001342063626, 0.7697919171694751, 0.7717445726808347, 0.7690509092317869, 0.7701924617432024, 0.7671257910663134, 0.7759189997633843, 0.7729311865758678, 0.7711825583079089, 0.7798067872927069, 0.7818057558852244, 0.7684989084391833, 0.7787816431968724, 0.7787891533821141, 0.7825630358909363, 0.7791559021222537, 0.7733868061135348, 0.7768014500674592, 0.7777740225944345, 0.7733592687676486, 0.7788792758771818, 0.779637807580434, 0.7754721421084992, 0.7782634382378565, 0.7803775635484147, 0.7804138627770829, 0.7789330991435813, 0.7759715713322435, 0.7760441700617472, 0.7832314448269535, 0.7776663763338028, 0.7835443703006936, 0.7831250499916947, 0.7781032204083657, 0.7836244792154391, 0.7797980254099249, 0.7751604683322993, 0.7824215932523824]\n",
            "[0.26476063817105394, 0.2777260637029688, 0.2773936168944582, 0.2892952130195942, 0.29148936176553686, 0.30113031921234534, 0.30738031927575454, 0.3062500002536368, 0.2998005319783028, 0.3127659575102177, 0.31170212778639284, 0.32041223410596237, 0.34148936195576446, 0.3474734045089559, 0.35478723429618997, 0.3661569147667986, 0.36283244668169223, 0.3535239360433944, 0.35558510650979713, 0.36628989399747647, 0.3628989362970312, 0.3632313831055418, 0.3595744682119248, 0.3625664894885205, 0.37393617059322115, 0.3619015958714992, 0.3719414897421573, 0.3702127660842652, 0.3719414897421573, 0.367619681231519, 0.367619681231519, 0.371276596125136, 0.36030585144428495, 0.36263297910385944, 0.3619680854868382, 0.36728723442300837, 0.363962766337902, 0.37220744693532903, 0.3681515956178625, 0.3682180852332014, 0.36788563842469074, 0.36456117033958435, 0.37061170250811476, 0.36462765995492324, 0.3796542553191489, 0.3805851067634339, 0.3816489361702128, 0.368550532041712, 0.37214095731999014, 0.37593085144428495, 0.37925531952939134, 0.37121010650979713, 0.3702127660842652, 0.38091755357194457, 0.3714760637029688, 0.3817819147667986, 0.3748005317880752, 0.3842420216570509, 0.379587766337902, 0.3783909577004453, 0.3880984047625927, 0.38238031940257294, 0.3779255322953488, 0.37420212778639284, 0.37799202127659576, 0.3807180853600198, 0.3718085105114795, 0.3793882981259772, 0.3876994683387432, 0.3793218085106383, 0.3781249998731816, 0.3789228727208807, 0.3831781916161801, 0.3746010642102424, 0.3748005317880752, 0.38118351076511625, 0.3830452130195942, 0.3821808511906482, 0.3808510639566056, 0.3779255322953488, 0.38025265995492324, 0.37413563817105394, 0.37606383004087085, 0.3788563831055418, 0.37646276583062843, 0.3898936174017318, 0.3793218085106383, 0.38716755331830777, 0.383244681231519, 0.3807845743412667, 0.3810505321685304, 0.38357712804002964, 0.383244681231519, 0.38091755357194457, 0.37918882991405245, 0.38091755357194457, 0.37779255306467097, 0.3863031914893617, 0.3808510639566056, 0.3800531917429985, 0.3783909577004453, 0.38311170200084116, 0.3849069152740722, 0.384175532041712, 0.3873005319148936, 0.3855718088910935, 0.38763297872340424, 0.38663563829787234, 0.3798537235310737, 0.3821808511906482, 0.380518617148095, 0.3798537235310737, 0.39035904280682826, 0.3923537236578921, 0.379587766337902, 0.3858377660842652, 0.38291223442300837, 0.38404255344512617, 0.383244681231519, 0.384175532041712, 0.38623670250811476, 0.38896276595744683, 0.3878989365506679, 0.3781914894885205, 0.3828457448076695, 0.38716755331830777, 0.38896276595744683, 0.3925531918698169, 0.3925531918698169, 0.38763297872340424, 0.3821808511906482, 0.39095744680851063, 0.38617021289277587, 0.3865026597012865, 0.3855718088910935, 0.3871010637029688, 0.384175532041712, 0.38816489374383967, 0.39740691540089057, 0.39029255319148937, 0.3911569150204354, 0.3911569150204354, 0.394281914893617, 0.3888962769761999, 0.39654255357194457, 0.39248670225447796, 0.394281914893617, 0.3998670216570509, 0.3954787235310737, 0.38916223416937157, 0.4035239365506679, 0.38783244693532903, 0.3932180854868382, 0.3941489362970312, 0.3959441489361702, 0.3986037234042553, 0.40019946846556154, 0.39561170212765956, 0.3992686170212766, 0.39561170212765956, 0.3863031914893617, 0.3959441489361702, 0.40232712791321124, 0.3932180854868382, 0.3978723408059871, 0.3972739361702128, 0.39029255319148937, 0.39261968085106386, 0.3978723408059871, 0.395212766337902, 0.3908909578272637, 0.3902260642102424, 0.38982712778639284, 0.39654255357194457, 0.3954787235310737, 0.3922207450613062, 0.39920212804002964, 0.3951462767225631, 0.39720744718896583, 0.39700797897704104, 0.39561170212765956, 0.3949468085106383, 0.3998670216570509, 0.408045213273231, 0.3996010638297872, 0.3959441489361702, 0.3916223404255319, 0.3962765957446808, 0.39468085131746655, 0.3956781917429985, 0.3932180854868382, 0.3976063829787234, 0.3949468085106383, 0.3972739361702128, 0.3925531918698169, 0.3976063829787234, 0.3986702130195942, 0.3986702130195942, 0.39261968085106386, 0.39893617021276595, 0.39953457484854027, 0.40186170250811476, 0.3998670216570509, 0.39853723442300837, 0.3968750003804552, 0.4009308510638298, 0.4035904255319149, 0.3921542554459673, 0.40113031927575454, 0.3986037234042553, 0.3962765957446808, 0.3966090425531915, 0.39820478761449773, 0.39561170212765956, 0.4002659574468085, 0.39388297910385944, 0.39288563867832754, 0.3918882982527956, 0.3942154259123701, 0.3992686170212766, 0.3908244682119248, 0.39740691540089057, 0.402593085106383, 0.39388297910385944, 0.3949468085106383, 0.394281914893617, 0.3942154259123701, 0.3982712765957447, 0.39288563867832754, 0.39654255357194457, 0.3867686168944582, 0.39388297910385944, 0.39148936182894606, 0.39288563867832754, 0.39720744718896583, 0.3954787235310737, 0.395212766337902, 0.3954787235310737, 0.3974734043821375, 0.4005319152740722, 0.3972739361702128, 0.3945478727208807, 0.40186170250811476, 0.396143617148095, 0.3954787235310737, 0.3942154259123701, 0.39720744718896583, 0.3978723408059871, 0.3976063829787234, 0.3998670216570509, 0.3955452131464126, 0.39820478761449773, 0.39288563867832754, 0.3902260642102424, 0.38683510650979713, 0.39561170212765956, 0.38816489374383967, 0.3922207450613062, 0.3894946809778822, 0.39049202140341416, 0.39261968085106386, 0.39853723442300837, 0.384175532041712, 0.39587765995492324, 0.3935505322953488, 0.3949468085106383, 0.3949468085106383, 0.39793882978723405, 0.4021941493166254, 0.3962765957446808, 0.3986037234042553, 0.3992686170212766, 0.3888962769761999, 0.3939494680851064, 0.3908244682119248, 0.3978723408059871, 0.39753989399747647, 0.39853723442300837, 0.39288563867832754, 0.3955452131464126, 0.3976063829787234, 0.3959441489361702, 0.3925531918698169, 0.39049202140341416, 0.39654255357194457, 0.39388297910385944, 0.3942154259123701, 0.39853723442300837, 0.4005319152740722, 0.39793882978723405, 0.3932180854868382, 0.39195478723404253, 0.3912234046357743, 0.3918882982527956, 0.39853723442300837, 0.39288563867832754, 0.3885638301676892, 0.3945478727208807, 0.398869681231519, 0.3966090425531915]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bu4vOqiLnwT"
      },
      "source": [
        "def get_pred_vs_act_per_label(pred_act):\n",
        "  preds, act = list(zip(*pred_act))\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  for i in preds:\n",
        "    _, pred = torch.max(i, 1)\n",
        "    for j in pred:\n",
        "      predictions.append(j.item())\n",
        "  for i in act:\n",
        "    for j in i:\n",
        "      actuals.append(j.item())\n",
        "\n",
        "  assert len(predictions) == len(actuals)\n",
        "\n",
        "  label_dict = {}\n",
        "  \n",
        "  for i in range(0, len(actuals)):\n",
        "    predic = predictions[i]\n",
        "    actual = actuals[i]\n",
        "    if label_dict.get(actual, None) != None:\n",
        "      in_dict = label_dict[actual]\n",
        "      if in_dict.get(predic, None) != None:\n",
        "        in_dict[predic] += 1\n",
        "      else:\n",
        "        in_dict[predic] = 1\n",
        "    else:\n",
        "      in_dict = {predic: 1}\n",
        "    label_dict[actual] = in_dict\n",
        "  print(\"label distribution: actual vs pred:\", label_dict)\n",
        "  return label_dict"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdP_YkZrCCiu",
        "outputId": "e9e030be-c9d7-41f6-c5ab-4108282e9668"
      },
      "source": [
        "preds, act = list(zip(*preds_actual_tup))\n",
        "zero_preds = 0\n",
        "same_probability = 0\n",
        "for i in preds:\n",
        "  for j in i:\n",
        "    maxv, pred = torch.max(j, dim = 0)\n",
        "    minv, pred = torch.min(j, dim = 0)\n",
        "    if pred == 0:\n",
        "      zero_preds += 1\n",
        "      if maxv - minv < 0.1:\n",
        "        same_probability += 1\n",
        "      \n",
        "zero_preds, same_probability"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZfnWo0abRx"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "\n",
        "\n",
        "def classify_review(review):\n",
        "    \n",
        "    categories = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3:\"Positive\", 4:\"Very Positive\"}\n",
        "    \n",
        "    # tokenize the review \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(review)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()], pred.item()"
      ],
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SnsyRRHL3Zb"
      },
      "source": [
        "def classify_and_print(test_list):\n",
        "  for i in test_list:\n",
        "    x,y = i\n",
        "    cat  = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3:\"Positive\", 4:\"Very Positive\"}\n",
        "    actual_label = cat[y]\n",
        "    predicted_label_str, predicted_label = classify_review(x) \n",
        "    print(\"sentence: \", x)\n",
        "    print(\"actual_label: \", actual_label, \"predicted_label: \", predicted_label_str)\n",
        "    print(\"\")"
      ],
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4CKUTC1PdIC",
        "outputId": "0b0fe25d-4b24-4b9a-8620-1ffcc7cfbcaf"
      },
      "source": [
        "testdata_to_test = random.sample(testlist, 25)\n",
        "classify_and_print(testdata_to_test)"
      ],
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence:  Possibly not since Grumpy Old Men have I heard a film so solidly connect with one demographic while striking out with another .\n",
            "actual_label:  Neutral predicted_label:  Very Negative\n",
            "\n",
            "sentence:  Burns never really harnesses to full effect the energetic cast .\n",
            "actual_label:  Negative predicted_label:  Negative\n",
            "\n",
            "sentence:  ( Screenwriter ) Pimental took the Farrelly Brothers comedy and feminized it , but it is a rather poor imitation .\n",
            "actual_label:  Neutral predicted_label:  Negative\n",
            "\n",
            "sentence:  Unless you 're a fanatic , the best advice is : ` Scooby ' do n't .\n",
            "actual_label:  Negative predicted_label:  Very Positive\n",
            "\n",
            "sentence:  The film takes the materials of human tragedy and dresses them in lovely costumes , Southern California locations and star power .\n",
            "actual_label:  Neutral predicted_label:  Negative\n",
            "\n",
            "sentence:  It is philosophy , illustrated through everyday events .\n",
            "actual_label:  Positive predicted_label:  Very Negative\n",
            "\n",
            "sentence:  Daughter From Danang reveals that efforts toward closure only open new wounds .\n",
            "actual_label:  Negative predicted_label:  Very Negative\n",
            "\n",
            "sentence:  A small movie with a big impact .\n",
            "actual_label:  Very Positive predicted_label:  Negative\n",
            "\n",
            "sentence:  ( Reaches ) wholly believable and heart-wrenching depths of despair .\n",
            "actual_label:  Positive predicted_label:  Negative\n",
            "\n",
            "sentence:  It 's not just the vampires that are damned in Queen of the Damned -- the viewers will feel they suffer the same fate .\n",
            "actual_label:  Very Negative predicted_label:  Negative\n",
            "\n",
            "sentence:  A woozy , roisterous , exhausting mess , and the off-beat casting of its two leads turns out to be as ill-starred as you might expect .\n",
            "actual_label:  Negative predicted_label:  Very Negative\n",
            "\n",
            "sentence:  It 's difficult to imagine the process that produced such a script , but here 's guessing that spray cheese and underarm noises played a crucial role .\n",
            "actual_label:  Very Negative predicted_label:  Neutral\n",
            "\n",
            "sentence:  For most of its footage , the new thriller proves that director M. Night Shyamalan can weave an eerie spell and that Mel Gibson can gasp , shudder and even tremble without losing his machismo .\n",
            "actual_label:  Very Positive predicted_label:  Positive\n",
            "\n",
            "sentence:  A model of what films like this should be like .\n",
            "actual_label:  Very Positive predicted_label:  Very Negative\n",
            "\n",
            "sentence:  Offers laughs and insight into one of the toughest ages a kid can go through .\n",
            "actual_label:  Positive predicted_label:  Very Negative\n",
            "\n",
            "sentence:  Is it something any true film addict will want to check out ?\n",
            "actual_label:  Neutral predicted_label:  Very Negative\n",
            "\n",
            "sentence:  What happens when something goes bump in the night and nobody cares ?\n",
            "actual_label:  Negative predicted_label:  Negative\n",
            "\n",
            "sentence:  It dares to be a little different , and that shading is what makes it worthwhile .\n",
            "actual_label:  Positive predicted_label:  Negative\n",
            "\n",
            "sentence:  If you like quirky , odd movies and/or the ironic , here 's a fun one .\n",
            "actual_label:  Very Positive predicted_label:  Very Positive\n",
            "\n",
            "sentence:  It would be interesting to hear from the other side , but in Talk to Her , the women are down for the count .\n",
            "actual_label:  Neutral predicted_label:  Negative\n",
            "\n",
            "sentence:  Ah yes , and then there 's the music ...\n",
            "actual_label:  Neutral predicted_label:  Negative\n",
            "\n",
            "sentence:  If you 're not the target demographic ... this movie is one long chick-flick slog .\n",
            "actual_label:  Negative predicted_label:  Very Positive\n",
            "\n",
            "sentence:  But watching Huppert , a great actress tearing into a landmark role , is riveting .\n",
            "actual_label:  Very Positive predicted_label:  Neutral\n",
            "\n",
            "sentence:  We hate ( Madonna ) within the film 's first five minutes , and she lacks the skill or presence to regain any ground .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  Filmmaker Stacy Peralta has a flashy editing style that does n't always jell with Sean Penn 's monotone narration , but he respects the material without sentimentalizing it .\n",
            "actual_label:  Positive predicted_label:  Positive\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVHxktkNPjFG",
        "outputId": "33ae0951-d4bc-4a5b-cea1-077e6d7d0bb6"
      },
      "source": [
        "count = 0\n",
        "false_pos_list = []\n",
        "for i in testlist:\n",
        "  x,y = i\n",
        "  _, predicted_label = classify_review(x) \n",
        "  if predicted_label in [3,4] and y in [0,1]:\n",
        "    false_pos_list.append(i)\n",
        "    count += 1\n",
        "    if count == 10:\n",
        "      break\n",
        "\n",
        "print(false_pos_list)"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"The movie is silly beyond comprehension , and even if it were n't silly , it would still be beyond comprehension .\", 0], ['One of those decades-spanning historical epics that strives to be intimate and socially encompassing but fails to do justice to either effort in three hours of screen time .', 1], ['Alas , getting there is not even half the interest .', 0], [\"It 's quite diverting nonsense .\", 1], [\"... Liotta is put in an impossible spot because his character 's deceptions ultimately undo him and the believability of the entire scenario .\", 1], ['After the first 10 minutes , which is worth seeing , the movie sinks into an abyss of clichés , depression and bad alternative music .', 1], ['Witless and utterly pointless .', 1], [\"The filmmakers juggle and juxtapose three story lines but fail to come up with one cogent point , unless it 's that life stinks , especially for sensitive married women who really love other women .\", 1], ['I hated every minute of it .', 0], ['Why he was given free reign over this project -- he wrote , directed , starred and produced -- is beyond me .', 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZvrK7k0RsGY",
        "outputId": "46d84f76-cf48-44aa-fa10-1702e05f09f7"
      },
      "source": [
        "classify_and_print(false_pos_list)"
      ],
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence:  The movie is silly beyond comprehension , and even if it were n't silly , it would still be beyond comprehension .\n",
            "actual_label:  Very Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  One of those decades-spanning historical epics that strives to be intimate and socially encompassing but fails to do justice to either effort in three hours of screen time .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  Alas , getting there is not even half the interest .\n",
            "actual_label:  Very Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  It 's quite diverting nonsense .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  ... Liotta is put in an impossible spot because his character 's deceptions ultimately undo him and the believability of the entire scenario .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  After the first 10 minutes , which is worth seeing , the movie sinks into an abyss of clichés , depression and bad alternative music .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  Witless and utterly pointless .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  The filmmakers juggle and juxtapose three story lines but fail to come up with one cogent point , unless it 's that life stinks , especially for sensitive married women who really love other women .\n",
            "actual_label:  Negative predicted_label:  Positive\n",
            "\n",
            "sentence:  I hated every minute of it .\n",
            "actual_label:  Very Negative predicted_label:  Very Positive\n",
            "\n",
            "sentence:  Why he was given free reign over this project -- he wrote , directed , starred and produced -- is beyond me .\n",
            "actual_label:  Very Negative predicted_label:  Very Positive\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMZLqxEPO21E"
      },
      "source": [
        "\n",
        "for i in testdata_to_test:\n",
        "  x,y = i\n",
        "  cat  = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3:\"Positive\", 4:\"Very Positive\"}\n",
        "  actual_label = cat[y]\n",
        "  predicted_label = classify_review(x) \n",
        "  print(\"sentence: \", x)\n",
        "  print(\"actual_label: \", actual_label, \"predicted_label: \", predicted_label)\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qUbSUk58RuD"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq6OyEd68RHG"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTqTxseVPmiN",
        "outputId": "cd2d03f5-b9e3-47f6-d660-ca9d4a58829c"
      },
      "source": [
        "torch.tensor([1.0000e+00, 9.0564e-22, 7.4852e-13, 5.6629e-07, 4.7160e-15],\n",
        "       device='cuda:0').shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK8qm_OjCoHx"
      },
      "source": [
        "preds2 = []\n",
        "for i in preds:\n",
        "  _, predictions = torch.max(i, 1)\n",
        "  #print(predictions)\n",
        "  #break\n",
        "  for j in predictions:\n",
        "    preds2.append(j)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCL9DmrjERmj",
        "outputId": "738e72f3-8303-485e-9dd8-ca6a96ba60d9"
      },
      "source": [
        "len(act)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ShyiCwd-y_p",
        "outputId": "30399d4b-6304-4b9e-82bf-2ba1162fad18"
      },
      "source": [
        "dic = get_pred_vs_act_per_label(preds_actual_tup)"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label distribution: actual vs pred: {3: {1: 194, 4: 32, 3: 105, 0: 194, 2: 33}, 2: {2: 202, 1: 203, 3: 23, 0: 32, 4: 6}, 1: {3: 84, 1: 426, 0: 127, 4: 26, 2: 143}, 0: {0: 354, 3: 121, 4: 80, 1: 159, 2: 27}, 4: {1: 72, 3: 41, 0: 201, 4: 59, 2: 20}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkpqgf3SO8KH",
        "outputId": "9d09ebe4-ecd2-477a-9eec-3e5b54fd7c89"
      },
      "source": [
        "final_su = 0\n",
        "final_co = 0\n",
        "for keys in dic:\n",
        "  key = dic[keys]\n",
        "  su = 0\n",
        "  predicted = 0\n",
        "  for keys2 in key:\n",
        "     su += key[keys2]\n",
        "     if keys2 == keys:\n",
        "       predicted = key[keys2]\n",
        "  print(keys, su, predicted)\n",
        "  final_su += su\n",
        "  final_co += predicted\n",
        "print(final_su, final_co)\n",
        "print(final_co/final_su)"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 558 105\n",
            "2 466 202\n",
            "1 806 426\n",
            "0 741 354\n",
            "4 393 59\n",
            "2964 1146\n",
            "0.3866396761133603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJLC9rIgC-oh",
        "outputId": "535911ce-d8a5-4d64-9e57-18f4614f0b23"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(trainlist, columns = ['sentence', 'label'])\n",
        "test_df = pd.DataFrame(testlist, columns = ['sentence', 'label'])\n",
        "train_df['label'].value_counts(), test_df['label'].value_counts()"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1    2365\n",
              " 3    2332\n",
              " 2    1696\n",
              " 4    1350\n",
              " 0    1148\n",
              " Name: label, dtype: int64, 3    779\n",
              " 1    775\n",
              " 2    546\n",
              " 4    502\n",
              " 0    362\n",
              " Name: label, dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLwDa0xxEGaS",
        "outputId": "abf636b3-e6c2-4eb5-ca6c-29904ca77083"
      },
      "source": [
        "trainlist[0:10]"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHb8uppDC5BC"
      },
      "source": [
        "t = torch.tensor([0, 3, 1, 0, 3, 1, 0, 3, 2, 0, 1, 1, 3, 1, 4, 0, 4, 4, 0, 2, 2, 1, 3, 4,\n",
        "        0, 1, 0, 4, 3, 0, 1, 4])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "q9zGBY9oMneX",
        "outputId": "04ae1a03-fd8e-461c-bc2e-0b1b52d1b6cb"
      },
      "source": [
        "F.softmax(torch.tensor([-8.7747e-02, -4.0806e-02, -8.3384e-02,  3.9979e-02, -6.0466e-02]), dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-ef3263738124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8.7747e-02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m4.0806e-02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m8.3384e-02\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m3.9979e-02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m6.0466e-02\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqc-ecwdDFJo"
      },
      "source": [
        "import torch.nn.functional as f\n",
        "#t.apply_(lambda x : torch.nn.functional.one_hot(torch.tensor(x))) #torch.nn.functional.one_hot(torch.tensor(x), 5))\n",
        "\n",
        "t = t.tolist()\n",
        "print(t)\n",
        "oneh = []\n",
        "for i in t:\n",
        "  print(i)\n",
        "  oneh.append(f.one_hot(torch.tensor(i), 5).tolist())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LIRDAnsFq09",
        "outputId": "e74c378e-e30c-4812-85a7-2a26e5bfff3f"
      },
      "source": [
        "torch.tensor(oneh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0],\n",
              "        [0, 1, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0],\n",
              "        [0, 1, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0],\n",
              "        [0, 0, 1, 0, 0],\n",
              "        [1, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0],\n",
              "        [0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 1],\n",
              "        [1, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 1],\n",
              "        [1, 0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0],\n",
              "        [0, 0, 1, 0, 0],\n",
              "        [0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 1],\n",
              "        [1, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 1, 0],\n",
              "        [1, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIO_S3X0Gebf",
        "outputId": "f5d5f12a-9667-4467-9359-c63becd1f754"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "output = loss(input, target)\n",
        "\n",
        "input, target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.5062,  0.1430, -0.0579,  0.2270, -0.0728],\n",
              "         [ 1.7862,  0.0257, -1.2147,  0.4436,  1.6470],\n",
              "         [ 0.3827, -1.1616, -1.6744,  1.2986, -1.1603]], requires_grad=True),\n",
              " tensor([4, 1, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQtiESL3xclN",
        "outputId": "6f3fbbac-0ebb-4c46-e2ac-530b37292061"
      },
      "source": [
        "t = torch.tensor([[0.3525, 0.3786, 0.1276, 0.0825, 0.0589],[0.3653, 0.3919, 0.1250, 0.0693, 0.0485],[0.3750, 0.3844, 0.1240, 0.0717, 0.0448]])\n",
        "x, y = torch.max(t, 1)\n",
        "x,y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.3786, 0.3919, 0.3844]), tensor([1, 1, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ifzc4o4b8T",
        "outputId": "d5a43d26-421c-454b-aadf-d5e86ab9bc38"
      },
      "source": [
        "t= torch.tensor([1])\n",
        "t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwb_O3J_5cyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de5f03c-892e-4097-bb9a-48fc454b25b4"
      },
      "source": [
        "import torch\n",
        "torch.nn.functional.one_hot(torch.tensor(2), 6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpmKkoIO8vEO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyEeEjXTGhX"
      },
      "source": [
        "By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU."
      ]
    }
  ]
}
