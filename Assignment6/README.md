# Assignment 6

## Data preparation

As the colab file used in the class used the same dataset, there were no extra special preparation required for this one. Data Augmentation was also not considered as the model was giving good accuracy without it

## Model Preparation

In the class we used a normal LSTM. But as per the assignment we are 

- Embedding Layer
  - We send the batch of sentences to the embedding layer

- Encoder Layer
  - The encoder layer consisted of the GRU Cell as its core unit. 
  - What makes it a layer, is sending every word of the sentence to the GRU Cell. Hence the length of the encoder layer is decided by the length of the sentence in the batch

- Decoder
  - It consists of a single GRU Cell
  - Alternative approach: Multi unit decoder - Usually encoder decoder consists of multiple cells (sometime same number of cells), but this is more applicable to the word translation problems. Hence did not try this out, as the accuracy was pretty decent with only one cell as decoder
  - The input of this decoder cell would be the output of the final encoder cell

- Fully connected layer of 3 output nodes to classify the output 

In both the encoder and decoder we do not initialise a random hidden vector but let the cells use the default zero values

Model diagram: 

![Image](https://github.com/sagawritescode/ENDTwoPointOPhase1/blob/main/Assignment6/Model%20architecture.png)
  
## Output 

Training logs 

```
Epoch 1 | Time Taken: 0.30s
	Train Loss: 1.083 | Train Acc: 49.29%
	 Val. Loss: 1.074 |  Val. Acc: 56.11% 

Epoch 2 | Time Taken: 0.28s
	Train Loss: 1.061 | Train Acc: 61.31%
	 Val. Loss: 1.046 |  Val. Acc: 68.99% 

Epoch 3 | Time Taken: 0.28s
	Train Loss: 1.017 | Train Acc: 67.05%
	 Val. Loss: 0.983 |  Val. Acc: 68.75% 

Epoch 4 | Time Taken: 0.28s
	Train Loss: 0.948 | Train Acc: 68.65%
	 Val. Loss: 0.912 |  Val. Acc: 70.98% 

Epoch 5 | Time Taken: 0.28s
	Train Loss: 0.890 | Train Acc: 68.74%
	 Val. Loss: 0.868 |  Val. Acc: 71.88% 

Epoch 6 | Time Taken: 0.29s
	Train Loss: 0.869 | Train Acc: 68.82%
	 Val. Loss: 0.853 |  Val. Acc: 71.43% 

Epoch 7 | Time Taken: 0.27s
	Train Loss: 0.858 | Train Acc: 69.08%
	 Val. Loss: 0.853 |  Val. Acc: 70.98% 

Epoch 8 | Time Taken: 0.27s
	Train Loss: 0.850 | Train Acc: 69.58%
	 Val. Loss: 0.845 |  Val. Acc: 70.98% 

Epoch 9 | Time Taken: 0.28s
	Train Loss: 0.846 | Train Acc: 70.26%
	 Val. Loss: 0.844 |  Val. Acc: 70.98% 

Epoch 10 | Time Taken: 0.30s
	Train Loss: 0.842 | Train Acc: 71.36%
	 Val. Loss: 0.836 |  Val. Acc: 71.43% 

Epoch 11 | Time Taken: 0.27s
	Train Loss: 0.838 | Train Acc: 71.44%
	 Val. Loss: 0.828 |  Val. Acc: 70.33% 

Epoch 12 | Time Taken: 0.28s
	Train Loss: 0.834 | Train Acc: 72.45%
	 Val. Loss: 0.825 |  Val. Acc: 71.22% 

Epoch 13 | Time Taken: 0.29s
	Train Loss: 0.831 | Train Acc: 73.13%
	 Val. Loss: 0.819 |  Val. Acc: 72.77% 

Epoch 14 | Time Taken: 0.28s
	Train Loss: 0.830 | Train Acc: 73.64%
	 Val. Loss: 0.816 |  Val. Acc: 73.01% 

Epoch 15 | Time Taken: 0.28s
	Train Loss: 0.828 | Train Acc: 74.40%
	 Val. Loss: 0.810 |  Val. Acc: 74.35% 

Epoch 16 | Time Taken: 0.27s
	Train Loss: 0.826 | Train Acc: 74.48%
	 Val. Loss: 0.800 |  Val. Acc: 76.79% 

Epoch 17 | Time Taken: 0.27s
	Train Loss: 0.824 | Train Acc: 75.24%
	 Val. Loss: 0.803 |  Val. Acc: 77.68% 

Epoch 18 | Time Taken: 0.28s
	Train Loss: 0.823 | Train Acc: 75.07%
	 Val. Loss: 0.803 |  Val. Acc: 77.03% 

Epoch 19 | Time Taken: 0.27s
	Train Loss: 0.820 | Train Acc: 75.49%
	 Val. Loss: 0.790 |  Val. Acc: 77.47% 

Epoch 20 | Time Taken: 0.29s
	Train Loss: 0.819 | Train Acc: 75.66%
	 Val. Loss: 0.792 |  Val. Acc: 77.92% 

Epoch 21 | Time Taken: 0.27s
	Train Loss: 0.811 | Train Acc: 76.51%
	 Val. Loss: 0.796 |  Val. Acc: 79.26% 

Epoch 22 | Time Taken: 0.27s
	Train Loss: 0.807 | Train Acc: 76.59%
	 Val. Loss: 0.783 |  Val. Acc: 79.70% 

Epoch 23 | Time Taken: 0.28s
	Train Loss: 0.801 | Train Acc: 76.85%
	 Val. Loss: 0.780 |  Val. Acc: 79.26% 

Epoch 24 | Time Taken: 0.28s
	Train Loss: 0.792 | Train Acc: 77.69%
	 Val. Loss: 0.770 |  Val. Acc: 80.15% 

Epoch 25 | Time Taken: 0.28s
	Train Loss: 0.786 | Train Acc: 78.11%
	 Val. Loss: 0.764 |  Val. Acc: 79.70% 

```

Sample output for encoder and decoder layer

```
Sending word no:  0 to the encoder
Output of the encoder:  tensor([[ 0.2058, -0.3136,  0.2856, -0.2534,  0.7220,  0.0217, -0.1964,  0.8956,
          0.2938,  0.2987,  0.4305, -0.0810, -0.6956,  0.8361,  0.0863, -0.1338,
          0.3219,  0.5745,  0.2199,  0.2676,  0.0442,  0.6336, -0.3882,  0.1586,
          0.2869, -0.4127, -0.5079,  0.0612, -0.3877, -0.1976,  0.1211, -0.1838,
         -0.3921,  0.5729, -0.1603, -0.1147,  0.8502, -0.4621, -0.3675, -0.0127,
         -0.2218,  0.3069, -0.1473,  0.4175,  0.4704,  0.1210,  0.3260,  0.2885,
         -0.2389, -0.3332,  0.0576,  0.0192,  0.1986,  0.3949,  0.4797, -0.1638,
          0.3114,  0.4872,  0.6424, -0.3082,  0.5766, -0.2461,  0.8563,  0.0576,
         -0.2997,  0.5999,  0.0838, -0.7253, -0.1684,  0.2831,  0.7313, -0.3701,
          0.4270,  0.2249,  0.3249,  0.3742,  0.3030,  0.4757,  0.4461,  0.4092,
          0.6452, -0.1660, -0.2076, -0.1364,  0.0728, -0.2401, -0.0221, -0.5756,
          0.6845,  0.4949, -0.0411,  0.1543, -0.3253, -0.2784, -0.4598, -0.0643,
          0.4968, -0.0067, -0.0465,  0.0973]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  1 to the encoder
Output of the encoder:  tensor([[ 0.0991, -0.0460,  0.1242, -0.4784,  0.2837,  0.3884,  0.6012,  0.5759,
         -0.2034, -0.1127, -0.0300,  0.0636, -0.5882,  0.7897,  0.5840,  0.6290,
         -0.5927,  0.3513,  0.3081,  0.7929,  0.4606,  0.4171,  0.3120, -0.4474,
         -0.1665, -0.6208, -0.7991, -0.7836, -0.1027, -0.7137, -0.2502,  0.6179,
         -0.0338,  0.4782, -0.2908, -0.4047,  0.8578, -0.1376,  0.4937, -0.5309,
          0.0070, -0.0411, -0.1238,  0.4604, -0.0882,  0.2680,  0.3277,  0.4321,
         -0.2438, -0.2144,  0.0655,  0.4064,  0.5706,  0.2072,  0.6716, -0.1398,
          0.0655, -0.8759,  0.6533, -0.5478,  0.4584, -0.1628,  0.8371,  0.1036,
         -0.3774,  0.1195, -0.2500, -0.4963,  0.8107,  0.4185, -0.4406,  0.4580,
          0.5076,  0.5406, -0.5163,  0.5738,  0.1753, -0.3424,  0.4725,  0.5928,
         -0.3224,  0.2851, -0.1028, -0.1065, -0.6237, -0.2104,  0.0559, -0.6511,
          0.3417,  0.4032,  0.3398,  0.1970, -0.0521, -0.3194,  0.4842,  0.2959,
         -0.3252,  0.1879,  0.1294, -0.2619]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  2 to the encoder
Output of the encoder:  tensor([[ 0.4188, -0.0789,  0.1145,  0.2066,  0.2195,  0.4637,  0.2490,  0.4881,
          0.3408, -0.3313, -0.2764,  0.2012, -0.2699,  0.3331,  0.6658,  0.6680,
          0.3098,  0.0738,  0.3649,  0.3969,  0.5321,  0.5491, -0.3478, -0.5191,
          0.4735, -0.6421, -0.4880, -0.8935,  0.0464, -0.6652, -0.2630, -0.1905,
         -0.0356, -0.0565, -0.0529,  0.8336,  0.4548, -0.1331,  0.3321, -0.1395,
         -0.1411,  0.0391,  0.0651,  0.4253,  0.1079,  0.2245,  0.5032,  0.9201,
         -0.1612, -0.3877,  0.3220,  0.7801,  0.6375,  0.2671, -0.3442, -0.4747,
          0.5381,  0.3480,  0.8027, -0.4025,  0.0533, -0.6413,  0.2637, -0.1296,
         -0.8448,  0.0875, -0.0338, -0.6124, -0.2625,  0.4830, -0.5472,  0.0963,
          0.3103,  0.8013, -0.2443,  0.3417, -0.1546, -0.2095,  0.4630,  0.0622,
         -0.4302, -0.0547, -0.2978,  0.0401,  0.3438, -0.4060,  0.6686,  0.3246,
          0.6042,  0.3036,  0.0858,  0.0517, -0.3912, -0.2777, -0.1980,  0.5825,
          0.3445,  0.6705,  0.3567,  0.0578]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  3 to the encoder
Output of the encoder:  tensor([[ 0.2150,  0.4772,  0.1679, -0.4055,  0.2090,  0.4073,  0.2870,  0.4939,
          0.6400,  0.2321, -0.3587,  0.7730,  0.2302, -0.3072,  0.7358,  0.2596,
         -0.0982,  0.2175,  0.7848,  0.1915,  0.2608,  0.1751,  0.4365, -0.2531,
          0.5836, -0.6647, -0.3326, -0.9454,  0.5825, -0.7192, -0.6377,  0.7796,
          0.5855, -0.4833, -0.7177,  0.7846,  0.3472, -0.3049,  0.7759,  0.0854,
          0.3373, -0.3511,  0.2794,  0.5171,  0.2050, -0.0790,  0.8615,  0.4221,
          0.2352, -0.2252,  0.6203,  0.8042,  0.7446, -0.1432, -0.4609, -0.1811,
          0.8219, -0.8293, -0.7328,  0.2988, -0.1690, -0.6925,  0.8133, -0.8195,
         -0.9135, -0.0226,  0.2255, -0.1962, -0.4865,  0.4752, -0.6941,  0.2726,
          0.4955,  0.9045,  0.0550, -0.7360, -0.5355, -0.2393,  0.5667,  0.6808,
         -0.4595,  0.2570, -0.2004,  0.2291,  0.4872, -0.6600,  0.4696,  0.1253,
          0.1795, -0.2595, -0.0124,  0.2265, -0.6905, -0.0235,  0.0030,  0.6047,
          0.0369,  0.8449,  0.0616,  0.4278]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  4 to the encoder
Output of the encoder:  tensor([[ 0.5747,  0.2438,  0.2554,  0.0795, -0.4029,  0.5425,  0.5529,  0.0283,
          0.6684,  0.5581, -0.2732, -0.3255, -0.0918, -0.0341, -0.5129,  0.3718,
         -0.6879,  0.2971, -0.5150,  0.4322,  0.0577,  0.5859,  0.3346, -0.5629,
         -0.1049, -0.6532, -0.3147, -0.0306,  0.8082, -0.7160, -0.3991,  0.5703,
          0.5387, -0.2516, -0.4981,  0.8000,  0.2604, -0.1303,  0.1441,  0.0150,
          0.7739,  0.1273, -0.1348,  0.5265,  0.3640, -0.0626,  0.8981,  0.2692,
          0.2398, -0.1274,  0.8441,  0.7409,  0.7240, -0.6563, -0.1001, -0.0427,
          0.5201, -0.8149, -0.3072,  0.0891, -0.7193, -0.9627,  0.4687, -0.8254,
         -0.9178,  0.7460,  0.3514, -0.3414, -0.6187,  0.7197, -0.7825,  0.2260,
          0.4549,  0.9619,  0.4020, -0.7784, -0.3165,  0.2074, -0.4910,  0.2754,
         -0.0852,  0.5740, -0.2857, -0.5935, -0.0835, -0.5505,  0.5828,  0.1896,
         -0.0251, -0.5251,  0.5322,  0.3875, -0.8226, -0.3926,  0.3760,  0.6591,
         -0.6491,  0.7346,  0.1006,  0.7669]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  5 to the encoder
Output of the encoder:  tensor([[-0.4182,  0.4700,  0.5925, -0.0671, -0.2987,  0.1874,  0.6536,  0.4182,
          0.4832,  0.6718,  0.6202,  0.8249,  0.2213,  0.1418, -0.3952, -0.7784,
         -0.7343,  0.2096, -0.1402,  0.7510, -0.1182, -0.5680,  0.6427, -0.6644,
         -0.1994, -0.6868,  0.1933, -0.5411,  0.8507, -0.0786, -0.5375, -0.1456,
          0.3783, -0.3399, -0.4999,  0.1241,  0.4923,  0.5912,  0.3912,  0.0308,
          0.2064,  0.6014, -0.4660, -0.3984,  0.4117, -0.5351, -0.1666, -0.3667,
          0.0591, -0.4037, -0.0125, -0.3685,  0.3154, -0.2087, -0.3334, -0.6689,
         -0.3700, -0.6588, -0.0219, -0.8090, -0.7683, -0.8402,  0.1412,  0.3969,
         -0.9291,  0.8935,  0.5739,  0.4249, -0.5360,  0.0608, -0.1675,  0.2606,
          0.2771,  0.9842,  0.2540, -0.6509, -0.4215,  0.1092,  0.1836, -0.5064,
          0.1119,  0.7551, -0.5536, -0.6985,  0.3275, -0.3827,  0.3601,  0.5342,
          0.4901, -0.7976,  0.6696, -0.0389, -0.9183, -0.4265, -0.4102,  0.2595,
         -0.6523,  0.8667, -0.0740,  0.6447]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  6 to the encoder
Output of the encoder:  tensor([[-0.1054,  0.2417,  0.6685, -0.2853,  0.2689,  0.5662,  0.7099,  0.2037,
         -0.3867, -0.3963,  0.1552,  0.8873, -0.2765, -0.2243, -0.4102,  0.1540,
         -0.3019,  0.0915, -0.6237,  0.8236, -0.3746, -0.0302, -0.6405, -0.4880,
         -0.3847, -0.3050,  0.3030, -0.8450,  0.3520,  0.1882, -0.2844, -0.7162,
          0.4935, -0.7959, -0.3893, -0.1101,  0.7703,  0.3324, -0.3912, -0.5854,
          0.1882, -0.3108, -0.6876,  0.5746, -0.0168, -0.0157,  0.7124, -0.4085,
          0.2232, -0.7783, -0.6578, -0.3831,  0.7975, -0.2207, -0.7601,  0.4801,
          0.3247, -0.8048,  0.2769, -0.8144, -0.8249, -0.8223,  0.5813, -0.6365,
         -0.5454,  0.6358,  0.7004,  0.5304,  0.2599,  0.0894, -0.3037, -0.1370,
          0.7195,  0.9649,  0.5681,  0.0957, -0.5384,  0.2598, -0.3433, -0.6906,
         -0.3680,  0.2775,  0.0808, -0.4519, -0.3214,  0.3613,  0.4017, -0.3111,
          0.3816, -0.2085,  0.8039,  0.3242, -0.9751,  0.2942, -0.6443,  0.7457,
         -0.4638,  0.8096,  0.0618,  0.7383]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  7 to the encoder
Output of the encoder:  tensor([[ 0.3900, -0.4285,  0.6596, -0.3686,  0.6445,  0.3226,  0.7872,  0.6551,
          0.2561, -0.1222,  0.2818, -0.2257, -0.4868,  0.6363,  0.1137,  0.0576,
          0.5379,  0.5029, -0.4444,  0.7796, -0.4514,  0.0433, -0.0779, -0.3221,
         -0.3297, -0.3340,  0.3501, -0.8094,  0.2799, -0.4677, -0.4373, -0.5994,
          0.7332, -0.8964, -0.1019, -0.3794,  0.7208, -0.0226,  0.3453, -0.6072,
          0.8040, -0.1477, -0.3746,  0.6179,  0.4793, -0.0252,  0.8602, -0.1632,
         -0.0241, -0.0461, -0.7003,  0.1779,  0.3662,  0.0182, -0.7933,  0.5635,
          0.3190, -0.6559,  0.4864, -0.2755, -0.8220, -0.3391,  0.5843, -0.7722,
         -0.3440,  0.4993,  0.7304,  0.3987, -0.3209,  0.7430, -0.2377,  0.3057,
          0.6609,  0.7059,  0.7217,  0.0801, -0.7339, -0.1827,  0.5531, -0.8051,
         -0.4848,  0.7346, -0.2247, -0.6651,  0.0125, -0.3366,  0.4345, -0.4978,
          0.6374, -0.2479,  0.5185,  0.4144, -0.9551,  0.1755,  0.5618,  0.6756,
         -0.5809,  0.1395,  0.4443,  0.7775]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  8 to the encoder
Output of the encoder:  tensor([[ 5.3665e-01, -2.1945e-02,  7.0521e-01, -4.8680e-01,  7.8215e-01,
          1.2280e-01,  3.0838e-01,  7.2419e-01,  3.0772e-01, -9.5673e-02,
         -3.1675e-01,  7.4006e-01, -6.1343e-01,  4.6049e-01,  6.1629e-01,
         -2.8196e-01,  1.0439e-01,  5.8934e-01,  3.0806e-01,  8.1512e-01,
         -6.3947e-01,  5.0840e-01,  1.7311e-01, -7.1822e-01, -7.1164e-01,
         -2.6898e-01,  7.9904e-01, -7.1152e-01, -5.6199e-01, -4.2433e-01,
         -7.2451e-01, -7.3965e-01,  8.6651e-01, -9.6490e-01, -2.4405e-01,
          3.0173e-01,  5.4751e-01, -4.0269e-01,  2.5135e-01, -5.1786e-01,
          3.9858e-05, -1.6628e-01, -9.6013e-02,  4.7903e-01, -1.5738e-01,
         -2.6621e-01,  7.0210e-01,  4.7682e-02,  9.0143e-02, -8.9779e-03,
         -4.3872e-01,  7.2290e-01,  5.3111e-01,  4.3288e-01, -7.0100e-01,
          4.4142e-02,  3.4646e-01, -4.7684e-01,  5.6030e-01, -4.3102e-02,
         -7.7039e-01, -8.1808e-01,  6.2038e-01, -2.5385e-01, -6.6043e-01,
          6.0955e-01,  8.3281e-01,  7.1228e-01, -4.1808e-01,  6.8222e-01,
         -7.3701e-01,  6.4858e-01,  6.6692e-02, -2.1607e-01,  7.9304e-01,
         -1.1975e-01, -1.7916e-01,  3.4959e-01,  6.7781e-01, -8.5210e-01,
         -4.5865e-01,  8.3459e-01, -1.9548e-01, -7.0115e-01,  3.6419e-01,
         -4.0645e-01,  1.6777e-01, -3.6905e-01,  6.3905e-01, -4.8447e-02,
          6.5053e-01,  6.8573e-01,  2.4530e-01, -4.9207e-01, -7.7367e-02,
          2.0365e-01, -6.4197e-01,  8.6567e-01,  5.3973e-01,  9.2746e-01]],
       device='cuda:0', grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  9 to the encoder
Output of the encoder:  tensor([[ 0.7754,  0.4799,  0.7470,  0.3194,  0.6580,  0.5309,  0.5772,  0.6368,
         -0.1621, -0.1797, -0.8680,  0.6574, -0.6471,  0.6031,  0.4499, -0.5037,
         -0.1553, -0.3225,  0.8248,  0.6432, -0.3437, -0.3466,  0.2177, -0.7148,
         -0.1299, -0.7470,  0.2672, -0.3095,  0.8144, -0.0419, -0.7851, -0.8610,
          0.7572, -0.9840, -0.7478,  0.5462,  0.4593, -0.8020,  0.5050, -0.5626,
          0.5580,  0.3867, -0.0325,  0.5070,  0.6225, -0.1570,  0.7861,  0.5235,
         -0.2366, -0.8968,  0.7507,  0.6424,  0.7989, -0.7977, -0.7273, -0.1942,
          0.5190,  0.2489,  0.9075, -0.2097, -0.9287, -0.7760,  0.1450, -0.6383,
         -0.7163,  0.5176,  0.4983,  0.8326, -0.5695,  0.8214, -0.6947,  0.5061,
          0.0234,  0.1903,  0.2599, -0.2420, -0.2739, -0.7993,  0.8788, -0.7093,
         -0.6826,  0.5485, -0.3158, -0.6847,  0.1429, -0.8904, -0.2703, -0.2707,
          0.3088, -0.5194,  0.7887,  0.5506, -0.7253, -0.5573, -0.2924, -0.1379,
         -0.9424,  0.8002,  0.4988,  0.7477]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  10 to the encoder
Output of the encoder:  tensor([[ 0.6905,  0.3044,  0.6705, -0.3724,  0.5257,  0.5159,  0.6496,  0.6617,
          0.3750,  0.5431, -0.4295,  0.9335, -0.8325,  0.4481,  0.0773, -0.8050,
          0.1267, -0.2593,  0.8905,  0.6675, -0.2562, -0.5598,  0.4702, -0.5532,
          0.1624, -0.7789, -0.1990, -0.4941,  0.7194, -0.2545, -0.8763,  0.0380,
          0.8051, -0.9504, -0.7745,  0.9032, -0.2651,  0.6454,  0.8017, -0.1269,
          0.8780,  0.9003, -0.2293,  0.6394,  0.8967, -0.4128,  0.7688,  0.7097,
         -0.2731, -0.6558,  0.9293,  0.7164,  0.8326, -0.7902, -0.7448,  0.7957,
          0.7802, -0.8799, -0.1903, -0.5889, -0.9731, -0.6397,  0.5967, -0.8790,
         -0.7775,  0.5498,  0.6302, -0.6680, -0.0420,  0.9239, -0.1728,  0.5549,
          0.4352,  0.8944,  0.4996,  0.4256, -0.3453, -0.7610, -0.6624, -0.9310,
         -0.8758,  0.8033, -0.2382, -0.6046,  0.1722, -0.9219, -0.3995, -0.5929,
         -0.2432, -0.8770,  0.8107,  0.9187, -0.9457,  0.3725, -0.7631,  0.4152,
         -0.9666,  0.4674, -0.3809,  0.8328]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  11 to the encoder
Output of the encoder:  tensor([[ 0.8098, -0.1682,  0.6723, -0.5256,  0.4813,  0.7585,  0.7515,  0.6176,
          0.6760,  0.5858, -0.5056, -0.0215, -0.5587, -0.0686,  0.8214, -0.7600,
         -0.4094, -0.0282,  0.8003,  0.7803, -0.3867,  0.0689,  0.6034, -0.1572,
         -0.0399, -0.8188,  0.4507, -0.6921,  0.9293, -0.6836, -0.7676, -0.1623,
          0.9298, -0.9109, -0.8115,  0.7834, -0.1129, -0.3641,  0.7193, -0.1374,
          0.9011,  0.6424, -0.5849,  0.4640,  0.7452, -0.6903,  0.8495,  0.8712,
         -0.1713, -0.8520,  0.9185,  0.8420,  0.8431,  0.5187, -0.8950,  0.7352,
          0.9199, -0.9803, -0.1107, -0.7727, -0.9522, -0.7042,  0.2338, -0.9306,
         -0.7221,  0.7022, -0.4971, -0.6111, -0.5500,  0.2860, -0.4391,  0.3709,
         -0.2530,  0.1786,  0.3620,  0.6754, -0.4432,  0.0622, -0.2792, -0.9694,
         -0.9821,  0.8748, -0.1613, -0.8225,  0.1648, -0.4626, -0.1465, -0.6355,
         -0.7975,  0.3274,  0.9032,  0.5534,  0.0482,  0.1149, -0.8238,  0.5270,
         -0.6241,  0.6670, -0.3441,  0.7507]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  12 to the encoder
Output of the encoder:  tensor([[ 0.6094, -0.7791,  0.7990, -0.7588,  0.2337,  0.1085,  0.8140,  0.6036,
          0.7980,  0.4755, -0.8446,  0.7175, -0.8789,  0.2502,  0.9296, -0.7256,
         -0.5290,  0.4481,  0.9338, -0.5830, -0.6011,  0.1181,  0.3462, -0.1868,
         -0.2854, -0.7416,  0.6943, -0.7402,  0.9804, -0.4253, -0.6918, -0.2850,
          0.7115, -0.9369, -0.1248, -0.1349,  0.5554, -0.9167,  0.9335, -0.4318,
          0.9418,  0.6115, -0.1347,  0.3007,  0.9189, -0.5819,  0.7689,  0.5627,
         -0.0317, -0.7625,  0.9237,  0.9137,  0.9128,  0.1669, -0.6459, -0.2572,
          0.9412, -0.9753,  0.2510, -0.8688, -0.8661, -0.4282,  0.0391, -0.4462,
         -0.7971,  0.1636, -0.0105, -0.2473, -0.6925,  0.4123, -0.3129,  0.5858,
         -0.2681,  0.5965, -0.2775,  0.4492, -0.5348, -0.1763,  0.3955,  0.3061,
         -0.9047,  0.8755, -0.2613, -0.4637,  0.7345, -0.6606, -0.1467, -0.9217,
         -0.2716,  0.8151,  0.8284,  0.0198, -0.5977, -0.7377, -0.9147,  0.6426,
         -0.5948,  0.7632, -0.2331,  0.8035]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  13 to the encoder
Output of the encoder:  tensor([[ 0.2043, -0.8165,  0.8629, -0.7912,  0.4592,  0.5972,  0.7433,  0.7687,
          0.8241,  0.7945, -0.9027,  0.8206, -0.9511,  0.3332,  0.9006,  0.2777,
         -0.7596,  0.6760,  0.9327,  0.0258, -0.5687,  0.7394,  0.8883, -0.1242,
         -0.6961, -0.9673,  0.9504, -0.8256,  0.9817, -0.0027, -0.8717, -0.2808,
          0.4058, -0.8863, -0.3183,  0.5978,  0.7633, -0.3149,  0.9804, -0.0194,
          0.5007,  0.6809, -0.2179,  0.4124,  0.8557, -0.7952,  0.6543,  0.1575,
         -0.1908, -0.4621,  0.5376,  0.9623,  0.9332, -0.4078, -0.8723, -0.3212,
          0.8282, -0.8110,  0.4816, -0.3238, -0.9443, -0.7605,  0.3275,  0.2219,
         -0.9744, -0.0369, -0.0672, -0.1972, -0.5898,  0.4408, -0.9135,  0.5765,
          0.1917,  0.6118,  0.0058,  0.4682, -0.5573, -0.8616,  0.5687, -0.7694,
         -0.9491,  0.7877, -0.2831, -0.6507,  0.7975, -0.6036, -0.1014,  0.1565,
          0.2949, -0.3661,  0.5777,  0.2236, -0.6867, -0.6714, -0.6791,  0.3680,
         -0.6776, -0.3708,  0.5579,  0.8625]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Sending word no:  14 to the encoder
Output of the encoder:  tensor([[-0.5000, -0.9476,  0.8617, -0.9721,  0.9260,  0.6188,  0.8501,  0.8697,
         -0.9280,  0.8226, -0.8882,  0.8935, -0.9940,  0.9821,  0.9287, -0.8081,
         -0.7167,  0.3292,  0.9595,  0.7789, -0.5147,  0.9764,  0.7664, -0.6563,
         -0.5456, -0.9477,  0.9384, -0.5583,  0.8968, -0.8368, -0.9207, -0.2990,
          0.9688, -0.8778,  0.4092,  0.6476, -0.5216, -0.2846,  0.9210, -0.9401,
          0.6937,  0.9480, -0.7556,  0.1015,  0.9602, -0.8507,  0.9873,  0.2779,
          0.9453, -0.8098,  0.8121,  0.7716,  0.9174, -0.7598, -0.8537, -0.2505,
          0.9893, -0.8937, -0.8138, -0.8881, -0.9299, -0.9125,  0.8812, -0.9670,
         -0.8789,  0.9229,  0.6799, -0.9315, -0.8449,  0.4595, -0.8798,  0.5345,
          0.9765,  0.8009,  0.6521,  0.8126, -0.5716, -0.0133,  0.8110, -0.8092,
         -0.7427,  0.8864, -0.7402, -0.6704,  0.8513, -0.8202, -0.0306, -0.0321,
          0.9321, -0.8368,  0.6127,  0.9162, -0.9277, -0.8764, -0.8512,  0.4468,
         -0.7012,  0.0662,  0.5260,  0.8503]], device='cuda:0',
       grad_fn=<ThnnFusedGruCellBackward>)
Decoder output:  tensor([[ 5.1876, -3.0806, -3.5352]], device='cuda:0', grad_fn=<AddmmBackward>)
Negative
```

## Group members

- Sagar Shete
- Pushya Mitra
- Kanchana Gore
